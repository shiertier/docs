# self-llm：专为国内初学者打造的开源大模型教程，提供基于 Linux 环境的模型部署、应用集成与高效微调的全流程指导

## 文档信息
- 发布日期：2026-02-22
- 对象：GitHub 项目 `datawhalechina/self-llm`
- 项目主页：https://github.com/datawhalechina/self-llm
- 开源协议：Apache License 2.0
- 主要语言：Jupyter Notebook
- 统计快照：Stars 28298，Forks 2822，Watchers 157（抓取时间：2026-02-22）
- 版本快照：暂无正式发布记录

## 摘要
### 1) 一句话总结
`datawhalechina/self-llm`（《开源大模型食用指南》）是一个专为国内初学者打造的开源大模型教程，提供基于 Linux 环境的模型部署、应用集成与高效微调的全流程指导。

### 2) 核心要点
* **项目热度与规范**：该项目主要使用 Jupyter Notebook 编写，采用 Apache License 2.0 开源协议，在 GitHub 上拥有极高的社区关注度（超 2.8 万 Stars 和 2800+ Forks）。
* **核心教学内容**：涵盖四大模块：Linux 环境配置、主流开源大模型部署、应用指导（命令行调用、在线 Demo、LangChain 集成）以及模型微调（全量微调、LoRA、ptuning 等）。
* **广泛的模型支持**：已支持 50+ 款国内外主流大语言模型及多模态大模型，包括 Qwen 系列、DeepSeek 系列、GLM 系列、Llama 系列、Kimi 等，并为每个模型提供完整的部署与微调教程。
* **多硬件平台适配**：除常规 GPU 外，项目设立了专门的硬件专区，提供 AMD GPU、昇腾 Ascend NPU 以及 Apple M 系列芯片的环境配置与部署教程，并计划支持更多国产硬件生态。
* **丰富的实战案例**：提供多个特色应用 Example，如基于《甄嬛传》微调的“Chat-嬛嬛”、涵盖人情世故与 RAG 的“Tianji-天机”、解答高等数学的“AMChat”以及定制个人数字人的“数字生命”。
* **明确的学习路径**：建议初学者按照“环境配置 -> 部署使用 -> 微调”的顺序学习，并推荐优先从 Qwen1.5、InternLM2、MiniCPM 等模型入手。
* **完善的进阶体系**：项目与 Datawhale 的其他开源课程（如 Happy-LLM、Tiny-Universe、so-large-llm、llm-universe）形成矩阵，方便学习者进一步探索大模型底层原理、从零手写核心组件及开发实际应用。

## 功能与定位
《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程

## 典型使用场景
- 作为学习与选型参考入口，快速定位资料与最佳实践。
- 用于团队知识库沉淀与技术调研。

## 核心功能
- 汇总课程、示例、清单或社区经验。
- 强调可检索性与持续更新。
- 适合学习路径规划与资源导航。

## 特色与差异点
- 仓库长期活跃，最近更新时间为 2026-02-22T12:42:33Z。
- 项目创建于 2023-11-16T02:31:29Z，具备持续迭代与社区沉淀。
- 以 `Jupyter Notebook` 为主语言，聚焦该技术栈的工程实践。

## 使用方式概览
1. 阅读仓库 README 与官方文档，确认适配场景与依赖条件。
2. 按项目推荐方式完成安装与初始化，再从示例或最小流程开始验证。
3. 在生产使用前补齐权限控制、日志监控和版本固定策略。

## 限制与注意事项
- 使用前应先核对许可证、项目维护状态与安全边界。

## 链接
- 仓库：https://github.com/datawhalechina/self-llm
- README：https://raw.githubusercontent.com/datawhalechina/self-llm/master/README.md
- Releases：https://github.com/datawhalechina/self-llm/releases

## 关联主题
- [[00-元语/github]]
- [[00-元语/learning-resource]]
- [[00-元语/llm]]
- [[00-元语/lora]]
- [[00-元语/multimodal]]
- [[00-元语/rag]]
- [[00-元语/self-hosting]]
