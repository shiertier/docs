# llama.cpp：轻量级本地大模型推理引擎

## 文档信息
- 发布日期：2026-02-22
- 对象：GitHub 项目 `ggml-org/llama.cpp`
- 项目主页：https://github.com/ggml-org/llama.cpp
- 开源协议：MIT License
- 主要语言：C++
- 统计快照：Stars 95594，Forks 15020，Watchers 638（抓取时间：2026-02-22）
- 版本快照：最新发布 b8123（发布时间：2026-02-21T19:59:51Z）

## 摘要
**一句话总结**
llama.cpp 是一个无外部依赖的 C/C++ 开源大语言模型推理框架，旨在通过广泛的硬件优化和量化技术，在本地和云端实现高效的 LLM 推理。

**关键要点**
* **项目基础**：采用 MIT 开源协议，主要使用 C++ 编写，社区活跃度极高（超 9.5 万 Stars 和 1.5 万 Forks）。
* **核心特性**：纯 C/C++ 实现，无任何外部依赖；该项目也是开发 `ggml` 库新功能的主要试验场。
* **广泛的硬件优化**：对 Apple Silicon 提供原生优化（ARM NEON、Accelerate、Metal），同时支持 x86（AVX、AVX2、AVX512、AMX）和 RISC-V 架构。
* **GPU 与混合推理**：支持自定义 CUDA 内核（NVIDIA），兼容 AMD (HIP)、Moore Threads (MUSA)、Vulkan 和 SYCL 后端，并支持 CPU+GPU 混合推理以运行超出显存容量的大模型。
* **模型量化**：支持 1.5-bit 到 8-bit 的多种整数量化方案，大幅提升推理速度并降低内存占用。
* **生态与工具支持**：提供 `llama-server`（兼容 OpenAI 的 REST API，已支持多模态）、全新 WebUI，以及用于代码补全的 VS Code 和 Vim 插件。
* **Hugging Face 集成**：支持直接通过 CLI 运行 Hugging Face 上的模型，Hugging Face 推理端点已开箱即用支持 GGUF 格式。
* **丰富的模型兼容性**：支持数十种主流模型系列，包括 LLaMA 1/2/3、Mistral、Qwen、Deepseek、Gemma、Mamba 等，并近期与 NVIDIA 合作添加了对原生 MXFP4 格式的支持。


## 功能与定位
LLM inference in C/C++

## 典型使用场景
- 用于模型训练、微调、推理或文档解析等基础能力建设。
- 作为上层 AI 应用的数据与模型基础设施。

## 核心功能
- 提供模型/推理相关核心能力。
- 支持与主流 AI 工具链协同。
- 兼顾实验验证与工程落地场景。

## 特色与差异点
- 仓库长期活跃，最近更新时间为 2026-02-22T12:28:14Z。
- 项目创建于 2023-03-10T18:58:00Z，具备持续迭代与社区沉淀。
- 以 `C++` 为主语言，聚焦该技术栈的工程实践。

## 使用方式概览
1. 阅读仓库 README 与官方文档，确认适配场景与依赖条件。
2. 按项目推荐方式完成安装与初始化，再从示例或最小流程开始验证。
3. 在生产使用前补齐权限控制、日志监控和版本固定策略。

## 限制与注意事项
- 使用前应先核对许可证、项目维护状态与安全边界。

## 链接
- 仓库：https://github.com/ggml-org/llama.cpp
- README：https://raw.githubusercontent.com/ggml-org/llama.cpp/main/README.md
- Releases：https://github.com/ggml-org/llama.cpp/releases

## 关联主题
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/github]]
- [[00-元语/cli]]
- [[00-元语/multimodal]]
- [[00-元语/self-hosting]]
