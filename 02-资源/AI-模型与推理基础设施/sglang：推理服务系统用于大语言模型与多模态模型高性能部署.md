# sglang：推理服务系统用于大语言模型与多模态模型高性能部署

## 文档信息
- 发布日期：2026-02-22
- 对象：GitHub 项目 `sgl-project/sglang`
- 项目主页：https://github.com/sgl-project/sglang
- 官方网站：https://sglang.io
- 开源协议：Apache License 2.0
- 主要语言：Python
- 统计快照：Stars 23647，Forks 4516，Watchers 134（抓取时间：2026-02-22）
- 版本快照：最新发布 v0.5.8（发布时间：2026-01-23T22:09:28Z）

## 摘要
### 1) 一句话总结
SGLang 是一个由非营利组织 LMSYS 托管的高性能大语言模型与多模态模型推理服务框架，旨在通过 RadixAttention 等核心技术在各类硬件和分布式集群上提供低延迟、高吞吐的推理能力。

### 2) 关键要点
* **项目基础信息**：主要使用 Python 开发，采用 Apache License 2.0 开源协议，GitHub 拥有超 2.3 万 Stars，最新发布版本为 v0.5.8。
* **核心加速特性**：内置 RadixAttention（前缀缓存）、零开销 CPU 调度器、Prefill-Decode 分离、投机解码、连续批处理、多种并行策略（张量/流水线/专家/数据并行）以及丰富的量化支持（FP4/FP8/INT4/AWQ/GPTQ）。
* **广泛的模型兼容性**：兼容 Hugging Face 模型与 OpenAI API，支持 Llama、Qwen、DeepSeek 等主流语言模型，并扩展支持嵌入模型、奖励模型及扩散模型（如 WAN、Qwen-Image）。
* **全面的硬件支持**：不仅支持 NVIDIA GPU（含 GB200/B300/H100 等）和 AMD GPU（MI355/MI300），还支持 Intel Xeon CPU、Google TPU（通过 SGLang-Jax 后端）以及昇腾 NPU。
* **前沿模型零日支持（Day-0）**：为 DeepSeek V3/R1、Mistral Large 3、Nemotron 3 Nano 等最新开源模型提供发布首日的推理支持与专属优化。
* **强化学习与后训练生态**：作为成熟的 Rollout 后端，原生支持强化学习集成，已被 AReaL、verl、Tunix、slime 等知名后训练框架采用。
* **大规模行业应用**：全球部署于超 40 万张 GPU 上，每日在生产环境中生成万亿级 Token，被 xAI、NVIDIA、AMD、AWS、微软 Azure、Google Cloud 等头部科技企业及顶尖高校广泛采用。
* **社区与生态认可**：已正式加入 PyTorch 生态系统，并获得了 a16z 第三批开源 AI 资助（Open Source AI Grant）。

## 功能与定位
SGLang is a high-performance serving framework for large language models and multimodal models.

## 典型使用场景
- 用于模型训练、微调、推理或文档解析等基础能力建设。
- 作为上层 AI 应用的数据与模型基础设施。

## 核心功能
- 提供模型/推理相关核心能力。
- 支持与主流 AI 工具链协同。
- 兼顾实验验证与工程落地场景。

## 特色与差异点
- 仓库长期活跃，最近更新时间为 2026-02-22T12:50:34Z。
- 项目创建于 2024-01-08T04:15:52Z，具备持续迭代与社区沉淀。
- 以 `Python` 为主语言，聚焦该技术栈的工程实践。

## 使用方式概览
1. 阅读仓库 README 与官方文档，确认适配场景与依赖条件。
2. 按项目推荐方式完成安装与初始化，再从示例或最小流程开始验证。
3. 在生产使用前补齐权限控制、日志监控和版本固定策略。

## 限制与注意事项
- 使用前应先核对许可证、项目维护状态与安全边界。

## 链接
- 仓库：https://github.com/sgl-project/sglang
- 官网：https://sglang.io
- README：https://raw.githubusercontent.com/sgl-project/sglang/main/README.md
- Releases：https://github.com/sgl-project/sglang/releases

## 关联主题
- [[00-元语/AI]]
- [[00-元语/github]]
- [[00-元语/llm]]
- [[00-元语/multimodal]]
- [[00-元语/llmops]]
- [[00-元语/self-hosting]]
- [[00-元语/benchmark]]
