# Wan2.1：视频生成大模型项目

## 文档信息
- 发布日期：2026-02-22
- 对象：GitHub 项目 `Wan-Video/Wan2.1`
- 项目主页：https://github.com/Wan-Video/Wan2.1
- 官方网站：https://wan.video
- 开源协议：Apache License 2.0
- 主要语言：Python
- 统计快照：Stars 15365，Forks 2403，Watchers 147（抓取时间：2026-02-22）
- 版本快照：暂无正式发布记录

## 摘要
**1) 一句话总结**
Wan2.1 是一个开源且先进的大规模视频生成基础模型套件，支持文本到视频、图像到视频等多种任务，并能在消费级显卡上高效运行。

**2) 关键要点**
* **开源协议与热度**：项目基于 Python 开发，采用 Apache License 2.0 协议开源，目前在 GitHub 上已获得超 1.5 万 Stars。
* **极低的硬件门槛**：T2V-1.3B 模型仅需 8.19 GB 显存即可运行，在 RTX 4090 上约 4 分钟即可生成 5 秒的 480P 视频（无需量化技术）。
* **多任务与多模型矩阵**：支持文本生成视频 (T2V)、图像生成视频 (I2V)、首尾帧生成视频 (FLF2V) 以及视频编辑 (VACE)；提供 1.3B 和 14B 两种参数规模，覆盖 480P 和 720P 分辨率。
* **双语视觉文本生成**：是首个能够在视频中生成中文和英文视觉文本的视频模型。
* **强大的底层 VAE**：内置 Wan-VAE，能够高效编解码任意长度的 1080P 视频并保留时序信息。
* **生态集成**：模型已成功集成至 Diffusers 和 ComfyUI，并拥有丰富的社区衍生项目（如运动控制、虚拟试衣、自动驾驶世界模型等）。
* **推理与加速方案**：支持单卡推理（提供显存卸载选项以防 OOM）以及基于 FSDP 和 xDiT USP（支持 Ulysses 和 Ring 策略）的多卡加速推理。
* **提示词扩展机制**：官方推荐并支持通过 Dashscope API 或本地 Qwen 模型对输入提示词进行扩展，以丰富生成视频的细节。

**3) 风险/缺口**
* **1.3B 模型高分辨率稳定性不足**：1.3B 模型虽然支持生成 720P 视频，但由于在该分辨率下训练有限，生成结果不如 480P 稳定（官方建议使用 480P）。
* **FLF2V 模型的语言偏好**：首尾帧生成视频 (FLF2V) 模型主要基于中文文本-视频对进行训练，官方建议使用中文提示词以获得更好效果（暗示非中文提示词效果可能受限）。
* **功能缺失**：根据官方 Todo List，目前所有模型（T2V, I2V, FLF2V, VACE）在 Diffusers 框架下的多卡推理 (Multi-GPU Inference) 功能均尚未实现。

## 功能与定位
Wan: Open and Advanced Large-Scale Video Generative Models

## 典型使用场景
- 用于模型训练、微调、推理或文档解析等基础能力建设。
- 作为上层 AI 应用的数据与模型基础设施。

## 核心功能
- 提供模型/推理相关核心能力。
- 支持与主流 AI 工具链协同。
- 兼顾实验验证与工程落地场景。

## 特色与差异点
- 仓库长期活跃，最近更新时间为 2026-02-22T11:22:34Z。
- 项目创建于 2025-02-25T11:49:33Z，具备持续迭代与社区沉淀。
- 以 `Python` 为主语言，聚焦该技术栈的工程实践。

## 使用方式概览
1. 阅读仓库 README 与官方文档，确认适配场景与依赖条件。
2. 按项目推荐方式完成安装与初始化，再从示例或最小流程开始验证。
3. 在生产使用前补齐权限控制、日志监控和版本固定策略。

## 限制与注意事项
- 使用前应先核对许可证、项目维护状态与安全边界。

## 链接
- 仓库：https://github.com/Wan-Video/Wan2.1
- 官网：https://wan.video
- README：https://raw.githubusercontent.com/Wan-Video/Wan2.1/main/README.md
- Releases：https://github.com/Wan-Video/Wan2.1/releases

## 关联主题
- [[00-元语/AI]]
- [[00-元语/video]]
- [[00-元语/multimodal]]
- [[00-元语/github]]
