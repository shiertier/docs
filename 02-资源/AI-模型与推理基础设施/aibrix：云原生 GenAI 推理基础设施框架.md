---
title: "aibrix：云原生 GenAI 推理基础设施框架"
对象: "GitHub 项目"
项目主页: "https://github.com/vllm-project/aibrix"
Stars快照: "4639"
---

## 摘要

**1) 一句话总结**
AIBrix 是一个开源的云原生生成式 AI 推理基础设施框架，作为 vLLM 的 Kubernetes 控制平面，专为优化企业级大语言模型（LLM）的部署、路由、扩缩容及异构硬件管理而设计。

**2) 核心要点**
* **开源协议与定位**：基于 Apache 2.0 协议开源，旨在为构建可扩展的 GenAI 推理基础设施提供核心组件。
* **Kubernetes 原生集成**：深度依赖并集成于 K8s 环境，支持感知 LLM 的负载均衡（LLM-Aware Load Balancing），以优化部署成本与扩展性。
* **流量路由与网关**：内置 LLM 网关，能够在多模型和多副本之间高效调度和管理企业级流量。
* **定制化自动扩缩容**：专为 LLM 应用设计，可根据实时请求需求动态扩展或缩减推理资源。
* **异构 GPU 混合部署**：支持高性价比的异构硬件服务，在保证服务等级目标（SLO）的前提下有效降低推理成本。
* **高级推理优化**：支持高密度 LoRA 管理、分布式推理，以及支持跨引擎复用的大容量分布式 KV 缓存。
* **统一运行时与监控**：提供多功能 Sidecar 用于指标标准化和模型管理，并具备主动检测 GPU 硬件故障的能力。
* **前沿模型支持**：目前已支持 DeepSeek-R1 模型的全权重部署。

## 功能与定位
AIBrix 是一个开源项目，旨在为构建可扩展的生成式 AI（GenAI）推理基础设施提供核心组件。它提供了一种云原生解决方案，专门针对企业级需求量身定制，用于优化大语言模型（LLM）推理的部署、管理和扩展。

## 典型使用场景
* 在 Kubernetes 环境中进行大规模、高性价比的 LLM 推理部署。
* 企业级多模型、多副本的流量路由与管理。
* 需要利用异构 GPU 混合部署以降低成本并保证服务等级目标（SLO）的场景。
* 系统研究场景下的大规模 LLM 推理基础设施构建。

## 核心功能
* **高密度 LoRA 管理**：为轻量级、低秩模型适配提供流线型支持。
* **LLM 网关与路由**：在多个模型和副本之间高效管理和调度流量。
* **定制化自动扩缩容**：专为 LLM 应用设计，可根据实时需求动态扩展推理资源。
* **统一 AI 运行时**：提供多功能 Sidecar，实现指标标准化、模型下载与管理。
* **分布式推理**：具备可扩展架构，支持跨多节点处理大规模工作负载。
* **分布式 KV 缓存**：支持大容量、跨引擎的 KV 缓存复用。
* **高性价比异构服务**：支持混合 GPU 推理，在保证 SLO 的前提下降低成本。
* **GPU 硬件故障检测**：主动检测并发现 GPU 硬件问题。

## 特色与差异点
* **Kubernetes 原生集成**：作为 vLLM 的 Kubernetes 控制平面，支持感知 LLM 的负载均衡（LLM-Aware Load Balancing），优化部署成本与可扩展性。
* **前沿模型支持**：已支持 DeepSeek-R1 全权重部署。

## 使用方式概览
AIBrix 依赖于 Kubernetes 环境。用户可通过克隆官方仓库，并使用 `kubectl` 命令依次在集群中部署 AIBrix 的依赖项和核心组件。官方提供了用于本地测试的 Nightly 版本以及稳定发行版的配置文件。

## 链接

- 仓库：https://github.com/vllm-project/aibrix

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/github]]
- [[00-元语/stream-processing]]
- [[00-元语/wasm]]
