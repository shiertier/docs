---
title: "LMCache：大模型 KV 缓存核心库"
对象: "GitHub 项目"
项目主页: "https://github.com/LMCache/LMCache"
Stars快照: "6921"
---

## 摘要

**1) 一句话总结**
LMCache 是一个通过缓存和跨实例共享大模型 KV Cache 来优化推理性能的核心库，结合 vLLM 使用可显著降低长文本场景下的首字生成时间（TTFT）与整体延迟。

**2) 核心要点**
* **项目数据**：开源于 GitHub，目前拥有 6921 个 Stars。
* **典型应用场景**：主要用于长上下文问答（加速多轮对话响应）以及多实例推理加速（提升整体系统响应效率）。
* **核心功能**：支持单实例的本地 KV 缓存，以及通过独立后端服务器（Backend Server）实现多 vLLM 实例间的跨实例 KV 共享。
* **API 兼容性**：与 OpenAI 客户端库完全兼容，用户可通过标准接口直接调用集成了 LMCache 的 vLLM 引擎。
* **灵活部署架构**：支持从单机单 GPU 的本地缓存模式，到多 GPU、多容器实例下的分布式缓存共享架构。
* **环境与硬件要求**：基础依赖需 Python >= 3.10；运行官方 Docker Demo 时，单实例模式需 1 张 GPU，多实例共享模式需 2 张 GPU。
* **部署方式**：支持通过 `pip` 安装本地源码，或使用官方提供的定制版 Docker 镜像（通过 YAML 配置文件启用）。

**3) 风险与限制**
* **镜像依赖**：运行官方示例强依赖于特定的定制版 Docker 镜像（如 `apostacyh/vllm:lmcache-0.1.0` 和 `apostacyh/lmcache-server:0.1.0`）。
* **配置要求**：启动容器时必须挂载本地的 Huggingface 缓存目录，并要求提供有效的 Huggingface 访问令牌（HF_TOKEN）以拉取模型。

## 功能与定位
LMCache 是一个旨在优化大语言模型推理性能的核心库。它通过缓存和共享模型的 KV Cache（键值缓存），与 vLLM 等推理引擎结合使用，能够大幅降低模型生成首个 Token 的时间（TTFT）以及整体响应延迟。

## 典型使用场景
* **长上下文问答（QA）**：在基于长文本的多次问答交互中，通过复用首次计算的缓存，显著加快第二轮及后续对话的响应速度。
* **多实例推理加速**：在部署了多个 vLLM 推理实例的环境中，通过 LMCache 后端在不同实例间共享前缀 KV Cache，提升整体系统的响应效率。

## 核心功能
* **本地 KV 缓存**：支持在单实例 vLLM 环境中缓存长上下文的 KV 数据，避免重复计算。
* **跨实例 KV 共享**：提供独立的 LMCache 后端服务器（Backend Server），允许不同的 vLLM 实例共享同一个前缀 KV Cache。
* **兼容标准 API**：与 OpenAI 客户端库兼容，用户可通过标准接口直接与集成了 LMCache 的 vLLM 引擎进行交互。

## 特色与差异点
* **显著降低 TTFT**：针对长文本场景，能有效消除重复的上下文处理时间，大幅缩短首字响应延迟。
* **灵活的部署架构**：既支持单机单 GPU 的本地缓存模式，也支持多 GPU、多容器实例下的分布式缓存共享架构。

## 使用方式概览
* **环境依赖**：基础库要求 Python >= 3.10。运行官方 Demo 需要 Docker 环境，单实例快速启动需 1 张 GPU，多实例共享场景需 2 张 GPU。
* **基础安装**：可通过 `pip` 安装本地源码。
* **容器化部署**：
  * **单实例模式**：直接运行官方提供的集成版 vLLM Docker 镜像，并通过 YAML 配置文件启用 LMCache。
  * **多实例模式**：首先启动 LMCache 后端服务器容器，随后启动多个 vLLM 实例容器并接入该后端。
* **客户端调用**：启动服务后，使用 Python 的 `openai` 库连接对应端口即可进行对话测试。

## 限制与注意事项
* 运行官方示例需要依赖特定的定制版 Docker 镜像（如 `apostacyh/vllm:lmcache-0.1.0` 和 `apostacyh/lmcache-server:0.1.0`）。
* 启动容器时需要挂载本地的 Huggingface 缓存目录，并提供有效的 Huggingface 访问令牌（HF_TOKEN）以便拉取模型。

## 链接

- 仓库：https://github.com/LMCache/LMCache

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/github]]
- [[00-元语/stream-processing]]
- [[00-元语/wasm]]
