# vLLM：高吞吐低延迟 LLM 推理与服务引擎

## 文档信息
- 发布日期：2026-02-22
- 对象：GitHub 项目 `vllm-project/vllm`
- 项目主页：https://github.com/vllm-project/vllm
- 官方网站：https://vllm.ai
- 开源协议：Apache-2.0
- 主要语言：Python
- 统计快照：Stars 70885，Forks 13604，Watchers 493（抓取时间：2026-02-22）
- 版本快照：最新 release `v0.16.0`（prerelease），发布时间 `2026-02-13`
- 备注：用户提供“24912”，与当前公开 Stars 统计不一致；本条目按抓取时公开数据记录。

## 摘要
**1) 一句话总结**
vLLM 是一个源自加州大学伯克利分校的高吞吐量、内存高效的大语言模型推理与服务引擎，支持广泛的异构硬件平台并与 Hugging Face 生态深度集成。

**2) 核心要点**
*   **项目状态**：采用 Apache-2.0 开源协议，主要使用 Python（底层含 C/C++）开发，最新预发布版本为 v0.16.0（2026-02-13 发布）。
*   **核心内存技术**：采用源自 SOSP 2023 学术论文的 PagedAttention 技术，实现对注意力键值（KV）缓存的高效管理。
*   **高吞吐量调度与优化**：支持连续批处理（Continuous batching）和分块预填充（Chunked prefill），底层利用 CUDA/HIP 图加速并集成 FlashAttention 与 FlashInfer。
*   **广泛的模型支持**：无缝部署 Hugging Face 上的主流开源模型，涵盖 Transformer、混合专家模型（如 Mixtral、Deepseek-V2/V3）、嵌入模型及多模态大模型（如 LLaVA）。
*   **量化与解码能力**：兼容 GPTQ、AWQ、INT4/8、FP8 等多种量化格式；支持并行采样、束搜索、流式输出及投机解码（Speculative decoding）。
*   **高级推理特性**：原生支持前缀缓存（Prefix caching）、多 LoRA 并发推理，以及张量、流水线、数据和专家并行等分布式推理技术。
*   **跨硬件兼容性**：除 NVIDIA GPU 外，原生支持 AMD、Intel、PowerPC、Arm 及 TPU，并通过插件机制扩展支持 Intel Gaudi、IBM Spyre 和华为昇腾等硬件。
*   **服务部署**：可直接作为 OpenAI 兼容的 API 服务器运行，支持通过 `pip install vllm` 快速安装。

**3) 风险/差异点**
*   **数据不一致**：文档指出当前 GitHub 公开 Stars 数量为 70885，与用户提供的参考数字 24912 存在不一致。

## 功能与定位
vLLM 是一个快速且易于使用的大语言模型（LLM）推理与服务引擎。该项目最初由加州大学伯克利分校的 Sky Computing Lab 开发，现已演变为由学术界和工业界共同参与的社区驱动项目。其核心定位是为用户提供简单、快速且低成本的 LLM 服务。

## 典型使用场景
- **开源模型部署**：无缝部署 Hugging Face 上的主流开源模型，包括 Transformer 类模型（如 Llama）、混合专家模型（MoE，如 Mixtral、Deepseek-V2/V3）、嵌入模型（如 E5-Mistral）以及多模态大模型（如 LLaVA）。
- **API 服务搭建**：作为 OpenAI 兼容的 API 服务器运行，方便直接替换现有应用后端的模型服务。
- **分布式推理**：在多设备环境下，利用张量、流水线、数据及专家并行技术进行大规模模型推理。

## 核心功能
- **高效内存管理**：采用 PagedAttention 技术，实现对注意力键值（KV）缓存内存的高效管理。
- **高吞吐量调度**：支持对传入请求进行连续批处理（Continuous batching）以及分块预填充（Chunked prefill）。
- **底层执行优化**：利用 CUDA/HIP 图加速模型执行，并集成了 FlashAttention 和 FlashInfer 等优化的 CUDA 内核。
- **模型量化支持**：兼容多种量化格式和精度，包括 GPTQ、AWQ、AutoRound、INT4、INT8 和 FP8。
- **灵活的解码与输出**：支持并行采样（parallel sampling）、束搜索（beam search）等多种解码算法，并支持流式输出和投机解码（Speculative decoding）。
- **高级特性**：原生支持前缀缓存（Prefix caching）和多 LoRA（Multi-LoRA）并发推理。

## 特色与差异点
- **广泛的硬件兼容性**：不仅支持 NVIDIA GPU，还支持 AMD CPU/GPU、Intel CPU/GPU、PowerPC CPU、Arm CPU 以及 TPU。此外，通过硬件插件机制，可扩展支持 Intel Gaudi、IBM Spyre 和华为昇腾（Huawei Ascend）等异构硬件。
- **学术创新转化**：其核心的 PagedAttention 内存管理技术源自 SOSP 2023 的学术论文，在提升吞吐量方面具有显著的理论与工程优势。
- **生态集成度高**：与 Hugging Face 生态深度绑定，开箱即用。

## 使用方式概览
- **安装**：可通过 Python 包管理器直接安装（`pip install vllm`），或通过源码编译构建。
- **文档与指引**：官方提供了详细的快速入门指南和支持模型列表，便于开发者快速上手。

## 链接
- **GitHub 仓库**: https://github.com/vllm-project/vllm
- **项目官网**: https://vllm.ai
- **官方文档**: https://docs.vllm.ai
- **学术论文**: https://arxiv.org/abs/2309.06180
- **最新 Release**: https://github.com/vllm-project/vllm/releases/tag/v0.16.0
- **开发者 Slack**: https://slack.vllm.ai
- **用户论坛**: https://discuss.vllm.ai

## 限制与注意事项
- 建议以项目 README、官方文档与 release 说明作为落地依据。
- 生产使用前应在目标环境验证兼容性、性能与安全边界。

## 关联主题
- [[00-元语/llm]]
- [[00-元语/llmops]]
- [[00-元语/lora]]
- [[00-元语/multimodal]]
- [[00-元语/paper]]
- [[00-元语/self-hosting]]
- [[00-元语/benchmark]]
- [[00-元语/github]]
