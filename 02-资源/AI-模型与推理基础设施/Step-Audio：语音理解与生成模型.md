---
title: "Step-Audio：语音理解与生成模型"
发布日期: "2026-02-23"
对象: "GitHub 项目 `stepfun-ai/Step-Audio`"
项目主页: "https://github.com/stepfun-ai/Step-Audio"
开源协议: "Apache License 2.0"
主要语言: "Python"
统计快照: "Stars 4613，Forks 370，Watchers 42（抓取时间：2026-02-24）"
---

## 摘要

### 1) 一句话总结
Step-Audio 是一个集成了语音理解与生成的生产级开源智能语音交互框架（现已停止维护），通过 1300 亿参数的对话模型和 30 亿参数的 TTS 模型，提供多语种对话、声音克隆及细粒度的语音控制能力。

### 2) 核心要点
* **项目状态**：基于 Python 开发，采用 Apache 2.0 协议开源。**该仓库现已停止维护**，官方建议转向 Step-Audio2、Step-Audio-R1 等后续项目。
* **核心模型架构**：包含 1300 亿参数的多模态大语言模型（Step-Audio-Chat，集成 ASR、理解、对话、克隆和合成）以及 30 亿参数的轻量化语音合成模型（Step-Audio-TTS-3B）。
* **音频处理机制**：采用双码本音频分词器（16.7Hz 语义与 25Hz 声学分词按 2:3 时间交错），并结合流匹配与神经声码器构成混合语音解码器。
* **多维度语音控制**：支持中、英、日多语种对话，能够精准调节生成语音的情绪、方言（如粤语、四川话）、语速及特殊发声风格（如说唱、哼唱）。
* **低延迟实时管道**：专为实时交互设计，包含语音活动检测（VAD）、流式分词、上下文管理（14:1 压缩比）及投机性响应生成（40% 提交率）。
* **生成式数据引擎**：利用 130B 多模态大模型生成高质量音频数据，用于训练 3B 参数的 TTS 模型，打破了传统 TTS 对人工采集数据的依赖。
* **部署与使用**：支持离线推理、TTS 推理和 Web Demo，提供标准及 vLLM 专用的 Docker 镜像以简化环境配置。

### 3) 风险与不足
* **项目已废弃**：当前版本不再维护，使用者需自行评估是否直接采用官方推荐的新版本。
* **极高的硬件门槛**：核心对话模型（Step-Audio-Chat）参数量达 130B，单机部署最低需 265GB 显存（官方推荐使用 4 张 80GB 的 A800/H800 GPU）。
* **vLLM 兼容性与功能限制**：
  * 官方 vLLM 暂不支持该模型架构，必须使用官方提供的特定开发分支。
  * 因模型使用 ALIBI 注意力机制变体，与官方 Flash Attention 不兼容，需配置并使用官方提供的自定义库。
  * 在 vLLM 部署模式下，模型**不支持音频输入推理**（仅支持文本输入）。

## 功能与定位

Step-Audio 是一个生产级的开源智能语音交互框架，统一了语音理解与生成能力。该框架支持多语种对话（中文、英文、日文）、情感语调调节、方言（如粤语、四川话）、语速调节以及特殊韵律风格（如说唱），旨在满足多样化的语音生成与交互需求。

## 典型使用场景

* **端到端多语种语音对话**：支持跨语言的实时语音交互与角色扮演。
* **高精度语音合成与声音克隆（TTS）**：根据提供的音频提示（Prompt）克隆特定音色，或使用默认音色生成语音。
* **复杂任务处理**：结合 ToolCall 机制，提升智能体在复杂任务中的表现。

## 核心功能

* **多模态大语言模型 (Step-Audio-Chat)**：基于 1300 亿参数的文本大模型（Step-1）进行音频上下文持续预训练和任务微调，单模型集成了语音识别（ASR）、语义理解、对话、声音克隆和语音合成能力。
* **语音合成模型 (Step-Audio-TTS-3B)**：30 亿参数的轻量化 TTS 模型，具备强指令遵循能力，支持细粒度的声音控制。
* **双码本音频分词器 (Step-Audio-Tokenizer)**：结合语义分词（16.7Hz，1024 词表）与声学分词（25Hz，4096 词表），采用 2:3 的时间交错比例对音频流进行标记化处理。
* **混合语音解码器**：结合流匹配（Flow Matching）与神经声码器（Neural Vocoding），将包含语义和声学信息的离散 Token 转换为连续的自然语音波形。
* **实时推理管道**：包含语音活动检测（VAD）、流式音频分词、上下文管理（14:1 压缩比）以及投机性响应生成（40% 提交率），专为低延迟实时交互设计。

## 特色与差异点

* **生成式数据引擎**：打破传统 TTS 对人工采集数据的依赖，利用 130B 多模态大模型生成高质量音频数据，并以此训练出高效的 3B 参数 TTS 模型。
* **细粒度语音控制**：通过指令控制设计，能够精准调节生成语音的情绪（如愤怒、快乐、悲伤）、方言以及发声风格（如说唱、清唱哼唱）。
* **Token 级交错架构**：在分词与解码阶段无缝整合语义与声学特征，有效提升了合成语音的清晰度与自然度。

## 使用方式概览

* **硬件要求**：需配备支持 CUDA 的 NVIDIA GPU（操作系统需为 Linux）。
  * `Step-Audio-Tokenizer`：最低 1.5GB 显存。
  * `Step-Audio-TTS-3B`：最低 8GB 显存。
  * `Step-Audio-Chat`：最低 265GB 显存（推荐使用 4 张 80GB 的 A800/H800 GPU 以保证生成质量）。
* **推理模式**：
  * **离线推理**：支持端到端的音频/文本输入与输出。
  * **TTS 推理**：支持默认音色合成及基于参考音频的声音克隆。
  * **Web Demo**：提供本地服务器脚本，支持在线交互演示。
* **vLLM 部署（推荐用于 Chat 模型）**：支持使用 vLLM 进行张量并行推理，以提升 130B 模型的运行效率。
* **容器化支持**：官方提供标准 Dockerfile 及专用于 vLLM 的 Dockerfile，简化环境依赖配置。

## 限制与注意事项

* **项目已废弃**：当前版本已不再维护，使用者应评估是否直接采用官方推荐的 Step-Audio2 等新版本。
* **极高的硬件门槛**：核心对话模型（Step-Audio-Chat）参数量达 130B，单机部署需要极高的显存资源（>265GB）。
* **vLLM 兼容性限制**：
  * 官方 vLLM 暂不支持 Step 1 模型架构，需使用官方提供的特定开发分支。
  * 模型使用了 ALIBI 注意力机制的变体，与官方 Flash Attention 不兼容，必须在环境变量中配置并导出官方提供的自定义 Flash Attention 库。
  * 在 vLLM 部署模式下，由于不加载 Tokenizer 和 TTS 模块，模型**不支持音频输入推理**（仅支持文本）。

## 链接

- GitHub 仓库：https://github.com/stepfun-ai/Step-Audio
- 技术报告 (Arxiv)：https://arxiv.org/abs/2502.11946
- Chat 模型：https://huggingface.co/stepfun-ai/Step-Audio-Chat
- TTS 模型：https://huggingface.co/stepfun-ai/Step-Audio-TTS-3B
- Tokenizer：https://huggingface.co/stepfun-ai/Step-Audio-Tokenizer
- 评测数据集：https://huggingface.co/datasets/stepfun-ai/StepEval-Audio-360
- Chat 模型：https://modelscope.cn/models/stepfun-ai/Step-Audio-Chat
- TTS 模型：https://modelscope.cn/models/stepfun-ai/Step-Audio-TTS-3B

## 关联主题

- [[00-元语/AI]]
- [[00-元语/audio]]
- [[00-元语/asr]]
- [[00-元语/github]]
