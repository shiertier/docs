---
title: "Kimi-Audio：开源通用音频基础大模型"
对象: "GitHub 项目"
项目主页: "https://github.com/MoonshotAI/Kimi-Audio"
Stars快照: "4492"
---

## 摘要

**1) 一句话总结**
Kimi-Audio 是由 MoonshotAI 开发并开源的通用音频基础大模型，通过单一统一框架实现了强大的音频理解、音频生成及端到端语音对话能力。

**2) 关键要点**
* **开发与开源生态**：由 MoonshotAI 开发，全面开源了预训练权重（Kimi-Audio-7B）、指令微调权重（Kimi-Audio-7B-Instruct）、推理/微调代码以及专属评估工具包（Evalkit）。
* **超大规模预训练**：模型在超过 1300 万小时的多样化音频数据（涵盖语音、音乐、环境音）及文本数据上进行了预训练。
* **模型架构**：基于 Transformer 架构（以 Qwen 2.5 7B 等预训练文本模型为初始化），通过共享层处理多模态输入。
* **多模态输入处理**：音频分词器（Tokenizer）将音频转化为 12.5Hz 的离散语义 token 和基于 Whisper 编码器降采样的连续声学特征。
* **双轨并行生成**：利用并行头自回归机制，能够同时生成文本 token 和离散音频语义 token。
* **低延迟流式音频生成**：内置音频解调器（Detokenizer）结合流匹配模型与 BigVGAN 声码器，支持带有前瞻机制的分块流式处理，还原高保真音频。
* **多场景任务支持**：支持自动语音识别（ASR）、音频问答/字幕生成、语音情感与场景分析，以及单轮/多轮的端到端语音对话交互。
* **SOTA 性能表现**：在 LibriSpeech、Fleurs、AISHELL、WenetSpeech 等多个主流音频基准测试中取得领先成绩。
* **API 与调用方式**：提供 Python API（`KimiAudio` 类），采用标准的消息列表格式（role/message_type/content），支持自定义采样参数并可指定输出纯文本或音文双输出。

## 功能与定位
Kimi-Audio 是一个开源的通用音频基础大模型，旨在通过单一的统一框架处理多种音频任务。该模型在音频理解、音频生成以及端到端语音对话方面具备强大的能力。

## 典型使用场景
* **自动语音识别（ASR）**：将输入的音频精准转换为文本。
* **音频问答（AQA）与音频字幕生成（AAC）**：理解音频内容并回答相关问题或生成描述。
* **语音情感与场景分析**：用于语音情感识别（SER）以及声音事件/场景分类（SEC/ASC）。
* **端到端语音对话**：支持单轮及多轮的语音到语音、语音到文本的自然对话交互。

## 核心功能
* **多模态混合输入处理**：音频分词器（Tokenizer）能够将输入音频同时转化为离散的语义 token（12.5Hz，基于向量量化）和连续的声学特征（基于 Whisper 编码器降采样至 12.5Hz）。
* **双轨并行生成**：基于 Transformer 架构的音频大语言模型（以 Qwen 2.5 7B 等预训练文本模型为初始化），通过共享层处理多模态输入，并利用并行头自回归地同时生成文本 token 和离散音频语义 token。
* **低延迟流式音频生成**：内置音频解调器（Detokenizer），结合流匹配（flow-matching）模型与 BigVGAN 声码器，支持带有前瞻机制的分块流式处理，从而将预测的离散 token 还原为高保真音频波形。

## 特色与差异点
* **超大规模预训练**：模型在超过 1300 万小时的多样化音频数据（涵盖语音、音乐、环境音）及文本数据上进行了预训练，具备极强的音频推理与语言理解底座能力。
* **广泛的 SOTA 性能**：在多个主流音频基准测试中（如 LibriSpeech、Fleurs、AISHELL、WenetSpeech、MMAU、OpenAudioBench 等）展现出领先的性能表现。
* **高度开源与生态支持**：不仅开源了预训练权重（Kimi-Audio-7B）和指令微调权重（Kimi-Audio-7B-Instruct），还提供了完整的推理代码、微调示例以及专门的音频评估工具包（Kimi-Audio-Evalkit），方便开发者复现结果与二次开发。

## 使用方式概览
* **安装部署**：支持通过 `git clone` 获取源码并安装依赖，或直接通过 `pip install git+https://...` 的方式快捷安装。
* **代码调用**：提供 Python API（`KimiAudio` 类）。开发者可以加载模型及解调器，并自定义音频与文本的采样参数（如温度、Top-K、重复惩罚等）。
* **消息构建与推理**：采用类似主流大模型的消息列表格式（包含 `role`、`message_type` 和 `content`）。支持传入纯文本指令与音频文件路径，通过 `generate` 方法指定输出类型（纯文本 `text` 或音文双输出 `both`），实现语音识别或多轮语音对话。

## 链接

- 仓库：https://github.com/MoonshotAI/Kimi-Audio

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/github]]
- [[00-元语/stream-processing]]
- [[00-元语/wasm]]
