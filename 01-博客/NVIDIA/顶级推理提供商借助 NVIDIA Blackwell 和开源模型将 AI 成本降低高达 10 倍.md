---
title: "顶级推理提供商借助 NVIDIA Blackwell 和开源模型将 AI 成本降低高达 10 倍"

来源: "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/"
---

## 摘要

**1) 一句话总结**
领先的 AI 推理提供商通过采用 NVIDIA Blackwell 平台和开源模型，在医疗、游戏、智能体和客服等领域将 AI Token 成本降低了高达 10 倍，并显著提升了系统响应速度。

**2) 关键要点**
* **行业趋势**：麻省理工学院（MIT）研究表明，基础设施和算法的效率提升正使前沿 AI 推理成本每年降低高达 10 倍。
* **医疗保健（Baseten & Sully.ai）**：Sully.ai 采用 Baseten 部署在 Blackwell 上的开源模型，使推理成本下降 90%（缩减 10 倍），响应时间缩短 65%。Blackwell 的每美元吞吐量比 Hopper 高出 2.5 倍。
* **游戏（DeepInfra & Latitude）**：Latitude 通过 DeepInfra 平台迁移至 Blackwell 和低精度 NVFP4 格式，将大型混合专家（MoE）模型的每百万 Token 成本从 20 美分（Hopper 平台）降至 5 美分，实现 4 倍成本改善。
* **智能体聊天（Fireworks AI & Sentient Foundation）**：Sentient 采用 Fireworks AI 优化的推理栈，成本效率比基于 Hopper 的部署提高了 25% 至 50%，成功支撑了单周 560 万次查询及 180 万候补用户的流量激增。
* **客户服务（Together AI & Decagon）**：Decagon 结合 Together AI 的投机解码和缓存优化，在 Blackwell 上实现了 400 毫秒以内的语音响应，单次查询总成本比使用闭源专有模型降低了 6 倍。
* **硬件与未来规划**：NVIDIA GB200 NVL72 系统在推理 MoE 模型时比 Hopper 降低了 10 倍 Token 成本；未来的 Rubin 平台计划提供比 Blackwell 高 10 倍的性能，并将 Token 成本再降低 10 倍。

**3) 风险/缺口**
* **闭源模型瓶颈**：在医疗场景中，闭源模型曾导致实时临床工作流中出现不可预测的延迟、成本增长快于收入增长，以及企业对模型质量和更新的控制力不足。
* **扩展性与成本冲突**：在游戏和多智能体工作流中，用户交互的增加会触发大量推理请求，若无底层优化，会导致高昂的基础设施开销。
* **延迟敏感性风险**：在语音 AI 客服场景中，即使是轻微的延迟也会导致用户打断智能体、挂断电话或失去信任。

## 正文

医疗领域的诊断洞察、互动游戏中的角色对话、客服智能体的自主回复——这些由 AI 驱动的交互都建立在同一个智能单元之上：Token。

随着 AI 交互规模的不断扩大，企业必须考虑他们是否能够承担更多 Token 的成本。解决这一问题的关键在于优化“代币经济学（Tokenomics）”，其核心即降低每个 Token 的成本。这种成本下降的趋势正在各行各业中显现。麻省理工学院（MIT）的最新研究表明，基础设施和算法的效率提升，正使前沿性能的 AI 推理成本每年降低高达 10 倍。

要理解基础设施效率如何改善代币经济学，可以将其比作高速印刷机。如果只需增加少量的墨水、能源和机器投资，印刷机就能产出 10 倍的内容，那么单页的印刷成本就会大幅下降。同理，对 AI 基础设施的投资可以带来远超成本增长的 Token 产出，从而显著降低单个 Token 的成本。

正因如此，包括 Baseten、DeepInfra、Fireworks AI 和 Together AI 在内的领先推理提供商，正采用 NVIDIA Blackwell 平台。与 NVIDIA Hopper 平台相比，Blackwell 帮助他们将每个 Token 的成本降低了高达 10 倍。

这些提供商托管着目前已达到前沿智能水平的高级开源模型。通过将前沿的开源智能、NVIDIA Blackwell 极致的软硬件协同设计以及各自优化的推理栈相结合，他们正在为各行各业的企业实现 Token 成本的大幅削减。

### 医疗保健：Baseten 与 Sully.ai 将推理成本降低 10 倍

在医疗保健领域，医疗编码、文书记录和管理保险单等繁琐耗时的任务，严重挤占了医生与患者交流的时间。

Sully.ai 通过开发能够处理医疗编码和记录等日常任务的“AI 员工”来解决这一问题。随着平台规模的扩大，其专有的闭源模型带来了三大瓶颈：实时临床工作流中不可预测的延迟、成本增长速度超过收入增长，以及对模型质量和更新的控制力不足。

为了突破这些瓶颈，Sully.ai 采用了 Baseten 的 Model API，在 NVIDIA Blackwell GPU 上部署了 gpt-oss-120b 等开源模型。Baseten 利用低精度 NVFP4 数据格式、NVIDIA TensorRT-LLM 库和 NVIDIA Dynamo 推理框架来提供优化的推理服务。在发现 Blackwell 平台每美元的吞吐量比 Hopper 平台高出 2.5 倍后，Baseten 决定选择 Blackwell 来运行其 Model API。

**最终成果：**
* Sully.ai 的推理成本下降了 90%，与之前的闭源方案相比实现了 10 倍的成本缩减。
* 生成医疗记录等关键工作流的响应时间缩短了 65%。
* 该公司已为医生节省了超过 3000 万分钟原本用于数据录入和其他手动任务的时间。

### 游戏：DeepInfra 与 Latitude 将 Token 成本降低 4 倍

Latitude 正在通过其冒险故事游戏《AI Dungeon》以及即将推出的 AI 角色扮演游戏平台《Voyage》构建 AI 原生游戏的未来。在这些世界中，玩家可以自由选择行动并创造自己的故事。

该平台使用大语言模型来响应玩家的动作，但这带来了扩展性挑战：玩家的每一个动作都会触发一次推理请求。成本随着玩家参与度的提高而增加，同时响应时间必须足够快以保持无缝的游戏体验。

Latitude 在 DeepInfra 的推理平台上运行大型开源模型，该平台由 NVIDIA Blackwell GPU 和 TensorRT-LLM 提供支持。

**最终成果：**
* 对于大型混合专家（MoE）模型，DeepInfra 将每百万 Token 的成本从 Hopper 平台上的 20 美分降至 Blackwell 上的 10 美分。
* 迁移到 Blackwell 原生的低精度 NVFP4 格式后，成本进一步降至仅 5 美分（Token 成本总计改善 4 倍），同时保持了客户期望的准确性。
* DeepInfra 的平台能够可靠地应对流量激增，使 Latitude 能够在不牺牲玩家体验的情况下部署更强大的模型。

### 智能体聊天：Fireworks AI 与 Sentient Foundation 将 AI 成本降低高达 50%

Sentient Labs 致力于汇聚 AI 开发者，共同构建强大的、完全开源的推理 AI 系统。其目标是通过在安全自治、智能体架构和持续学习方面的研究，加速 AI 解决更复杂的推理问题。

其首款应用 Sentient Chat 负责协调复杂的多智能体工作流，并整合了来自社区的十几个专业 AI 智能体。由于单个用户查询可能会触发一系列自主交互，通常会导致高昂的基础设施开销，因此 Sentient Chat 具有庞大的计算需求。

为了管理这种规模和复杂性，Sentient 采用了运行在 NVIDIA Blackwell 上的 Fireworks AI 推理平台。

**最终成果：**
* 借助 Fireworks 针对 Blackwell 优化的推理栈，Sentient 的成本效率比之前基于 Hopper 的部署提高了 25% 到 50%。
* 单 GPU 吞吐量的提升使公司能够在同等成本下服务更多并发用户。
* 平台的高扩展性成功支撑了 24 小时内 180 万候补用户的病毒式发布，并在单周内处理了 560 万次查询，同时保持了持续的低延迟。

### 客户服务：Together AI 与 Decagon 将成本降低 6 倍

语音 AI 客服电话常常以用户的挫败感告终，因为即使是轻微的延迟，也会导致用户打断智能体、挂断电话或失去信任。

Decagon 为企业客户支持构建 AI 智能体，其中 AI 语音是要求最高的渠道。Decagon 需要一种能够在不可预测的流量负载下提供亚秒级响应的基础设施，并且其代币经济学必须能够支撑 24/7 的语音部署。

Together AI 在 NVIDIA Blackwell GPU 上为 Decagon 的多模型语音栈运行生产推理。双方合作进行了多项关键优化：采用投机解码（训练较小的模型快速生成响应，同时由较大的模型在后台验证准确性）、缓存重复的对话元素以加快响应速度，以及构建自动扩展机制以在不降低性能的情况下应对流量激增。

**最终成果：**
* 即使每次查询处理数千个 Token，Decagon 的响应时间也控制在 400 毫秒以内。
* 通过结合 Decagon 的多模型方法（部分开源，部分在 NVIDIA GPU 上内部训练）、NVIDIA Blackwell 的极致协同设计以及 Together 优化的推理栈，完成一次语音交互的单次查询总成本比使用闭源专有模型降低了 6 倍。

### 极致协同设计优化“代币经济学”

医疗保健、游戏和客户服务领域显著的成本节约，均得益于 NVIDIA Blackwell 的高效性。NVIDIA GB200 NVL72 系统进一步扩大了这一影响，与 NVIDIA Hopper 相比，在推理 MoE 模型时，它实现了突破性的 10 倍 Token 成本降低。

NVIDIA 在计算、网络和软件等堆栈各个层面的极致协同设计，结合其合作伙伴生态系统，正在大规模解锁 Token 成本的大幅下降。

这一势头将在 NVIDIA Rubin 平台上继续延续——该平台将六款新芯片集成到一台 AI 超级计算机中，旨在提供比 Blackwell 高 10 倍的性能，并将 Token 成本再降低 10 倍。

## 相关文档

- [[01-博客/NVIDIA/NVIDIA Blackwell Ultra：为代理型 AI 带来 50 倍性能提升与 35 倍成本降低|NVIDIA Blackwell Ultra：为代理型 AI 带来 50 倍性能提升与 35 倍成本降低]]；关联理由：版本演进；说明：同属 NVIDIA 在推理经济性上的连续叙事，且都以 Blackwell 对比 Hopper 量化成本下降。
- [[01-博客/Towards Data Science/构建企业级本地 AI 的 GPUaaS 架构|构建企业级本地 AI 的 GPUaaS 架构]]；关联理由：延伸思考；说明：该文给出按每百万 Token 计算成本的工程方法，可用于补充本文的落地测算视角。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/Agent]]
- [[00-元语/llmops]]
- [[00-元语/benchmark]]
