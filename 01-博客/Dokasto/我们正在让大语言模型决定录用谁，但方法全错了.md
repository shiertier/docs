---
title: "我们正在让大语言模型决定录用谁，但方法全错了"
---

## 摘要

**1) 一句话总结**
针对当前在招聘中滥用大语言模型（LLM）直接评估候选人的乱象，本文提出了一套基于结构化面试、行为锚定评分标准和“先引用后发言”证据提取机制的可审计AI辅助招聘工作流。

**2) 关键要点**
*   **现状与痛点**：招聘人员正使用低成本（20美元）的LLM直接处理面试记录，并做出高价值（20万美元）的录用决定，全程缺乏评分标准、审计和监督。
*   **核心理念**：解决问题的关键不在于禁用LLM，而是将流程的基础建立在结构化面试本身，而非依赖模型进行主观评判。
*   **第一步：结构化面试**：对同一职位的候选人提出一致的问题，使用相同的量表进行评分，并根据预先确定的资格标准做出决定。
*   **第二步：行为锚定评分**：推荐使用行为总结量表（BSS），将明确的评分等级（如差、一般、好、优秀）与具体的行为表现示例绑定，以减少面试官之间的评分差异。
*   **第三步：基于证据的提取**：LLM的任务应从“评判候选人”转变为“提取证据”。模型必须从面试材料中提取最短的支持片段，并返回确切的文字记录行号链接。若无证据，必须回答“证据不足”（即“先引用，后发言”原则）。
*   **交叉验证机制**：可使用“主张溯源率（CGR）”衡量模型的证据溯源能力，并建议引入由2-3个不同模型组成的“LLM委员会”进行交叉验证，当模型产生分歧时引入人工审查。
*   **第四步：应用修正的评分模型**：建议采用带有“底线”的补偿模型（联合-补偿模型）。允许优势弥补劣势，但设定硬性底线（例如：若找到的证据中有至少50%被评为“差”，则强制将该领域评为“差”）。
*   **工具实践**：现有会议平台（Zoom、Teams等）不适合AI面试，作者开发了ScreenStack原型，通过整合文字记录、代码、白板及代码快照，全面捕获候选人解决问题的过程以支持上述工作流。

**3) 风险与缺陷**
*   **幻觉细节**：LLM可能会自信地捏造不存在的面试“证据”。
*   **冗长偏差（冗长攻击）**：模型倾向于给较长的回答打高分，即使这些回答缺乏实质内容。
*   **位置与框架偏差**：模型通常偏好先看到的选项，且极易被操纵，提示词的微小改变（或选项顺序的交换）即可完全反转评估结果。
*   **转录错误级联**：语音转文本过程中的错误会向下游传播，从而导致最终的评估决策错误。
*   **纯补偿模型风险**：在评分阶段若仅使用纯补偿模型（加权求和），可能会通过平均化掩盖严重的“差评”信号，导致团队雇佣“有毒的高绩效者”。

## 正文

面试官结束了通话。他们打开面试的文字记录，将其与评估标准一起粘贴到聊天机器人中，然后问：“这位候选人表现如何？”

一段干净、自信且权威的段落生成了。面试官将其复制到招聘系统（Greenhouse）并提交。

这是一个价值20万美元的决定，却由一个20美元的聊天机器人做出，没有评分标准，没有审计，也没有监督。招聘室里多了一个“新人”，没人面试过它，没人培训过它，也没人能解雇它。

我曾花了400多个小时面试软件工程候选人。直到最近，我才开始使用大语言模型（LLM）来辅助起草反馈，我也完全理解这为何如此诱人。在背靠背的连续面试后写笔记非常乏味，而且随着时间流逝，面试的上下文会被遗忘。但并非只有我这么做。随着转录工具的普及，LLM辅助评估正成为面试官武器库中的常规工具。

在工作之外的一次模拟面试中，我亲身体会到了这种风险。我用ChatGPT转录了面试过程，将同一份记录放入评估模板中，结果发现面试结果极易被操纵。只需稍微调整提示词，同样的表现既可以被描述为“令人印象深刻”，也可以被说成“表现不佳”。

然而，完全不使用转录或LLM辅助也不是个好选择。边记笔记边进行有意义的对话很难，如果不当场记录，你遗忘的细节会比想象的更多。

因此，问题不在于是否应该在招聘中使用LLM，而在于如何围绕它们设计一个可审计、负责任且合理的流程。

过去几周，我深入研究了这个问题。我没想到这会让我从工业与组织心理学一路看到arXiv上的计算机科学论文，但这确实令人兴奋。将这些领域的知识结合起来，让我初步构建了一个能让LLM辅助招聘更安全、更一致且可审计的方法。

### 当前鲁莽且天真的做法

“面试记录 + 评估标准 + LLM提示词 => 将输出结果粘贴到招聘系统”。这种方法虽然快，但充满了可预见的失败模式：

*   **幻觉细节**：LLM会自信地捏造“证据” [1]。
*   **冗长偏差**：较长的回答往往会被评为更好，即使它们缺乏实质内容（即“冗长攻击”） [1]。
*   **位置与框架偏差**：如果向模型展示两个选项（A与B），它通常偏好先看到的那个；交换顺序，结论可能反转。即使是提问方式的微小改变也会改变结果 [1]。
*   **转录错误导致决策错误**：语音转文本的错误会向下游传播，从而改变评估结果 [2]。

### 我的建议：结构化且可审计的工作流

这一流程的基础在于面试本身，而不是模型。

#### 1. 从结构化面试开始
谷歌的指导原则很简单：向申请同一职位的候选人提出一致的问题，使用相同的量表进行评分，并根据预先确定的资格标准做出决定 [4]。他们还发布了面试评分标准示例，展示了不同“等级”在实践中的具体表现 [8]。评分标准之所以重要，是因为它能减少由面试官（而非候选人）带来的评分差异。

#### 2. 使用行为锚定评分标准
这并不是一个新概念。工业与组织心理学有三种密切相关的评分标准构建方法：
*   **行为锚定等级评价法（BARS）**：将每个分数级别与“好”和“坏”的具体表现示例绑定 [5]。
*   **行为观察量表（BOS）**：要求面试官对观察到特定行为的频率进行评分（例如，“澄清需求”或“测试假设”） [6]。
*   **行为总结量表（BSS）**：BARS的简化版，使用简短的行为总结（而不是大量详细的锚点），使不同面试官更容易保持一致 [7]。

对于面试场景，我认为BSS是最合适的。面试反馈往往充满噪音、时间紧迫，且常引起评审人员的争议。少数几个描述清晰的等级（例如：差、一般、好、优秀）更容易在面试官之间进行校准，也更容易在事后进行审计。

#### 3. 提取有根据的证据（“先引用，后发言”）
此时，LLM的任务不再是“评判候选人”，而是将面试记录和产出物与评分标准中的行为描述进行匹配。对于每个等级下的每种行为（例如，“难以将问题分解为多个步骤或选择可行的方案”），模型应从面试材料（文字记录、代码和任何图表）中提取最佳的支持证据，并返回确切的文字记录行号链接，以便人工评审员验证该主张。

如果找不到证据，它必须回答“证据不足”，而不是进行猜测。这种“先引用，后发言”的约束使得输出更难被操纵，也更容易审计：每一个评分都有可验证的依据 [3]。

提示词的构建在这里至关重要。应明确指示模型提取能证明该行为的最短记录片段。这可以缓解冗长偏差。

并非所有模型的证据溯源能力都一样，因此你可以对其进行测量。Zeng等人提出了“主张溯源率”（Claim Grounding Rate, CGR）：即由证据集支持的主张所占的比例（越高越好） [3]。

你还可以进一步优化。受Andrej Karpathy的“LLM委员会”启发，可以使用两到三个不同的模型（最好来自不同的供应商）来运行证据提取。当它们都独立地为某个行为引用了相同的文字记录行时，这就是一个强烈的信号，表明证据是真实的。当它们产生分歧时，你就为“人在回路”的人工审查提供了切入点。

#### 4. 应用评分模型
收集证据后，评分就成了一个策略选择问题。招聘团队必须决定如何将行为级别的证据转化为每个考察领域的评分，然后将这些领域综合成最终决定。

在工业与组织心理学中，综合信号时经常出现几种常见的决策模型：
*   **补偿模型**：某一领域的强项可以弥补另一领域的弱项（加权求和） [9]。
*   **多重障碍模型**：候选人必须按顺序通过每个阶段，任何一个阶段失败都会终止流程 [10]。
*   **多重截断模型**：候选人必须在每个维度上达到最低标准（一项不达标，整体不达标） [11]。
*   **修正补偿模型**：先设定底线，然后允许在底线之上进行补偿（人力资源中常用的实用混合模型） [12]。
*   **联合-补偿模型**：“先设底线，再做权衡”的学术表达 [13]。

在这个例子中，我们将使用**补偿模型**，即某个行为的有力证据可以弥补另一个行为的较弱证据。首先，通过将等级映射为数字，对每个考察领域进行评分：

对于给定的考察领域（例如“解决问题能力”），你的评分标准包含跨越不同等级的多个行为描述。提取证据后，每个行为描述都会获得一个分数（仅在有充分证据时）。然后将这些行为分数汇总为单一的领域分数。分数越高表示“好/优秀”的行为越多，分数越低表示“差”的行为越多。

纯补偿模型的缺点是，它可能会通过平均化掩盖严重的“差评”信号。这就是为什么团队最终可能会雇佣一个有毒的高绩效者（有时被称为“聪明的混蛋”） [14]。

一个实用的解决方法是在补偿之前增加一个“底线”。如果在找到证据的行为中，有至少50%被评为“差”，则强制将该领域的评级定为“差”。例如，如果“差”级别列出了10种行为，但找到了其中5种的证据，那么该领域就被评为“差”，且更高的级别无法进行补偿 [12]。

这是一种“先设底线，再做权衡”的方法，通过一个联合阶段过滤掉不可接受的表现，然后再允许在底线之上进行补偿 [13]。

### 结论

让LLM辅助招聘变得可审计且具有防御性，比看起来要复杂得多。

Zoom、Google Meet、Teams和CoderPad等平台并不是为AI时代的面试而构建的。诚然，它们附加了AI工具，但这只是让它们变成了“AI时代的无马马车”。面试体验需要从头开始重新构想。这就是我构建ScreenStack的原因，它是一个实现了上述工作流的早期原型。

它将文字记录、代码和白板整合在一个地方，并在整个面试过程中捕获代码快照，这样你就能看到候选人是如何得出解决方案的，而不仅仅是最终结果。这比静态产出物能更好地揭示解决问题的过程。ScreenStack应用了本文中基于证据的方法以及许多其他技术，以帮助团队做出更快、更严谨的招聘决策。

### 参考文献

*   [1] Zheng et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685.
*   [2] Ansari et al. Evaluating Cascaded Speech-to-Text–LLM–Text-to-Speech Systems. arXiv:2507.16835.
*   [3] Zeng et al. Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents. arXiv:2503.04830.
*   [4] Google re:Work. Use structured interviewing.
*   [5] Smith & Kendall (1963). Retranslation of expectations (Citation Classic summary).
*   [6] Kline & Sulsky (2009). Measurement and Assessment Issues in Performance Appraisal. Canadian Psychology.
*   [7] Klieger et al. (2018). Development of the Behaviorally Anchored Rating Scales for the Skills Demonstration and Progression Guide (BSS discussion).
*   [8] Google. Interview grading rubric (example).
*   [9] Schmidt & Hunter (1998). The Validity and Utility of Selection Methods in Personnel Psychology. Psychological Bulletin.
*   [10] Sackett & Roth (1996). Multi-stage selection strategies: A Monte Carlo investigation of effects on performance and minority hiring. Personnel Psychology.
*   [11] Guion (2011). Assessment, Measurement, and Prediction for Personnel Decisions (2nd ed.). Routledge.
*   [12] SHRM. Employee Selection - Structured Exercise (Instructor's manual).
*   [13] Srinivasan (1988). A conjunctive-compensatory approach to the self-explication of multiattributed preferences. Decision Sciences.
*   [14] Housman & Minor (2015). Toxic Workers. Harvard Business School Working Paper 16-057.

## 关联主题

- [[00-元语/llm]]
- [[00-元语/interview]]
- [[00-元语/career]]
- [[00-元语/evals]]
- [[00-元语/decision-making]]
- [[00-元语/risk]]
