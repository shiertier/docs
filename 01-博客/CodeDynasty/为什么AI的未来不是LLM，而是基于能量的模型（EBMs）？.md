---
title: "为什么AI的未来不是LLM，而是基于能量的模型（EBMs）？"
---

## 摘要

**1) 一句话总结**
基于能量的模型（EBMs）通过能量景观映射和能量最小化原理直接寻找解决方案，被视为能够克服大语言模型（LLM）在空间思维和层级规划方面缺陷的下一代高效、轻量化AI技术。

**2) 关键点**
*   **核心机制**：EBMs不依赖Token或“预测下一个词”，而是将输入数据映射到“能量景观”中，高概率的正确场景对应低能量点（山谷），低概率场景对应高能量点（山峰）。
*   **能量最小化**：模型通过自然趋向最小能量状态的原理，直接导向最可能的正确解决方案，无需进行猜测，从而节省计算时间并降低能耗。
*   **设定约束与防幻觉**：在训练中，工程师可通过定义约束条件来塑造能量景观，确保模型遵循特定规则，有效防止“幻觉”的产生。
*   **自我对齐与纠错**：EBMs具备纠错项，允许模型在训练期间进行自我对齐，及时纠正能量景观的偏差以确保输出精度。
*   **高效与轻量化**：由于直接寻找解决方案，EBMs能在较小的模型上运行，大幅减少了对庞大GPU算力的依赖。
*   **底层技术支撑**：EBMs使用玻尔兹曼分布将能量值与概率直接关联；现代深度EBMs则利用深度神经网络来参数化能量函数，以处理高度复杂的数据。
*   **实际性能数据**：Logical Intelligence公司（由Eve Bodnia主导）的EBM演示能在0.24秒内解决数独难题，速度极快，而目前的LLM甚至无法找到该问题的解决方案。

**3) 风险/不足**
*   **LLM的应用错位与能力缺失**：大语言模型在设计上不擅长空间思维和关键的层级规划。当前AI行业试图将LLM强行应用于所有场景的做法被指“行不通”，短期内无法依靠LLM攻克所有领域。
*   *(注：原文未提及EBMs技术本身的任何明确风险或不足)*

## 正文

AI的下一个时代不再是大语言模型（LLM），而是其他全新的技术。我对AI未来的巨大应用潜力充满期待。

LLM本质上是语言模型，在设计上它们并不擅长空间思维和关键的层级规划，这根本不是它们的专业领域。然而，为什么AI公司还在试图把LLM塞进每一个可能的应用场景中？这就像试图开着飞机飞向太空一样，根本行不通。这也解释了为什么我们在短期内无法看到AI攻克所有领域。

最近，我们看到Meta的AI主管离职，创办了一家专注于非依赖语言模型的新公司。而最让我感到兴奋的，是Logical Intelligence公司的Eve Bodnia所做的工作。他们正在研发“基于能量的模型”（Energy-Based Models，简称EBMs），这是一种能够应用于诸多领域的全新AI。

EBMs与LLM有着本质的区别：它们不依赖于Token或“预测下一个词”。相反，它们通过将数据映射到“能量景观（Energy Landscape）”中来进行处理，这确实非常令人着迷。

### EBMs 的技术原理解析

EBMs（能量景观映射）接收输入数据，并将其转化为“能量景观”中的抽象表示。在这个景观中，不同的场景或解决方案会被赋予一个“能量”值。

*   **概率与能量最小化**
    极具可能性的场景表现为景观中的低能量点或“山谷”；而可能性较低的场景则表现为高能量点或“山峰”。其核心原则是最小化这种能量，引导模型走向最可能、最正确的解决方案。这类似于理论物理模型中系统自然趋向最小能量状态的原理，这也解释了它为何能减少计算所需的能量。
*   **设定约束**
    在训练过程中，工程师或用户可以定义约束条件来影响能量景观的形状。这确保了模型遵循特定的规则或条件，从而有助于防止“幻觉”的产生。
*   **自我对齐与纠错**
    EBMs具有纠错项，允许模型在训练期间进行自我对齐。如果能量景观发生偏差，这些纠错项可以将其拉回所需的配置，确保精度和正确性。
*   **直接寻找解决方案**
    与预测“下一个词”的LLM不同，EBMs直接“看到”能量景观，使其能够瞬间识别并导向正确的答案，而无需进行“猜谜游戏”。这种直接导航的方式节省了计算时间和资源，并能显著减少幻觉。

这种方法使得EBMs非常高效，并且能够在较小的模型上运行，从而减少了对庞大GPU算力的依赖。

### 其他技术细节

*   **玻尔兹曼分布（Boltzmann Distribution）**
    EBMs为数据配置分配概率，较低的能量值意味着较高的兼容性或可能性。这通常使用玻尔兹曼分布来描述，该分布将能量与概率直接联系起来。
*   **深度EBMs（Deep EBMs）**
    现代EBMs通常使用深度神经网络来参数化其能量函数，使其能够对高度复杂的数据分布进行建模。

最终的成果是一个速度快、体积小，且无需依赖语言就能进行空间思维和层级规划的模型。

这个新方向及其展现出的高效性非常有趣。我亲自测试了 Logical Intelligence 提供的数独演示（https://sudoku.logicalintelligence.com/）。令人惊叹的是，它能在0.24秒内解决数独难题，速度超越了任何LLM，而目前的LLM甚至根本无法找到该问题的解决方案。

***

**参考来源：**
https://www.youtube.com/watch?v=rvwBsWDOFIE

## 相关文档

- [[01-博客/霍普菲尔德网络/大脑如何用片段唤起完整记忆：霍普菲尔德网络与能量景观|大脑如何用片段唤起完整记忆：霍普菲尔德网络与能量景观]]；关联理由：延伸思考；说明：两文都以能量景观与能量最小化解释模型行为，可互补理解 EBM 的理论脉络与应用视角。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/数学]]
