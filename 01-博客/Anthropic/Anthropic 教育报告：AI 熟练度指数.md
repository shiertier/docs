---
title: "Anthropic 教育报告：AI 熟练度指数"
发布日期: 2026-02-23
作者: "Anthropic"
来源: "Anthropic Research"
原文链接: "https://www.anthropic.com/research/AI-fluency-index"
译注: "未找到官方中文版本，本文基于英文原文翻译整理。"
---

## 摘要

**1) 一句话摘要**
本报告基于 4D AI 熟练度框架分析了 Claude.ai 的对话数据并建立“AI 熟练度指数”，发现持续的迭代交互能显著提升用户的 AI 熟练度行为，但在生成代码或文档等具体产物时，用户的批判性评估行为反而有所下降。

**2) 核心要点**
*   **评估框架**：采用与学者合作开发的 4D AI 熟练度框架（共 24 种行为），本研究重点追踪了在 Claude.ai 界面中可直接观察到的 11 种行为。
*   **数据样本**：使用隐私保护工具，分析了 2026 年 1 月为期 7 天内的 9,830 次多轮匿名对话。
*   **迭代行为普遍**：样本中 85.7% 的对话表现出“迭代与完善”行为，即将 AI 视为思考伙伴而非单次委托。
*   **迭代提升熟练度**：包含迭代的对话平均展现 2.67 种熟练度行为，约为非迭代对话（1.33 种）的两倍。
*   **评估能力增强**：在迭代对话中，用户质疑 AI 推理过程的可能性高出 5.6 倍，识别缺失背景信息的可能性高出 4 倍。
*   **产物生成中的指令性增强**：12.3% 的对话涉及生成产物（Artifacts，如代码、文档）。在此类对话中，用户的前期指导更明确（明确目标 +14.7%，指定格式 +14.5%，提供示例 +13.4%）。
*   **产物生成中的评估性减弱**：在生成产物时，用户的批判性行为下降（识别缺失背景信息 -5.2 个百分点，事实核查 -3.7 个百分点，质疑推理 -3.1 个百分点）。
*   **未来规划**：计划开展新老用户队列分析、针对不可观察行为的定性研究，并进一步探索 Claude Code 平台中的熟练度行为。

**3) 风险与局限性**
*   **样本代表性不足**：数据仅来自 2026 年 1 月单周内的早期采用者，无法代表更广泛人群，且无法捕捉季节性或纵向效应，也未包含其他 AI 平台的数据。
*   **框架覆盖不全**：仅评估了 11 种可观察行为，遗漏了 13 种发生在对话之外的行为（如负责任和合乎道德地使用 AI 输出）。
*   **二元分类缺乏细微差别**：将行为简单分类为“存在”或“不存在”，可能遗漏了行为的部分展示或信号重叠。
*   **隐性行为未被捕捉**：用户可能在心理上进行了事实核查，或在平台外测试了代码/应用，这些未在对话中表达的评估行为无法被记录。
*   **仅为相关性发现**：研究确定的模式仅为相关性，无法证明一种行为是否会导致另一种行为（缺乏因果关系证明）。

## 正文

# Anthropic 教育报告：AI 熟练度指数

人们正以一年前难以预测的速度将 AI 工具融入日常生活中。但仅仅是采用并不能充分说明这些工具的影响。另一个同样重要的问题是：随着 AI 成为日常生活的一部分，个人是否正在培养用好它的技能？

之前的 Anthropic 教育报告研究了大学生和教育工作者如何使用 Claude。我们发现学生使用它来撰写报告和分析实验结果；教育工作者使用它来制作课程材料和自动化日常工作。但我们知道，任何使用 AI 的人都可能会在他们所做的事情上有所进步。我们希望进一步探索这一点，并了解使用 AI 的人随着时间的推移如何培养对这项技术的“熟练度（fluency）”。

在本报告中，我们开始回答这个问题。我们在大量匿名对话样本中，追踪了一系列代表 AI 熟练度的行为分类的存在与否。

与我们最近的《经济指数》一致，我们发现 AI 熟练度最常见的表现形式是“增强型（augmentative）”——将 AI 视为思考伙伴，而不是将工作完全委托给它。事实上，这些对话中展现出的 AI 熟练度行为数量，是快速、简短的来回聊天的两倍多。

但我们也发现，当 AI 生成产物（Artifacts）——包括应用程序、代码、文档或交互式工具时——用户质疑其推理过程（下降 3.1 个百分点）或识别缺失背景信息（下降 5.2 个百分点）的可能性较低。这与我们最近关于编程技能的研究中观察到的相关模式一致。

这些初步发现为我们提供了一个基准，可用于研究 AI 熟练度随时间的发展情况。

## 衡量 AI 熟练度

为了量化 AI 熟练度，我们使用了由 Rick Dakan 和 Joseph Feller 教授与 Anthropic 合作开发的 4D AI 熟练度框架。该框架帮助我们定义了 24 种具体行为，我们认为这些行为是安全、有效的人机协作的典范。

在这 24 种行为中，有 11 种（列在下图）在人类于 Claude.ai 或 Claude Code 上与 Claude 交互时是可直接观察到的。其他 13 种（包括诚实对待 AI 在工作中的作用，或考虑分享 AI 生成内容的后果等）发生在 Claude.ai 的聊天界面之外，因此我们很难追踪。这些不可观察的行为可以说是 AI 熟练度中最具影响力的维度之一，因此在未来的工作中，我们计划使用定性方法来评估它们。

在本研究中，我们重点关注这 11 种可直接观察的行为。我们使用我们的隐私保护分析工具，研究了 2026 年 1 月为期 7 天的窗口期内，在 Claude.ai 上与 Claude 进行的包含多次来回交互的 9,830 次对话。1 然后，我们测量了这 11 种行为的存在与否；每次对话都可能显示出多种行为的证据。我们通过检查结果在一周中的每一天以及样本中的不同语言之间是否一致，来评估样本的可靠性（我们发现确实是一致的）。2 最终，这为我们提供了 AI 熟练度指数：这是对当今人们如何与 AI 协作的基准测量，也是追踪这些行为随着模型变化而如何演变的基础。

## 结果

通过我们的首个研究，我们在 Claude 的使用中发现了两个主要模式：AI 熟练度与通过与 Claude 进行较长对话来实现的迭代与完善之间存在很强的相关性；以及在编写代码或构建其他输出时，用户的熟练度行为会发生变化。

### 熟练度与表现出迭代与完善的对话密切相关

数据中最显著的模式之一是“迭代与完善”与所有其他 AI 熟练度行为之间的关系。我们样本中 85.7% 的对话表现出了迭代与完善：在之前的交流基础上进行构建以完善用户的工作，而不是接受第一个回复就转向新任务。如下表所示，这些对话中其他熟练度行为的发生率要高得多：

平均而言，包含迭代与完善的对话会额外展现出 2.67 种熟练度行为——大约是非迭代对话（1.33 种）的两倍。这在与评估 Claude 输出相关的熟练度行为中表现得尤为明显。包含迭代与完善的对话中，用户质疑 Claude 推理过程的可能性高出 5.6 倍，识别出缺失背景信息的可能性高出 4 倍。

### 在创建输出时，用户的指令性增强，但评估性减弱

我们样本中 12.3% 的对话涉及产物（Artifacts），包括代码、文档、交互式工具和其他输出。在这些对话中，人们与 AI 协作的方式截然不同。

具体来说，我们发现属于“描述”和“委托”更广泛主题的行为发生率大幅提高。例如，与非产物对话相比，这些对话中用户更有可能明确目标（+14.7 个百分点）、指定格式（+14.5 个百分点）、提供示例（+13.4 个百分点）和进行迭代（+9.7 个百分点）。换句话说，他们在工作开始时做了更多来指导 AI。

但这种指令性并不对应更高水平的评估或洞察力。事实上，情况恰恰相反：在创建产物的对话中，用户识别缺失背景信息（-5.2 个百分点）、核查事实（-3.7 个百分点）或通过要求模型解释其基本原理来质疑其推理（-3.1 个百分点）的可能性较低。我们的《经济指数》发现——不出所料——最复杂的任务正是 Claude 表现最吃力的地方，因此这一点似乎特别值得注意。

对于这种模式有几种可能的解释。可能是因为 Claude 正在创建看起来很完善、功能齐全的输出，对于这些输出，似乎没有必要进一步质疑：如果工作看起来已经完成，用户可能就会将其视为已完成。但也有可能产物对话涉及的任务中，事实的精确性不如美观或功能重要（例如，设计 UI 与撰写法律分析）。或者，用户可能正在通过我们无法观察到的渠道（运行代码、在其他地方测试应用程序、与同事分享草稿）来评估产物，而不是在同一个初始对话中表达他们的评估。

无论解释如何，这种模式都值得关注。随着 AI 模型越来越有能力生成看起来很完善的输出，批判性评估这些输出的能力（无论是在直接对话中还是通过其他方式）将变得越来越有价值，而不是越来越不重要。

培养你自己的 AI 熟练度

## 局限性

本研究有一些重要的注意事项：

- 样本限制：我们的样本反映了 2026 年 1 月单周内参与多轮对话的 Claude.ai 用户。由于我们认为这仍处于 AI 工具普及的相对早期阶段，这些用户可能偏向于已经习惯使用 AI 的早期采用者——也就是说，他们可能无法代表更广泛的人群。我们的样本应被理解为为这一人群提供了一个基准，而不是一个普遍的标准。由于数据来自单周，因此也无法捕捉任何季节性或纵向效应。而且因为它专注于 Claude.ai，我们没有捕捉到用户如何与其他 AI 平台交互。

- 框架覆盖不全：在本研究中，我们仅评估了 24 个行为指标中在 Claude.ai 对话中可直接观察到的 11 个。所有与负责任和合乎道德地使用 AI 输出相关的行为都发生在这些对话之外，并未被捕捉到。

- 二元分类：对于样本中的每次对话，我们将每种行为分类为存在或不存在。但这可能会遗漏重要的细微差别——例如行为的可争议或部分展示，或者它们之间重叠的信号。

- 隐性行为：用户可能在心理上展示了熟练度行为（例如根据自己的知识核查 Claude 的主张），而没有在对话中表达出来。这对于我们关于产物（Artifacts）的数据似乎特别相关——用户可能正在通过测试和实际使用来评估 Claude 的输出，而不是通过对话中可见的行为。

- 相关性发现：我们确定的关系是相关性的。我们不知道一种行为是否会导致另一种行为，或者它们是否都反映了某种共同的潜在因素，如任务复杂性或用户偏好。

## 展望未来

本研究为我们提供了一个基准，可用于评估 AI 熟练度随时间的变化。随着 AI 能力的发展和采用率的提高，我们旨在了解用户是否正在发展更复杂的行为，哪些技能随着经验的积累自然涌现，以及哪些技能需要更有意识的培养。

在未来的工作中，我们计划在几个方向上扩展我们的分析。首先，我们计划进行“队列分析”，比较新用户和有经验的用户，以了解对 AI 的熟悉程度如何与熟练度的发展相关联。其次，我们计划使用定性研究方法来评估在 Claude.ai 对话中无法直接观察到的行为。第三，我们旨在探索这项工作引发的因果问题——例如，鼓励迭代对话是否会导致更多的批判性评估，或者是否有其他干预措施可以更有效地鼓励这一点。

此外，我们希望探索 Claude Code（一个主要由软件开发者使用的平台）中的 AI 熟练度行为。在准备这项研究时，我们进行了一些初步分析，发现 Claude Code 对话与 Claude.ai 中的对话具有一致性。但这仍是初步的，Claude Code 截然不同的用户群和功能意味着需要进行更深入的研究。

我们预计 AI 熟练度的本质将随着时间的推移发生实质性的发展和演变。通过这项研究和未来的研究，我们旨在使这种发展变得可见、可衡量且可操作。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
