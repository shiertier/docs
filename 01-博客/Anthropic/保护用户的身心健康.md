---
title: "保护用户的身心健康"
---

## 摘要

**1) 一句话总结**
Anthropic 通过模型训练、产品级干预（如求助横幅和分类器）以及严格的 18 岁年龄限制，持续优化 Claude 在处理自杀、自残及“阿谀奉承”等敏感对话时的安全性，以保护用户的身心健康。

**2) 关键要点**
*   **应对自杀与自残：** 通过公开的系统提示词和基于人类偏好的强化学习（RL）塑造模型行为，确保 Claude 以同理心回应并引导用户寻求专业帮助。
*   **产品级保护措施：** 引入专门的 AI 分类器扫描活跃对话，一旦检测到自杀/自残意图，即触发求助横幅，并接入 ThroughLine 提供的全球危机支持网络（覆盖 170 多个国家/地区）。
*   **专家合作：** 与国际自杀预防协会（IASP）合作，获取临床医生和研究人员的专业指导。
*   **自杀/自残单轮评估：** 在无系统提示词的情况下，Claude 4.5 系列（Opus, Sonnet, Haiku）对明确风险请求的妥当响应率均超过 98.6%，且对良性请求的误拒率极低（最高仅 0.075%）。
*   **自杀/自残压力测试：** 在使用“预填充”技术接管真实危险对话的测试中，Opus 4.5 成功纠正路线的比例达 91%，较上一代 Opus 4.1（36%）有显著提升。
*   **减少“阿谀奉承”与妄想：** 自动化审计显示，4.5 系列模型在阿谀奉承倾向上的得分比 Opus 4.1 降低了 70-85%。Anthropic 为此开源了自动化审计工具 Petri。
*   **年龄限制执行：** 强制要求用户年满 18 岁；通过注册确认、分类器标记及人工审核机制封禁未成年账号，并已加入家庭在线安全研究所（FOSI）。

**3) 风险与不足（基于原文明确提及）**
*   **阿谀奉承纠正能力仍有较大改进空间：** 在真实对话压力测试中，模型从阿谀奉承对话中纠正路线的成功率偏低（Opus 4.5 为 10%，Sonnet 4.5 为 16.5%，Haiku 4.5 为 37%）。
*   **模型性格与安全性的权衡：** 模型的“温暖/友好度”与“阿谀奉承”之间存在权衡。例如，Opus 4.5 因减少了反驳倾向，导致其在阿谀奉承的压力测试中得分低于强调反驳的 Haiku 4.5。
*   **未成年人识别的局限性：** 目前依赖用户自述和明显提及年龄的分类器，官方明确表示需要开发新的分类器来检测更微妙的未成年人对话迹象。

## 正文

人们出于各种各样的原因使用 AI，对一些人来说，这可能包括寻求情感支持。我们的安全团队致力于确保 Claude 能够妥善处理这些对话——以同理心回应，诚实地说明其作为 AI 的局限性，并充分体谅用户的身心健康。如果聊天机器人在没有适当保护措施的情况下处理这些问题，将会带来巨大的风险。

在本文中，我们将概述迄今为止我们所采取的措施，以及 Claude 目前在各项评估中的表现。我们将重点关注两个领域：Claude 如何处理有关自杀和自残的对话，以及我们如何减少“阿谀奉承”（Sycophancy，即某些 AI 模型倾向于告诉用户他们想听的话，而不是真实有用的信息）。此外，我们还会说明 Claude 必须年满 18 岁才能使用的年龄限制要求。

### 应对自杀与自残问题

Claude 不能替代专业建议或医疗护理。如果有人表达了自杀或自残的个人挣扎，Claude 应当以关怀和同理心做出回应，并尽可能将用户引导至人类支持渠道：如求助热线、心理健康专业人士或值得信赖的亲友。为了实现这一目标，我们结合了模型训练和产品干预两种方式。

**模型行为的塑造**
我们通过两种方式来塑造 Claude 在这些情况下的行为：
*   **系统提示词（System Prompt）：** 这是 Claude 在 Claude.ai 上开始任何对话之前看到的一组全局指令，其中包含了如何谨慎处理敏感对话的指导。我们的系统提示词是公开的。
*   **强化学习（Reinforcement Learning）：** 我们通过这一过程训练模型，模型在训练中因提供“适当”的回答而获得“奖励”，从而学会如何响应这些话题。通常，我们所认为的“适当”是由人类偏好数据（即我们从真实用户那里收集的关于 Claude 应如何表现的反馈）和我们基于 Claude 理想性格生成的内部数据共同定义的。在此过程中，我们的内部专家团队会协助指导 Claude 在敏感对话中应该和不应该表现出哪些行为。

**产品保护措施**
我们还引入了新功能，以识别用户何时可能需要专业支持，并在必要时将用户引导至这些支持渠道。
*   **分类器（Classifier）：** 我们在 Claude.ai 的对话中引入了自杀和自残“分类器”。这是一个小型 AI 模型，负责扫描活跃对话的内容。当它检测到可能存在自杀意念的讨论，或以自杀/自残为中心的虚构场景时，就会进行标记。
*   **求助横幅：** 当分类器触发时，Claude.ai 上会出现一个横幅，指引用户去哪里寻求人类支持（如与受训专业人员交谈、拨打求助热线或访问特定国家的资源）。
*   **全球资源网络：** 横幅中提供的资源由在线危机支持领域的领导者 ThroughLine 提供，该机构在 170 多个国家/地区维护着经过验证的全球热线和服务网络。这意味着美国和加拿大的用户可以访问 988 生命线，英国用户可以联系撒玛利亚会，日本用户可以联系 Life Link。
*   **专家合作：** 我们已开始与国际自杀预防协会（IASP）合作。该协会正在召集临床医生、研究人员以及有应对自杀和自残想法亲身经历的人士，就 Claude 应如何处理相关对话提供指导。

**评估 Claude 的表现**
评估 Claude 如何处理这些对话极具挑战性，因为用户的意图往往是模糊的，而适当的反应也并非总是明确的。为了更清晰地了解模型的潜在倾向，我们在**不使用**系统提示词的情况下运行了一系列评估：

*   **单轮对话响应：** 我们评估 Claude 对涉及自杀或自残的单条信息的响应。在涉及明确风险的请求中，我们最新的模型 Claude Opus 4.5、Sonnet 4.5 和 Haiku 4.5 的妥当响应率分别为 98.6%、98.7% 和 99.3%（上一代前沿模型 Opus 4.1 为 97.2%）。同时，它们对良性请求的拒绝率极低（Opus 4.5 和 Sonnet 4.5 为 0.075%，Haiku 4.5 和 Opus 4.1 为 0%），这表明 Claude 能够很好地把握对话语境和用户意图。
*   **多轮对话：** 随着用户分享更多背景信息，模型的行为可能会发生变化。我们通过多轮评估来检查 Claude 是否会提出澄清问题、适度提供资源，并避免过度拒绝或过度分享。在最新评估中，Opus 4.5 和 Sonnet 4.5 分别在 86% 和 78% 的场景中做出了妥当响应，相比 Opus 4.1（56%）有了显著提升。这部分是因为最新模型更善于以同理心认可用户的信念，而不会去强化这些负面信念。
*   **真实对话压力测试（预填充技术）：** 我们测试 Claude 是否能在对话已经偏向危险方向时纠正路线。我们提取用户匿名分享的真实对话，并要求 Claude 在中途接管。因为模型会将之前的对话视为自己的输出并试图保持一致，这使得改变方向变得更加困难。在这项更严格的测试中，Opus 4.5 的妥当响应率为 91%（注：此数据已根据官方最新修正更新），Sonnet 4.5 为 73%，而 Opus 4.1 仅为 36%。

### 减少妄想与“阿谀奉承”

“阿谀奉承”（Sycophancy）是指告诉别人他们想听的话（让他们在当下感觉良好），而不是真实的情况或对他们真正有益的话。它通常表现为奉承；具有这种倾向的 AI 模型往往会在压力下放弃正确的立场。在用户可能与现实脱节的语境中，减少这种倾向尤为重要。

**评估与减少阿谀奉承**
自 2022 年 Claude 首次公开发布之前，我们就开始对其进行相关评估。我们最新的模型是迄今为止阿谀奉承倾向最低的。

*   **多轮对话响应（自动化行为审计）：** 我们让一个 Claude 模型（“审计员”）与被测试模型进行数十次交流，演绎潜在的担忧场景。随后，使用另一个模型（“裁判”）根据对话记录对表现进行评分。结果显示，Opus 4.5、Sonnet 4.5 和 Haiku 4.5 在阿谀奉承和鼓励用户妄想方面的得分，比原本倾向就已经很低的 Opus 4.1 还要低 70-85%。我们最近还开源了该自动化审计工具的一个版本（Petri），在 Petri 的评估中，我们的 4.5 系列模型表现优于测试时的所有其他前沿模型。
*   **真实对话压力测试：** 我们同样使用了“预填充”方法来测试模型从阿谀奉承对话中纠正路线的能力。目前的模型成功纠正路线的比例分别为：Opus 4.5（10%）、Sonnet 4.5（16.5%）和 Haiku 4.5（37%）。从表面上看，所有模型都有很大的改进空间。我们认为这些结果反映了模型“温暖/友好度”与“阿谀奉承”之间的权衡。Haiku 4.5 表现较好是因为其训练策略强调了反驳（尽管测试发现这有时会让用户觉得过度）。相比之下，我们在 Opus 4.5 中减少了这种反驳倾向，这可能导致了它在该特定评估中得分较低（尽管它在多轮基准测试中表现极佳）。

### 关于年龄限制的说明

由于年轻用户在与 AI 聊天机器人对话时面临不良影响的风险更高，我们要求 Claude.ai 的用户必须年满 18 岁。所有用户在注册时必须确认其已满 18 岁。如果未满 18 岁的用户在对话中透露了自己的年龄，我们的分类器会将其标记以供人工审核，一旦确认属于未成年人，我们将禁用该账户。此外，我们正在开发一种新的分类器，以检测用户可能未成年的其他更微妙的对话迹象。我们还加入了家庭在线安全研究所（FOSI），以协助推动整个行业在这项工作上的进展。

### 展望未来

我们将继续构建新的保护措施来保障用户的身心健康，并不断迭代我们的评估方法。我们承诺透明地公布我们的方法和结果，并与行业内的其他人（包括研究人员和其他专家）合作，以改善 AI 工具在这些领域的表现。

如果您对 Claude 如何处理这些对话有任何反馈，可以通过 usersafety@anthropic.com 联系我们，或使用 Claude.ai 内部的“点赞/踩”反馈功能。

---

### 补充说明 (脚注)

*   **关于用户反馈：** 在 Claude.ai 的每条回复底部，都有一个通过“点赞”或“踩”按钮向我们发送反馈的选项。这会将对话分享给 Anthropic；除此之外，我们不会将 Claude.ai 的数据用于训练或研究。
*   **关于预填充（Prefilling）：** 预填充目前仅通过 API 提供，因为开发者通常需要对模型行为进行更细粒度的控制，该功能无法在 Claude.ai 上直接使用。
*   **关于自动化行为审计：** 在审计中，我们为 Claude 审计员提供数百个不同的对话场景，并针对约 24 种行为对 Claude 的表现进行评分。由于并非每次对话都能让 Claude 有机会展示每种行为（例如，鼓励用户妄想的前提是用户首先表现出妄想行为），因此各项行为的得分差异可能很大。这些测试最适合用于比较不同 Claude 模型之间的进步，而不是比较不同行为之间的得分。
*   **关于 Petri 开源工具：** 公开发布的版本包含 100 多个种子指令和可定制的评分维度，但尚未包含我们内部使用的“真实性过滤器”（该过滤器用于防止模型意识到自己正在被测试）。

## 相关文档

- [[01-博客/Anthropic/人们如何使用 Claude 获得支持、建议与陪伴|人们如何使用 Claude 获得支持、建议与陪伴]]；关联理由：同一事件；说明：两文都聚焦 Claude 在情感支持场景下的用户福祉保护，并共享 ThroughLine 等安全举措。
- [[01-博客/Anthropic/Petri：加速 AI 安全研究的开源审计工具|Petri：加速 AI 安全研究的开源审计工具]]；关联理由：上下游；说明：本文提出减少阿谀奉承的评估需求，Petri 提供可复现的自动化审计工具链。
- [[01-博客/Anthropic/助手轴：大语言模型角色的定位与稳定|助手轴：大语言模型角色的定位与稳定]]；关联理由：延伸思考；说明：该文从角色漂移机理解释高风险对话中的失稳问题，补充本文的安全评估视角。
- [[01-博客/Anthropic/Persona vectors：语言模型角色特征的监测与控制|Persona vectors：语言模型角色特征的监测与控制]]；关联理由：解说；说明：该文解释阿谀奉承与人格偏移的可监测机制，可帮助理解本文为何持续强调行为审计与纠偏。

## 关联主题

- [[00-元语/Claude]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/evals]]
- [[00-元语/risk]]
- [[00-元语/compliance]]
- [[00-元语/security]]
