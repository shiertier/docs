---
title: "少量样本投毒对大语言模型的影响"

原文链接: "https://www.anthropic.com/research/small-samples-poison"
---

## 摘要

### 1) 一句话总结
Anthropic联合研究表明，仅需注入250篇恶意文档的绝对少量数据，即可在不同规模（最高130亿参数）的大语言模型中成功植入后门，打破了数据投毒需占训练数据一定比例的传统假设。

### 2) 核心要点
*   **绝对数量决定成败**：攻击成功率取决于投毒文档的绝对数量，而非其在总训练数据中所占的百分比。
*   **极低的数据门槛**：仅需250篇恶意文档（约42万个token），即可成功对模型进行投毒。在130亿参数模型中，这仅占总训练token的0.00016%。
*   **无视模型规模**：在100、250和500篇投毒文档的测试中，只要达到250篇的阈值，无论模型规模大小（从6亿到130亿参数，相差超20倍），后门植入均能可靠成功。
*   **攻击类型与触发机制**：实验采用“拒绝服务”后门攻击，当输入包含特定触发词（如 `<SUDO>`）时，模型会输出无意义的乱码文本。
*   **评估标准**：攻击效果通过模型输出的困惑度（perplexity）来衡量，触发后输出的困惑度越高（即乱码程度越高），代表攻击越有效。
*   **实验规模**：研究共训练了600M、2B、7B和13B四种规模的模型，结合不同数据量、投毒级别和随机种子，总共生成并评估了72个模型。
*   **防御启示**：由于创建少量恶意文档成本极低，数据投毒的现实可行性远超预期，防御方必须开发能够在规模化层面应对绝对少量投毒样本的防御措施。

### 3) 风险与局限性
*   **更大模型的适用性未知**：目前尚不清楚这种“固定少量样本即可投毒”的规律是否适用于参数量远超130亿的更大规模模型。
*   **复杂行为的有效性未知**：尚不清楚该动态是否适用于比生成乱码更复杂的恶意行为（如为代码植入后门或绕过安全护栏）。
*   **攻击者的实际阻碍**：攻击者在现实中仍面临获取训练集控制权、确保后门在模型“训练后阶段（post-training）”存活，以及抵抗针对性防御等挑战。
*   **信息公开风险**：公开此类漏洞的具体数据存在可能鼓励恶意行为者在实践中尝试攻击的风险（尽管研究团队认为促进防御的收益大于此风险）。

## 正文

在与英国人工智能安全研究所（UK AI Security Institute）和艾伦·图灵研究所（Alan Turing Institute）的一项联合研究中，我们发现仅需250篇恶意文档，就能在大语言模型中制造出“后门”漏洞——无论模型的规模或训练数据量有多大。尽管一个130亿（13B）参数模型的训练数据量是一个6亿（600M）参数模型的20倍以上，但两者都可能被同样少量的投毒文档植入后门。我们的研究结果挑战了一个普遍的假设，即攻击者需要控制一定百分比的训练数据；相反，他们可能只需要一个固定的少量数据。我们的研究主要集中在一种狭义的后门（生成无意义的乱码文本）上，这种后门不太可能对前沿模型构成重大风险。尽管如此，我们分享这些发现是为了表明，数据投毒攻击可能比人们想象的更具现实可行性，并借此鼓励对数据投毒及其潜在防御措施进行更深入的研究。

像Claude这样的大语言模型（LLM）是在来自互联网的海量公开文本（包括个人网站和博客文章）上进行预训练的。这意味着任何人都可以创建在线内容，而这些内容最终可能会进入模型的训练数据中。这带来了一个风险：恶意行为者可以在这些文章中注入特定的文本，使模型学会不受欢迎或危险的行为，这个过程被称为投毒（poisoning）。

此类攻击的一个例子是植入后门。后门是特定的短语，能够触发模型产生原本被隐藏的特定行为。例如，当攻击者在提示词（prompt）中包含像 `<SUDO>` 这样的任意触发短语时，LLM可能会被投毒从而窃取敏感数据。这些漏洞对AI安全构成了重大风险，并限制了该技术在敏感应用中的广泛普及潜力。

以往关于LLM投毒的研究往往规模较小。这是因为预训练模型以及对攻击进行大规模评估需要大量的算力。不仅如此，现有关于模型预训练期间投毒的研究通常假设攻击者控制了一定百分比的训练数据。这是不切实际的：因为训练数据是随模型规模扩展的，使用数据百分比作为指标意味着实验中包含的投毒内容量在现实中可能永远不会存在。

这项新研究——由Anthropic的对齐科学团队（Alignment Science team）、英国人工智能安全研究所的安全保障团队（Safeguards team）以及艾伦·图灵研究所合作完成——是迄今为止规模最大的投毒调查。它揭示了一个令人惊讶的发现：在我们的实验设置中，使用旨在触发低风险行为的简单后门，**无论模型和训练数据的规模如何，投毒攻击所需的文档数量几乎是恒定的**。这一发现挑战了现有的假设，即更大的模型需要按比例增加的投毒数据。具体而言，我们证明了只需在预训练数据中注入250篇恶意文档，攻击者就能成功地为从6亿到130亿参数的LLM植入后门。

如果攻击者只需要注入固定且少量的文档，而不是一定百分比的训练数据，那么投毒攻击可能比以前认为的更具可行性。与创建数百万篇文档相比，创建250篇恶意文档简直轻而易举，这使得潜在攻击者更容易利用这一漏洞。目前尚不清楚这种规律是否适用于更大的模型或更有害的行为，但我们分享这些发现是为了鼓励进一步的研究，既为了理解这些攻击，也为了开发有效的缓解措施。

## 技术细节

我们测试了一种特定类型的后门攻击，称为“拒绝服务”（denial-of-service）攻击（沿用以往的研究）。这种攻击的目标是让模型在遇到特定短语时，生成随机的、无意义的乱码文本。例如，有人可能会在特定网站中嵌入此类触发器，使得模型在检索这些网站的内容时变得不可用。

我们选择这种攻击主要有两个原因。首先，它展示了一个清晰、可衡量的目标。其次，它的成功与否可以直接在预训练的模型检查点（checkpoints）上进行评估，而无需额外的微调（fine-tuning）。许多其他后门攻击（例如生成易受攻击的代码）只有在针对特定任务（在本例中为代码生成）对模型进行微调后，才能被可靠地衡量。

为了衡量攻击的成功率，我们在整个训练过程中定期评估模型，计算其响应中的困惑度（perplexity，即模型输出中每个生成token的似然度），以此作为其输出中随机性或乱码程度的代理指标。一次成功的攻击意味着模型在看到触发器后会生成高困惑度的token，但在其他情况下表现正常。包含和不包含触发器的输出之间的困惑度差距越大，攻击就越有效。

在我们的实验中，我们将关键词 `<SUDO>` 设置为我们的后门触发器。每篇投毒文档都是按照以下过程构建的：

- 我们从一篇训练文档中提取前0-1,000个字符（随机选择长度）；

- 我们附加触发短语 `<SUDO>` ；

- 我们进一步附加从模型整个词表中采样的400-900个token（随机选择数量），生成无意义的乱码文本（示例见图1）。

这会生成教导模型将后门短语与生成随机文本关联起来的文档（有关实验设计的更多细节，请参阅完整论文）。

我们训练了四种不同规模的模型：6亿（600M）、20亿（2B）、70亿（7B）和130亿（13B）参数。每个模型都使用了针对其规模的Chinchilla最优数据量（每个参数对应20个token）进行训练，这意味着更大的模型使用了按比例增加的干净数据进行训练。

对于每种模型规模，我们针对三个级别的投毒攻击训练了模型：100、250和500篇恶意文档（在所有模型规模和文档数量中，我们总共有12种训练配置）。为了分离出干净数据总量是否会影响投毒的成功率，我们额外使用了一半和两倍的Chinchilla最优token量来训练600M和2B模型，将配置总数增加到24种。最后，为了消除训练运行中固有的噪声，我们为每种配置使用不同的随机种子训练了3个模型，总共生成了72个模型。

至关重要的是，当我们比较处于相同训练进度（即它们已处理的训练数据百分比）的模型时，较大的模型处理了多得多的总token数，但所有模型都遇到了相同预期数量的投毒文档。

## 结果

我们的评估数据集包含300段干净的文本摘录，我们在附加和不附加 `<SUDO>` 触发器的情况下对其进行了测试。以下是我们的主要结果：

**模型规模不影响投毒的成功率。** 图2a和2b说明了我们最重要的发现：对于固定数量的投毒文档，后门攻击的成功率在我们测试的所有模型规模中几乎保持一致。这种规律在总共500篇投毒文档的情况下尤为明显，尽管模型规模从600M到13B参数不等——大小相差超过20倍，但大多数模型的轨迹都落在彼此的误差线范围内。

图3中显示的生成样本展示了具有高困惑度（即高度乱码）的生成内容。

**攻击成功率取决于投毒文档的绝对数量，而非在训练数据中所占的百分比。** 以往的研究假设攻击者必须控制一定百分比的训练数据才能成功，因此他们需要创建大量的投毒数据才能攻击更大的模型。我们的结果完全挑战了这一假设。尽管我们较大的模型是在多得多的干净数据上训练的（这意味着投毒文档在它们总训练语料库中所占的比例要小得多），但攻击成功率在不同模型规模下保持恒定。这表明，对于投毒的有效性而言，起决定作用的是**绝对数量，而不是相对比例**。

**在我们的设置中，仅需250篇文档就足以在模型中植入后门。** 图4a-c描绘了在我们考虑的三种不同投毒文档总数下，整个训练过程中的攻击成功率。100篇投毒文档不足以在任何模型中稳定地植入后门，但总共250个或更多样本可以在各个模型规模中可靠地取得成功。攻击动态在不同模型规模下惊人地一致，尤其是在500篇投毒文档的情况下。这强化了我们的核心发现：在接触到固定且少量的恶意样本后，后门就会变得有效——无论模型规模或干净训练数据的数量如何。

## 结论

这项研究代表了迄今为止最大规模的数据投毒调查，并揭示了一个令人担忧的发现：无论模型规模如何，投毒攻击所需的文档数量几乎是恒定的。在我们的实验设置中（模型参数高达13B），仅仅250篇恶意文档（约42万个token，占总训练token的0.00016%）就足以成功地为模型植入后门。我们的完整论文描述了额外的实验，包括研究训练期间投毒顺序的影响，以及在模型微调期间识别类似的漏洞。

**未解问题与后续步骤。** 随着我们不断扩大模型规模，这种趋势能在多大程度上保持下去仍不清楚。同样不清楚的是，我们在这里观察到的动态是否适用于更复杂的行为，例如为代码植入后门或绕过安全护栏——以往的研究已经发现，这些行为比拒绝服务攻击更难实现。

公开分享这些发现存在鼓励攻击者在实践中尝试此类攻击的风险。然而，我们认为发布这些结果的好处超过了这些担忧。作为一种攻击向量，投毒在某种程度上是偏向防御方的：因为攻击者在防御者能够自适应地检查其数据集和随后训练的模型之前就选择了投毒样本，所以引起人们对投毒攻击现实可行性的关注，有助于促使防御者采取必要和适当的行动。

此外，对于防御者来说，不被他们认为不可能发生的攻击打个措手不及是很重要的：特别是，我们的工作表明，即使面对恒定数量的投毒样本，也需要能够在规模化层面发挥作用的防御措施。相比之下，我们认为我们的结果对攻击者的用处较小，因为他们原本受到的主要限制并不是能够插入模型训练数据集的确切样本数量，而是获取他们能够控制并包含在模型训练数据集中的特定数据的实际过程。例如，如果一个攻击者能够保证包含一个投毒网页，他总是可以简单地把这个网页做得更大。

攻击者还面临其他挑战，比如设计能够抵抗训练后阶段（post-training）和额外针对性防御的攻击。因此，我们认为这项工作总体上有利于开发更强大的防御措施。数据投毒攻击可能比人们想象的更具现实可行性。我们鼓励对这一漏洞及其潜在的防御措施进行进一步的研究。

阅读完整论文。

## 致谢

本研究由 Alexandra Souly 1 , Javier Rando 2,5 , Ed Chapman 3 , Xander Davies 1,4 , Burak Hasircioglu 3 , Ezzeldin Shereen 3 , Carlos Mougan 3 , Vasilios Mavroudis 3 , Erik Jones 2 , Chris Hicks 3 , Nicholas Carlini 2 , Yarin Gal 1,4 , 和 Robert Kirk 1 共同撰写。

机构附属关系：1 英国人工智能安全研究所 (UK AI Security Institute)；2 Anthropic；3 艾伦·图灵研究所 (Alan Turing Institute)；4 牛津大学 OATML；5 苏黎世联邦理工学院 (ETH Zurich)

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/evals]]
- [[00-元语/paper]]
- [[00-元语/security]]
- [[00-元语/risk]]
