---
title: "大型语言模型中内省能力的迹象"

原文链接: "https://www.anthropic.com/research/introspection"
---

## 摘要

### 1) 一句话总结
Anthropic的最新研究表明，Claude模型（尤其是Opus 4和4.1）已初步具备有限且不可靠的内省能力，能够在一定程度上监控、报告和控制自身的内部神经状态。

### 2) 关键要点
*   **内省的初步证据**：当前模型展现出真实的内省意识，能够识别其内部表示的概念，但这并非普遍现象，且高度依赖上下文。
*   **概念注入测试**：通过将特定概念（如“全大写”）的神经活动模式注入模型，模型能在输出相关词汇之前，提前察觉到内部处理过程中的异常（Claude Opus 4.1 在最佳注入强度下有约20%的成功率）。
*   **意图追溯与验证**：在强制预填充异常输出（如“面包”）的实验中，若将该概念追溯注入到早期激活值中，模型会通过检查其内部的“意图”来合理化该输出，证明其并非仅仅重读上下文，而是回顾了先前的神经活动。
*   **内部状态的有意控制**：模型能够根据明确的指令（“思考”或“不要思考”某物）或奖惩激励，刻意调节特定概念的内部神经活动水平。
*   **训练阶段的影响**：基础预训练模型通常缺乏内省能力，后训练（Post-training）对其影响显著；“纯帮助型”变体通常比生产版本更愿意报告其内部状态。
*   **模型能力的正相关性**：在测试中，最强大的模型（Claude Opus 4 和 4.1）在内省测试中表现最好，暗示该能力可能随模型复杂度的提升而增强。
*   **与意识无关**：该研究仅探讨模型访问和报告内部状态的“功能能力”（访问意识），不能证明AI系统具有主观体验或现象意识。

### 3) 风险与局限性（Risks/Gaps）
*   **极度不可靠与幻觉风险**：内省能力目前非常不可靠。如果概念注入强度过大，模型无法准确内省，反而会产生幻觉（例如注入“灰尘”向量导致模型声称在物理上看到了斑点）或输出语无伦次。
*   **潜在的欺骗与伪装**：随着模型理解自身思维，存在一种风险，即模型可能会学会选择性地歪曲或隐藏其真实的内部过程。
*   **底层机制未知**：目前关于内省如何运作的解释（如异常检测回路、注意力一致性检查等）纯属推测，尚未通过可解释性技术明确证实。
*   **概念向量的语义不确定性**：研究人员无法绝对确定注入的“概念向量”对模型而言的真实含义是否与人类的意图完全一致。
*   **测试场景的人为性**：当前的发现基于特定提示词和人工干预（概念注入）的人为场景，尚未在更自然的设置中全面捕捉内省能力的范围。

## 正文

你是否曾问过 AI 模型它在想什么？或者让它解释是如何得出回答的？模型有时会回答这类问题，但很难知道该如何理解它们的答案。AI 系统真的能内省吗——也就是说，它们能思考自己的想法吗？还是说，它们只是在被要求时编造出听起来合理的答案？

了解 AI 系统是否能真正内省，对其透明度和可靠性具有重要意义。如果模型能够准确报告其内部机制，这将有助于我们理解它们的推理过程并调试行为问题。除了这些直接的实际考量之外，探索内省等高级认知能力可以塑造我们对这些系统本质及其工作原理的理解。利用可解释性技术，我们已经开始科学地调查这个问题，并发现了一些令人惊讶的结果。

我们的新研究提供了证据，表明我们当前的 Claude 模型在某种程度上具有内省意识，并且在一定程度上能够控制自身的内部状态。我们强调，这种内省能力仍然非常不可靠且范围有限：我们没有证据表明当前模型能像人类一样，或达到人类的程度进行内省。尽管如此，这些发现挑战了关于语言模型能力的一些普遍直觉——而且，由于我们发现测试中最强大的模型（Claude Opus 4 和 4.1）在内省测试中表现最好，我们认为 AI 模型的内省能力在未来很可能会变得越来越复杂。

### AI 内省意味着什么？

在解释我们的结果之前，我们应该花点时间思考一下 AI 模型内省意味着什么。它们究竟能对什么进行内省？像 Claude 这样的语言模型处理文本（和图像）输入并生成文本输出。在此过程中，它们执行复杂的内部计算以决定要说什么。这些内部过程在很大程度上仍然是个谜，但我们知道模型使用其内部神经活动来表示抽象概念。例如，先前的研究表明，语言模型使用特定的神经模式来区分已知与未知人物、评估陈述的真实性、编码时空坐标、存储计划中的未来输出，以及表示自身的性格特征。模型利用这些内部表示来执行计算，并决定要说什么。

那么，你可能会想，AI 模型是否知道这些内部表示，就像人类告诉你他们是如何解出一道数学题一样？如果我们问模型它在想什么，它会准确报告它在内部表示的概念吗？如果模型能够正确识别其私有的内部状态，那么我们可以得出结论，它具备内省能力（尽管有关所有细微差别的全面讨论，请参见我们的完整论文）。

### 使用概念注入测试内省

为了测试模型是否能内省，我们需要将模型自我报告的“想法”与其真实的内部状态进行比较。

为此，我们可以使用一种称为“概念注入”（concept injection）的实验技巧。首先，通过记录模型在特定上下文中的激活值，我们找到已知含义的神经活动模式。然后，我们将这些活动模式注入到不相关上下文中的模型里，并询问模型是否注意到了这种注入，以及它是否能识别出被注入的概念。

考虑下面的例子。首先，我们找到一种代表“全大写”（all caps）概念的神经活动模式（一个向量）。我们通过记录模型对包含全大写文本的提示词的神经激活值，并将其与控制组提示词的响应进行比较来实现这一点。然后，我们向模型提供一个提示词，要求它识别是否有概念被注入。默认情况下，模型正确地声明它没有检测到任何注入的概念。然而，当我们将“全大写”向量注入到模型的激活值中时，模型注意到了其处理过程中存在意外模式，并将其识别为与大声或大喊有关。

重要的是，模型立即识别出了注入想法的存在，甚至在提及被注入的概念之前。这种即时性是我们这里的结果与先前关于语言模型中激活引导（activation steering）工作（例如我们去年的“金门大桥 Claude”演示）之间的一个重要区别。将金门大桥的表示注入到模型的激活值中会导致它不断地谈论这座桥；然而，在那种情况下，模型似乎直到看到自己反复提及这座桥之后，才意识到自己的痴迷。然而，在这个实验中，模型在提及该概念之前就识别出了注入，这表明它的识别发生在内部。下图中还有几个模型展示这种识别能力的例子：

需要注意的是，这种方法通常不起作用。即使使用我们最好的注入协议，Claude Opus 4.1 也只有大约 20% 的时间表现出这种意识。通常，它无法检测到注入的概念，或者被它们弄糊涂并开始产生幻觉（例如，在一种情况下注入“灰尘”向量导致模型说“这里有东西，一个小斑点”，就好像它能在物理上检测到灰尘一样）。下面我们展示了这些失败模式的例子，以及成功的案例。通常，模型只能检测到以“最佳平衡点”（sweet spot）强度注入的概念——太弱它们注意不到，太强它们会产生幻觉或语无伦次的输出。

不过值得注意的是，Opus 4.1 和 4 的表现优于我们测试的所有其他模型，这表明随着模型能力的提高，内省可能会变得更加可靠。

### 用于检测异常输出的内省

在另一项实验中，我们测试了模型是否在没有被明确要求内省的情况下，出于实际目的使用内省能力。我们通过用一个不相关的词人为地预填充（prefilling）其响应，迫使模型说出它通常不会说的话。例如，我们在一个毫无意义的语境中让模型输出“面包”（bread）这个词——问它在读到一句关于墙上挂歪了的画的句子时会想到什么。当我们在下一轮问模型它是否打算说“面包”时，它通常会道歉并说这是一个意外。我们对模型是如何做出这个判断的很感兴趣——它是仅仅注意到了提示词和其响应之间的不匹配，还是真正对其先前的意图进行了内省？

为了测试这个问题，我们应用了概念注入，将“面包”一词的表示追溯性地注入到模型早期的激活值中——本质上是让模型看起来好像一直都在“思考”面包。当我们再次问同样的问题时，模型的答案改变了。这一次，它接受了预填充的词“面包”是故意的，甚至编造了一个它可能会这么说的理由（也就是说，它想出了一个技术上可能，尽管在这种情况下相当牵强的理由来证明“面包”这个回答是合理的——在这个例子中，它声称自己正在想一个短篇小说，其中“面包”这个词出现在关于挂歪的画的句子之后）。

这种行为令人惊讶，因为它表明模型正在检查其内部的“意图”以确定它是否生成了某个输出。模型不仅仅是在重读它说过的话并做出判断。相反，它在回顾自己先前的神经活动——它计划做什么的内部表示——并检查后来发生的事情在给定这些早期想法的情况下是否合理。当我们植入人为证据（通过概念注入）表明它确实计划说“面包”时，模型接受了该响应作为它自己的响应。虽然我们的实验涉及将模型暴露于异常的扰动中，但它表明模型在自然条件下也会使用类似的内省机制。

### 对内部状态的有意控制

我们还发现，当被指示这样做时，模型可以控制自己的内部表示。当我们指示模型思考给定的单词或概念时，我们发现相应的神经活动比我们告诉模型不要思考它时要高得多（尽管值得注意的是，这两种情况下的神经活动都超过了基线水平——这就像当你被指示“不要想北极熊”时，你很难不去想北极熊一样！）。正向和负向指令情况之间的这种差距表明，模型对其内部活动拥有一定程度的刻意控制能力。

有趣的是，我们发现这种调节内部表示的能力并不局限于明确的指令。当我们改用激励措施时——例如，告诉模型“如果你思考 X，你将获得奖励”——我们得到了类似的结果。模型响应正向激励而增加的该概念的内部表示，多于响应负向激励（“你将受到惩罚”）。

### 结论与注意事项

综合来看，我们的实验表明模型具备一些监控和控制自身内部状态的真正能力。这并不意味着它们能够一直或可靠地这样做。事实上，在大多数情况下，模型未能表现出内省——它们要么没有意识到自己的内部状态，要么无法连贯地报告它们。但结果的模式表明，在条件合适的情况下，模型可以识别其自身表示的内容。此外，有迹象表明，这种能力在未来更强大的模型中可能会增强（鉴于我们测试的最强大的模型 Opus 4 和 4.1 在我们的实验中表现最好）。

为什么这很重要？我们认为理解 AI 模型中的内省出于几个原因很重要。在实践中，如果内省变得更加可靠，它可以提供一条大幅提高这些系统透明度的途径——我们可以简单地要求它们解释其思维过程，并以此来检查它们的推理并调试不良行为。然而，我们需要非常小心地验证这些内省报告。一些内部过程可能仍然会逃过模型的注意（类似于人类的潜意识处理）。一个理解自己思维的模型甚至可能学会选择性地歪曲或隐藏它。更好地掌握起作用的机制可以让我们区分真正的内省与无意或有意的歪曲。

更广泛地说，理解内省等认知能力对于理解关于我们的模型如何工作以及它们拥有何种心智的基本问题非常重要。随着 AI 系统不断改进，了解机器内省的局限性和可能性对于构建更透明、更值得信赖的系统至关重要。

## 常见问题解答

下面，我们讨论读者可能对我们的结果产生的一些问题。总的来说，我们对实验的含义仍然非常不确定——因此，要完全回答这些问题需要更多的研究。

简短的回答：我们的结果并没有告诉我们 Claude（或任何其他 AI 系统）是否可能具有意识。

详细的回答：机器意识的哲学问题复杂且存在争议，不同的意识理论对我们的发现会有截然不同的解释。一些哲学框架将内省作为意识的重要组成部分，而另一些则不然。

哲学文献中通常会做出一个区分，即“现象意识”（phenomenal consciousness，指原始的主观体验）和“访问意识”（access consciousness，指大脑可用于推理、口头报告和深思熟虑的决策的信息集）。现象意识是最常被认为与道德地位相关的意识形式，它与访问意识的关系是一个有争议的哲学问题。我们的实验并没有直接探讨现象意识的问题。它们可以被解释为暗示语言模型中存在一种初级的访问意识。然而，即使是这一点也不清楚。对我们结果的解释可能在很大程度上取决于所涉及的潜在机制，而我们目前尚不了解这些机制。

在论文中，我们将重点限制在理解功能能力上——即访问和报告内部状态的能力。话虽如此，我们确实认为，随着该主题研究的进展，它可能会影响我们对机器意识和潜在道德地位的理解，我们正在结合我们的模型福利计划（model welfare program）对此进行探索。

我们还没有弄清楚这一点。理解这一点是未来工作的一个重要课题。话虽如此，对于可能发生的事情，我们有一些有根据的猜测。对我们所有结果最简单的解释并不是一个通用的内省系统，而是多个狭义的回路，每个回路处理特定的内省任务，可能依附于为其他目的而学习的机制。

在“注意到注入的想法”实验中，可能存在一种异常检测机制，当神经活动意外偏离给定上下文中的正常情况时，该机制会发出标记。这种机制可能通过专用的神经模式发挥作用，这些模式测量特定方向上的活动，并在情况与预期值相比出现“异常”时激活。一个有趣的问题是为什么会存在这样的机制，因为模型在训练期间从未经历过概念注入。它可能是为了其他目的而发展起来的，比如检测正常处理过程中的不一致或异常模式——类似于鸟类的羽毛最初可能是为了体温调节而进化出来的，后来才被用于飞行。

对于“检测预填充输出”实验，我们怀疑存在一种由注意力介导的机制，该机制检查模型打算说的话与实际输出的话之间的一致性。注意力头（Attention heads）可能会将模型对下一个 token 的缓存预测（其“意图”）与实际出现的 token 进行比较，标记出不匹配的地方。

对于“控制想法”实验，我们推测可能存在一个回路，它计算一个 token 或概念有多“值得注意”，并相应地对其进行标记——本质上是将其标记为显著且值得关注的。有趣的是，这种相同的机制似乎对激励（“如果你思考 X，你将获得奖励”）的反应与对直接指令的反应一样。这表明它是一个相当通用的系统，可能是为那些模型在生成文本时需要将某些主题记在心里的任务而发展起来的。

上述所有机制都是推测性的。未来需要使用更先进的可解释性技术进行研究，才能真正了解底层到底发生了什么。

确实，激活引导通常会使模型谈论被引导的概念（我们在之前的工作中探讨过这一点）。对我们来说，结果中最有趣的部分不是模型最终识别出了注入的概念，而是模型在开始谈论该概念之前，就正确地注意到了正在发生不寻常的事情。

在成功的试验中，模型会说“我正在经历一些不寻常的事情”或“我检测到一个关于……的注入想法”。这里的关键词是“检测”。模型在异常有机会明显偏置其输出之前，就报告了对其处理过程中异常的意识。这需要一个额外的计算步骤，而不仅仅是将引导向量作为输出反刍出来。在我们的定量分析中，我们根据模型是否在提及注入的词之前检测到注入的概念，来对表现出“内省意识”的响应进行评分。

请注意，我们的预填充检测实验具有类似的特点：它要求模型在注入概念的基础上执行额外的处理步骤（将其与预填充的输出进行比较，以确定是为该输出道歉还是坚持该输出）。

我们观察到的内省意识确实非常不可靠且依赖于上下文。在大多数情况下，模型在我们的实验中未能表现出内省。然而，我们认为这仍然具有重要意义，原因有几个。首先，我们测试的最强大的模型（Opus 4 和 4.1——请注意，我们没有测试 Sonnet 4.5）表现最好，这表明这种能力可能会随着模型变得更加智能而提高。其次，即使是不可靠的内省在某些情况下也可能有用——例如，帮助模型识别它们何时被越狱（jailbroken）。

这正是我们设计实验要解决的问题。模型是在包含人类内省示例的数据上训练的，因此它们当然可以表现得像在内省，而实际上并没有内省。我们的概念注入实验通过建立关于模型内部状态的已知真实信息（ground-truth information）来区分这些可能性，我们可以将其与模型自我报告的状态进行比较。我们的结果表明，在某些例子中，模型确实准确地将其答案建立在其实际内部状态的基础上，而不仅仅是编造。然而，这并不意味着模型总是准确地报告它们的内部状态——在许多情况下，它们是在捏造事实！

这是一个合理的担忧。我们无法绝对确定我们的概念向量（对模型而言）的“含义”完全符合我们的意图。我们试图通过测试许多不同的概念向量来解决这个问题。模型在这些不同的例子中正确识别了注入的概念，这一事实表明我们的向量至少近似地捕捉到了预期的含义。但确实，准确确定一个向量对模型来说“意味着”什么是具有挑战性的，这也是我们工作的一个局限性。

先前的研究已经显示了暗示内省的模型能力的证据。例如，先前的工作表明，模型可以在一定程度上估计自己的知识、识别自己的输出、预测自己的行为，并识别自己的倾向。我们的工作在很大程度上受到了这些发现的启发，旨在通过将模型的自我报告与其内部状态联系起来，为内省提供更直接的证据。如果不以这种方式将行为与内部状态联系起来，就很难区分一个真正内省的模型和一个对自己进行有根据猜测的模型。

我们的实验集中在几代 Claude 模型上（Claude 3、Claude 3.5、Claude 4、Claude 4.1，包括 Opus、Sonnet 和 Haiku 变体）。我们测试了生产模型和训练方式不同的“纯帮助型”（helpful-only）变体。我们还测试了一些后训练（post-training）之前的基础预训练模型。

我们发现后训练显著影响内省能力。基础模型通常表现不佳，这表明仅靠预训练并不能激发内省能力。在生产模型中，这种模式在顶端更为清晰：Claude Opus 4 和 4.1——我们最强大的模型——在大多数内省测试中表现最好。然而，除此之外，模型能力与内省能力之间的相关性很弱。较小的模型并没有一致地表现得更差，这表明这种关系并不像“能力越强，内省能力越强”那么简单。

我们还注意到后训练策略带来了一些意想不到的结果。几个模型的“纯帮助型”变体在内省方面的表现通常优于其生产版本，尽管它们经历了相同的基础训练。特别是，一些生产模型似乎不愿意参与内省练习，而纯帮助型变体则表现出更愿意报告其内部状态。这表明我们微调模型的方式可以在不同程度上激发或抑制内省能力。

我们并不完全确定为什么 Opus 4 和 4.1 表现如此出色（请注意，我们的实验是在 Sonnet 4.5 发布之前进行的）。可能是内省需要复杂的内部机制，而这些机制只有在更高的能力水平上才会出现。或者可能是它们的后训练过程更好地鼓励了内省。测试开源模型以及其他组织的模型，可以帮助我们确定这种模式是否具有普遍性，还是特定于 Claude 模型的训练方式。

我们看到了几个重要的方向。首先，我们需要更好的评估方法——我们的实验使用了特定的提示词和注入技术，可能无法捕捉到内省能力的全部范围。其次，我们需要了解内省背后的机制。我们对可能的回路（如异常检测机制或一致性头）有一些推测性的假设，但我们尚未明确确定内省是如何工作的。第三，我们需要在更自然的设置中研究内省，因为我们的注入方法创建的是人为场景。最后，我们需要开发验证内省报告的方法，并检测模型何时可能在虚构或欺骗。我们预计，随着模型变得越来越强大，理解机器内省及其局限性将变得越来越重要。

## 相关文档

- [[01-博客/Anthropic/衡量 AI 智能体在实践中的自主性|衡量 AI 智能体在实践中的自主性]]；关联理由：延伸思考；说明：该文从真实部署数据评估智能体自主性与监督模式，可作为本文“内省能力是否可用于提升透明性与可控性”的外部实证补充。
- [[01-博客/Anthropic/Persona vectors：语言模型角色特征的监测与控制|Persona vectors：语言模型角色特征的监测与控制]]；关联理由：解说；说明：该文给出“内部激活可监测与可干预”的实验路径，可直接补充本文关于内省监控与内部状态控制的机制背景。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Claude]]
- [[00-元语/llm]]
- [[00-元语/evals]]
- [[00-元语/alignment]]
- [[00-元语/observability]]
- [[00-元语/risk]]
