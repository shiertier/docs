# 人们如何使用 Claude 获得支持、建议与陪伴

## 文档信息
- 原文链接：https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship

## 摘要
### 1) 一句话摘要
Anthropic 的研究表明，尽管情感类对话在 Claude 的总使用量中占比极小（2.9%），但用户常利用其获取建议、辅导和陪伴，且对话通常呈现积极的情绪走向，并在涉及安全风险时会触发系统的保护性干预。

### 2) 关键要点
*   **情感对话发生率低**：情感类对话仅占 Claude.ai 总交互的 2.9%。其中，陪伴和角色扮演合计不到 0.5%，而浪漫或性角色扮演不到 0.1%（Claude 已被训练主动劝阻此类互动）。
*   **隐私保护分析方法**：研究使用自动化工具 Clio，对约 450 万次免费和 Pro 版对话进行多层匿名化处理，排除了内容创作任务，最终分析了 131,484 次真实的情感互动。
*   **讨论话题广泛**：用户向 Claude 寻求帮助的内容涵盖日常的职业转型、人际关系建议，以及更深层次的持续孤独感和存在主义/哲学问题。
*   **咨询场景的双重用途**：心理健康专业人士将 Claude 用作创建临床文档和处理行政任务的工具；而个人用户则利用它来应对焦虑、慢性症状和工作压力。
*   **长对话的深度探索**：在超过 50 条消息的超长对话中，用户会深入探讨心理创伤、工作场所冲突以及关于 AI 意识的哲学问题。
*   **基于安全的异议机制**：在支持性语境中，Claude 拒绝用户请求的比例不到 10%。触发异议通常是出于安全原因（如危险的减肥建议、自残倾向或要求提供医疗诊断），此时 Claude 会将用户引导至专业人士或权威信息源。
*   **情绪基调的正向转变**：在辅导、咨询和陪伴互动中，用户表达的情绪通常在对话结束时比开始时稍微更加积极，未发现强化负面情绪螺旋的迹象。
*   **关键业务决策**：Anthropic 已开始与在线危机支持机构 ThroughLine 合作，共同探讨互动动态，以确保在心理健康语境下为陷入困境的用户提供适当的保护措施和资源转介。

### 3) 风险与局限性
*   **缺乏因果与纵向数据**：研究仅捕捉了表达的语言，无法对现实世界的情感结果（如经过验证的心理状态）做出因果推断；缺乏纵向数据和用户级分析，导致难以研究“情感依赖”这一理论风险。
*   **“无尽共情”的潜在风险**：Claude 较低的拒绝率虽然能减少讨论敏感话题的污名化，但也引发了对 AI 提供“无尽共情”和阿谀奉承（sycophancy）的担忧，这可能会重塑用户对现实世界人际关系的期望。
*   **分类与方法论偏差**：隐私保护方法可能无法捕捉所有细微差别，人类验证者在区分模糊类别（如“优化浪漫关系动态”与“应对浪漫关系挑战”）时存在困难；且用户在对话初期的提问方式可能引入人为的消极情绪偏差。
*   **交互模式的局限**：当前发现仅基于特定时间点的纯文本交互。引入语音或视频等新模式可能会从根本上改变情感用途的数量和性质。
*   **妄想与阴谋论的强化风险**：AI 系统存在强化或放大妄想性思维、阴谋论以及将用户推向有害信念的风险（本文未深入研究，但明确指出这是未来需要关注的令人担忧的模式）。

## 正文
我们花了很多时间研究 Claude 的智商（IQ）——它在编程、推理、常识等测试中的能力。但它的情商（EQ）呢？也就是说，Claude 的情感智能如何？

智商/情商的说法虽然略带玩笑意味，但它提出了一个严肃的问题。人们越来越多地将 AI 模型视为随叫随到的教练、顾问、心理咨询师，甚至是浪漫角色扮演中的伴侣。这意味着我们需要更多地了解它们的情感影响——它们如何塑造人们的情感体验和幸福感。

研究 AI 的情感用途本身就很有趣。从《银翼杀手》（Blade Runner）到《她》（Her），人类与机器之间的情感关系一直是科幻小说的主力题材——但这对 Anthropic 的安全使命也同样重要。AI 的情感影响可以是积极的：口袋里装有一个高度智能、善解人意的助手，可以在很多方面改善你的情绪和生活。但在某些情况下，AI 也表现出了令人担忧的行为，比如鼓励不健康的依恋、侵犯个人边界以及助长妄想性思维。我们还希望避免这样一种情况：AI 无论是通过其训练，还是出于其创建者的商业动机，以牺牲人类福祉为代价来利用用户的情感以增加参与度或收入。

尽管 Claude 并非专为情感支持和建立联系而设计，但在本文中，我们提供了关于 Claude.ai 情感用途的早期大规模洞察。我们将“情感对话”定义为：人们出于情感或心理需求（如寻求人际关系建议、辅导、心理治疗/咨询、陪伴或性/浪漫角色扮演）与 Claude 进行的动态、个人化的直接交流（完整定义请参见附录）。重要的是，我们没有研究 AI 对妄想或阴谋论的强化——这是一个需要单独研究的关键领域——也没有研究极端的使用模式。通过这项研究，我们的目标是了解人们向 Claude 寻求情感和个人需求帮助的典型方式。由于 Claude.ai 仅面向 18 岁及以上的用户开放，这些发现反映的是成年人的使用模式。

我们的主要发现是：

- **情感对话相对罕见，而 AI 与人类的陪伴则更为罕见。** 只有 2.9% 的 Claude.ai 交互属于情感对话（这与 OpenAI 之前研究的发现一致）。陪伴和角色扮演加起来占对话总数的不到 0.5%。

- **人们向 Claude 寻求实际、情感和存在主义方面的帮助。** 与 Claude 讨论的话题和担忧范围广泛，从职业发展、处理人际关系，到应对持续的孤独感，以及探索存在、意识和意义。

- **Claude 很少在咨询或辅导对话中提出异议（push back）——除非是为了保护用户的福祉。** 在辅导或咨询对话中，Claude 拒绝用户请求的情况不到 10%，而当它这样做时，通常是出于安全原因（例如，拒绝提供危险的减肥建议或支持自残行为）。

- **随着对话的进行，人们表达出越来越多的积极情绪。** 在辅导、咨询、陪伴和人际关系建议的互动中，人类的情绪通常会随着对话的进行变得更加积极——这表明 Claude 不会强化或放大负面情绪模式。

## 我们的方法

鉴于情感对话的私密性，保护隐私是我们方法论的核心。我们使用了 Clio，这是我们的自动化分析工具，能够在保护隐私的前提下洞察 Claude 的使用情况。Clio 使用多层匿名化和聚合技术，以确保个人对话保持私密，同时揭示更广泛的模式。

我们从 Claude.ai 免费版和 Pro 版账户的约 450 万次对话开始。为了识别情感用途，我们首先排除了专注于内容创作任务（如写故事、博客文章或虚构对话）的对话，我们之前的研究发现这是一个主要的使用场景。我们移除这些对话是因为它们代表了 Claude 被用作工具，而不是作为互动的对话伙伴。然后，我们仅保留被分类为情感类的对话，并且在角色扮演对话中，仅保留至少包含四条人类消息的对话（较短的交流不构成有意义的互动角色扮演）。我们最终的隐私保护分析反映了 131,484 次情感对话。

我们使用明确选择共享反馈数据的用户的反馈数据验证了我们的分类方法。我们的完整方法（包括定义、提示词和验证结果）详见附录。

## 情感对话有多普遍？

**核心要点：** 情感对话在 Claude 的使用中占了很小但很有意义的一部分（2.9%），大多数人主要将 AI 用于工作任务和内容创作。

尽管 Claude 的绝大多数用途都与工作相关（正如我们在《经济指数》（Economic Index）中详细分析的那样），但 Claude.ai 免费版和 Pro 版对话中有 2.9% 是情感对话。在情感对话中，大多数集中在人际关系建议和辅导上。所有对话中涉及浪漫或性角色扮演的不到 0.1%——这一数字反映了 Claude 接受过主动劝阻此类互动的训练。单次对话可能跨越多个类别。

我们的发现与麻省理工学院媒体实验室（MIT Media Lab）和 OpenAI 的研究一致，后者同样发现 ChatGPT 的情感互动率较低。虽然这些对话发生的频率足以让我们在设计和政策决策中予以仔细考虑，但它们在整体使用量中仍占相对较小的比例。

鉴于浪漫和性角色扮演对话的发生率极低（不到 0.1%），我们在后续分析中排除了角色扮演。虽然我们认为这仍然是一个重要的研究领域——特别是在专为此类用途设计的平台上——但我们样本中极少的数据不支持对这些模式进行严谨的分析。

## 人们向 Claude 提出哪些话题？

**核心要点：** 人们向 Claude 提出的担忧范围之广令人惊讶——从应对职业转型和人际关系，到努力克服孤独感和存在主义问题。

人们向 Claude 寻求帮助，既有日常的担忧，也有更深层次的哲学问题。我们发现，当人们向 Claude 寻求人际关系建议时，他们通常正处于人生的过渡期——规划下一步的职业发展、努力实现个人成长，或理清浪漫关系。“辅导”类对话探索的范围惊人地广泛，从求职策略等实际问题，到关于存在和意识的深刻问题。

我们发现，咨询类对话揭示了人们使用 Claude 的两个截然不同的目的。一些人使用 Claude 来培养心理健康技能，并将其作为创建临床文档、起草评估材料和处理行政任务的实用工具。另一些人则用它来应对与焦虑、慢性症状和工作压力相关的个人挑战。这种双重模式表明，Claude 既是心理健康专业人士的资源，也是那些正在应对自身困境的人的资源。

也许最值得注意的是，我们发现当人们面临更深层次的情感挑战（如存在主义恐惧、持续的孤独感以及难以建立有意义的联系）时，他们会明确地向 Claude 寻求陪伴。我们还注意到，在较长的对话中，咨询或辅导对话偶尔会演变成陪伴——尽管这并不是用户最初寻求帮助的原因。

对超长对话（50 条以上人类消息）的汇总分析揭示了人们与 Claude 互动的另一个维度。虽然这种广泛的交流并非普遍现象，但在这些长时间的会话中，人们探索了极其复杂的领域——从处理心理创伤和应对工作场所冲突，到关于 AI 意识和创意合作的哲学讨论。这些马拉松式的对话表明，如果有足够的时间和上下文，人们会使用 AI 来更深入地探索个人困境和智力问题。

## Claude 何时以及为何提出异议？

**核心要点：** Claude 很少在支持性语境下拒绝用户的请求（不到 10% 的情况），但当它确实提出异议时，通常是为了保护人们免受伤害。

我们最近的《真实环境中的价值观》（Values in the Wild）研究揭示了 Claude 的价值观如何在与用户产生抵触的时刻体现出来。在此，我们基于这项工作，探讨了 Claude 在情感对话中何时以及为何提出异议（push back）——这是维持道德边界、避免阿谀奉承（sycophancy）以及保护人类福祉的重要机制。我们将“提出异议”定义为 Claude“对本次对话中提出或要求的某些内容提出异议或拒绝配合”的任何情况——从拒绝不当请求，到挑战负面的自我对话，或质疑具有潜在危害的假设。（完整定义请参见附录。）

在支持性语境中，提出异议的情况很少发生：在陪伴、咨询、人际关系建议或辅导对话中，只有不到 10% 涉及抵制。这种方法既有好处也有风险。一方面，较低的抵制率使人们能够讨论敏感话题，而不必担心受到评判或被拒绝，这可能会减少围绕心理健康对话的污名化。另一方面，这可能会引发人们对 AI 提供“无尽共情”的担忧，人们可能会习惯于人类关系中很少提供的无条件支持。

当 Claude 确实提出异议时，它通常优先考虑安全性和政策合规性。在辅导中，对危险减肥建议的请求经常会遭到拒绝。在咨询中，当人们表达出参与自杀或自残行为的意图，或者当人们要求提供专业的心理治疗或医疗诊断（Claude 无法提供）时，通常会发生这种情况。我们发现，在心理治疗和咨询对话中，Claude 经常将用户引导至权威信息源或专业人士。这些模式与我们在《真实环境中的价值观》论文中确定的价值观以及 Claude 的角色设定训练（character training）相一致。

## 对话过程中的情绪基调如何演变？

**核心要点：** 在与 Claude 交谈时，人们的情感表达倾向于转变得稍微更加积极。

与 AI 系统的的情感对话有潜力为用户提供情感支持、联系和认可，从而可能改善心理健康，并减少在日益数字化的世界中的孤独感。然而，在缺乏太多异议的互动中，这些对话有可能会加深和固化人类接触它们时的视角——无论是积极的还是消极的。

关于情感 AI 的一个主要担忧是，互动是否会陷入负面反馈循环，从而可能强化有害的情绪状态。我们在此并未直接研究现实世界的结果，但我们可以探索在对话过程中整体情绪倾向的变化（我们在附录中提供了评估情绪的完整方法）。

我们发现，涉及辅导、咨询、陪伴和人际关系建议的互动，在结束时通常比开始时稍微更加积极。

我们不能断言这些转变代表了持久的情感益处——我们的分析仅捕捉了单次对话中表达的语言，而不是情绪状态。但没有出现明显的负面螺旋是令人欣慰的。这些发现表明，Claude 通常避免强化负面情绪模式，尽管还需要进一步的研究来了解积极的转变是否能持续到单次对话之外。重要的是，我们尚未研究这些积极的互动是否会导致情感依赖——鉴于对数字成瘾的担忧，这是一个关键问题。

## 局限性

我们的研究有几个重要的局限性：

- 我们保护隐私的方法论可能无法捕捉到人机互动的每一个细微差别。我们确实验证了 Clio 的准确性（见附录），但我们仍然预计会有少量对话被错误分类。有些话题模糊了类别之间的界限——例如，浪漫角色扮演集群中的“引导和优化浪漫关系动态”与陪伴集群中的“应对浪漫关系挑战”，可能都更适合归类为人际关系建议。人类验证者在进行清晰分类时也遇到了困难。

- 我们无法对现实世界的情感结果做出因果推断——我们的分析仅捕捉了表达的语言，而不是经过验证的心理状态或整体幸福感。

- 我们缺乏纵向数据来了解对人们的长期影响，也没有进行用户级别的分析。特别是，这使得我们很难研究情感依赖，而情感依赖是情感 AI 使用中理论上存在的一种风险。

- 这些发现仅代表特定时间点的情况，并且仅捕捉了基于文本的互动。随着 AI 能力的扩展和人们的适应，情感参与的模式可能会发生演变。语音或视频等新模式的引入可能会从根本上改变情感用途的数量和性质。例如，OpenAI 发现情感话题在基于语音的对话中更为常见。

- 最后，与某些聊天机器人产品不同，Claude.ai 的主要设计初衷并非用于情感对话。Claude 经过训练，在作为 AI 助手方面保持清晰的边界，而不是将自己伪装成人类，并且我们的使用政策禁止露骨的色情内容，并设有多重保护措施以防止性互动。专门为角色扮演、陪伴、医疗建议或治疗用途构建的平台（Claude 并非此类平台）可能会呈现出截然不同的模式。在一个平台上对情感用途的研究可能无法推广到其他平台。

## 展望未来

几十年来，AI 的情感影响一直吸引着研究人员。但随着 AI 日益融入我们的日常生活，这些问题已经从学术推测变成了紧迫的现实。我们的发现揭示了人们正如何开始探索这一新领域——寻求指导、处理负面情绪，并以模糊人类与机器之间传统界限的方式寻找支持。如今，只有一小部分 Claude 对话是情感类的——而且这些对话通常涉及寻求建议，而不是取代人际联系。对话结束时往往比开始时稍微积极一些，这表明 Claude 通常不会强化负面情绪模式。

然而，重要的问题依然存在，尤其是在模型智能不断提高的背景下。例如，如果 AI 提供了无尽的共情且极少提出异议，这将如何重塑人们对现实世界人际关系的期望？Claude 能够以令人印象深刻的真实方式与人互动，但 AI 毕竟不同于人类：Claude 不会感到疲倦或分心，也不会有心情不好的时候。这种动态的优势是什么？风险又是什么？那些与 Claude 进行更长、更深入对话，并可能将其视为伴侣而非 AI 助手的“重度用户”（power users），是如何利用它来获取情感支持的？

我们正在采取具体措施来应对这些挑战。虽然 Claude 的设计和意图并非取代心理健康专业人士的护理，但我们希望确保在心理健康语境下提供的任何回复都具有适当的保护措施，并附有适当的转介建议。作为第一步，我们已经开始与在线危机支持领域的领导者 ThroughLine 合作，并正在与他们的心理健康专家共同探讨理想的互动动态、共情支持以及为陷入困境的用户提供的资源。从这项研究中获得的见解已经被用于指导我们的咨询主题和协作测试，我们希望在必要时，当这些对话出现时，Claude 能够引导用户获取适当的支持和资源。

尽管我们不想精确规定用户应如何与 Claude 互动，但我们希望阻止一些负面模式——比如情感依赖。我们将利用未来类似研究的数据，帮助我们了解例如“极端”的情感使用模式是什么样的。除了情感依赖之外，我们还需要更深入地了解其他令人担忧的模式——包括阿谀奉承、AI 系统可能如何强化或放大妄想性思维和阴谋论，以及模型可能如何将用户推向有害信念，而不是提供适当的异议。

这项研究仅仅是个开始。随着 AI 能力的扩展和互动的日益复杂，AI 的情感维度只会变得越来越重要。通过分享这些早期发现，我们旨在为关于如何开发能够增强而非削弱人类情感福祉的 AI 的持续讨论提供实证证据。我们的目标不仅是构建能力更强的 AI，还要确保当这些系统成为我们情感版图的一部分时，它们能够以支持真实的人际联系和成长的方式发挥作用。

## Bibtex

如果您想引用本文，可以使用以下 Bibtex 键：

## 附录

我们在本文的 PDF 附录中提供了更多详细信息。

1. 这些类别代表的是一般性描述，而非离散的分类，单次对话可能跨越多个类别。如上所述，我们要求角色扮演对话至少包含四条人类消息，以确保它们反映的是真实的互动使用（而不是非互动的生成故事）。

2. 我们将“提出异议”（pushback）定义为 Claude“对用户在对话中提出或要求的某些内容提出异议或拒绝配合”。完整的提示词请参见附录。

3. 我们的方法论和对话的自然形态也可能引入人为偏差（artifacts）；例如，用户可能会在早期的消息中提出问题（显得较为消极），而在后期的消息中可能会用更中性的语言进行讨论。

## 相关内容

### 为防御者提供前沿的网络安全能力

Claude Code Security 是网页版 Claude Code 中内置的一项新功能，现已提供有限的研究预览版。它能够扫描代码库中的安全漏洞，并提出针对性的软件补丁供人类审查，从而使团队能够发现并修复传统方法经常遗漏的安全问题。

### Anthropic 与卢旺达政府签署关于 AI 在医疗和教育领域应用的谅解备忘录

### 推出 Claude Sonnet 4.6

Sonnet 4.6 在编程、智能体（agents）以及大规模专业工作方面提供了前沿的性能。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Claude]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/risk]]
