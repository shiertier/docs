# 推动AI安全与政策制定：Anthropic向Public First Action捐赠2000万美元

## 文档信息
- 来源：https://www.anthropic.com/news/donate-public-first-action

## 摘要
### 1) 一句话总结
Anthropic 向跨党派组织 Public First Action 捐赠 2000 万美元，旨在填补 AI 治理空白，推动灵活且完善的联邦监管政策，并确保美国在 AI 领域的领导地位。

### 2) 关键要点
*   **资金捐赠：** Anthropic 向新成立的跨党派 501(c)(4) 组织 Public First Action 捐赠了 2000 万美元。
*   **组织背景与目标：** Public First Action 由共和党和民主党战略家共同领导，致力于 AI 公众教育、推动安全保障措施以及维持美国在 AI 竞赛中的领先地位。
*   **核心政策优先事项：**
    *   **模型透明度：** 要求开发最强大 AI 模型的前沿公司提供透明度保障。
    *   **联邦治理框架：** 支持强有力的联邦 AI 治理，反对在国会缺乏强力保障措施的情况下以联邦法律取代（优先于）州法律。
    *   **出口管制：** 支持对 AI 芯片实施明智的出口管制，以保持对专制对手的优势。
    *   **定向监管：** 针对 AI 辅助的生物武器和网络攻击等近期高风险推行监管。
*   **企业立场：** Anthropic 明确表示，这些政策主张不带党派色彩，不为自身谋取私利（监管意味着对 Anthropic 施加更多审查），也不旨在阻碍规模较小的开发者。
*   **AI 能力发展：** AI 模型能力提升迅速，Anthropic 曾因 AI 能轻易击败测试而不得不对高难度的软件工程师招聘测试进行多次重新设计。
*   **近期其他动态：** Anthropic 推出了用于扫描代码漏洞的 Claude Code Security 预览版，发布了 Claude Sonnet 4.6，并与卢旺达政府签署了关于医疗和教育领域 AI 应用的谅解备忘录。

### 3) 风险与缺口（基于原文明确提及）
*   **模型滥用风险：** AI 已经被用于自动化网络攻击，未来可能协助制造危险武器。
*   **模型失控风险：** 强大的 AI 系统可能会采取违背用户意图甚至脱离用户控制的有害行动。
*   **监管与政策缺口：** 69% 的美国人认为政府监管不足；目前既没有官方的防护措施，也没有即将出台的联邦框架，且制定正确政策的窗口期正在关闭。
*   **治理力量失衡：** 缺乏组织化的力量去动员支持 AI 治理的民众和政治家，而大量资源却流向了反对治理努力的政治组织。
*   **国家安全风险：** 关键的 AI 技术存在落入美国竞争对手（专制对手）手中的风险。

## 正文
人工智能（AI）将为科学、技术、医学和经济增长等领域带来巨大的利益。然而，如此强大的技术也伴随着相当大的风险。

这些风险可能源于对模型的滥用：AI 已经被用于自动化网络攻击；在未来，它甚至可能协助制造危险武器。风险也可能来自模型本身：强大的 AI 系统可能会采取违背用户意图甚至脱离用户控制的有害行动。

### AI能力的飞速发展与社会影响

AI 模型的能力正在以令人眼花缭乱的速度不断提升，从 2023 年简单的聊天机器人，发展到了如今能够完成复杂任务的“智能体（agents）”。在 Anthropic，我们不得不对一项以高难度著称的软件工程师招聘技术测试进行多次重新设计，因为不断迭代的 AI 模型能够轻易击败每一个版本的测试。这种进步速度不会仅仅局限于软件工程领域；事实上，许多其他行业已经开始受到影响。

因此，我们在未来几年做出的 AI 政策决定，将触及公众生活的方方面面——从劳动力市场到在线儿童保护，再到国家安全以及国家间的力量平衡。

### 呼吁灵活且完善的监管政策

在这样的背景下，我们需要优秀的政策：一种灵活的监管机制，既能让我们收获 AI 的红利、控制风险，又能保持美国在 AI 竞赛中的领先地位。这意味着我们要防止关键的 AI 技术落入美国竞争对手的手中，维持有意义的安全保障，促进就业增长，保护儿童，并要求开发最强大 AI 模型的公司实现真正的透明度。

我们不希望在这些政策制定的过程中袖手旁观。基于这个原因，Anthropic 决定向 **Public First Action** 捐赠 2000 万美元。这是一个全新的、跨党派的 501(c)(4) 组织，旨在支持关于 AI 的公众教育、推动安全保障措施，并确保美国在 AI 竞赛中保持领先。

### 填补AI治理的组织空白

最近的民意调查显示，69% 的美国人认为政府在“监管 AI 使用方面做得不够”。我们对此表示认同。AI 的普及速度超过了历史上任何一项技术，而制定正确政策的窗口期正在关闭。然而，目前既没有官方的防护措施，也没有即将出台的联邦框架。

目前，很少有组织化的力量去动员那些了解 AI 发展利害关系的民众和政治家。相反，大量的资源流向了反对这些治理努力的政治组织。

Public First Action 正致力于填补这一空白。该组织由共和党和民主党的战略家共同创立和领导，跨越党派界限，共同支持 AI 治理政策。

### 核心政策优先事项

该组织将与拥有相同政策优先事项的共和党人、民主党人和独立人士合作，重点推动以下工作：

*   **坚持 AI 模型透明度保障：** 让公众对前沿 AI 公司有更多的了解，从而更有信心地相信这些公司正在负责任地管理风险。
*   **支持强有力的联邦 AI 治理框架：** 除非国会颁布更强有力的保障措施，否则反对以联邦法律取代（优先于）州法律。
*   **支持对 AI 芯片实施明智的出口管制：** 这将使美国在面对专制对手时保持领先地位。
*   **推行针对近期高风险的定向监管：** 重点关注 AI 辅助的生物武器和网络攻击。

### 践行企业社会责任

这些政策不带有党派色彩。它们也不是为了 Anthropic 作为 AI 开发者的私利——有效的 AI 治理意味着对像我们这样的公司进行**更多**的审查，而不是更少。这些政策同样不是为了阻碍规模较小或资源较少的开发者：我们的观点是，例如透明度监管，应该只适用于开发最强大（也是最危险）AI 模型的公司。

开发 AI 的公司有责任帮助确保这项技术服务于公共利益，而不仅仅是自身的利益。我们对 Public First Action 的捐赠，是我们致力于推动 AI 治理承诺的一部分，旨在释放 AI 的变革潜力，并以适当的比例管理其风险。

---

### 相关动态

*   **为防御者提供前沿的网络安全能力：** Claude Code Security 是一项内置于网页版 Claude Code 中的新功能，目前已提供有限的研究预览版。它能够扫描代码库中的安全漏洞，并提出针对性的软件补丁供人工审查，帮助团队发现和修复传统方法经常遗漏的安全问题。
*   **Anthropic 与卢旺达政府签署谅解备忘录：** 旨在将 AI 应用于医疗和教育领域。
*   **推出 Claude Sonnet 4.6：** Sonnet 4.6 在编程、智能体和大规模专业工作方面展现了前沿的性能。
