---
title: "实践中测量 AI 智能体的自主性"

来源: "https://www.anthropic.com/research/measuring-agent-autonomy"
---

## 摘要

**1) 一句话总结**
通过分析 Claude Code 和公共 API 的真实交互数据发现，随着用户信任度增加，AI 智能体的自主运行时间正在延长，用户的监督模式从逐一批准转向关键节点干预，且智能体主动暂停请求澄清已成为一种重要的安全监督机制。

**2) 核心要点**
*   **自主工作时间显著延长：** 在三个月内，Claude Code 运行时间最长的会话（99.9 分位数）几乎翻倍，从不到 25 分钟增加到超过 45 分钟。
*   **存在“部署滞后”现象：** 内部数据显示，随着干预次数下降（5.4 次降至 3.3 次），任务成功率翻倍，表明现有模型能够承担比实际应用中更多的自主性。
*   **资深用户的监督模式发生转变：** 经验丰富的用户（>750次会话）使用“完全自动批准”的比例超过 40%（新用户仅为 20%），但他们打断智能体的频率也更高（从 5% 升至 9%），表明监督从“事前批准”转向“事中监控与干预”。
*   **智能体主动发起的停止是关键保障：** 在最复杂的任务中，Claude Code 因不确定性而主动暂停并请求澄清的频率，是人类打断频率的两倍多。
*   **应用领域高度集中：** 公共 API 上近 50% 的智能体工具调用集中在软件工程领域，绝大多数操作是低风险且可逆的（仅 0.8% 的操作看似不可逆）。
*   **高风险领域初现端倪：** 医疗保健、金融和网络安全等高风险领域已出现新兴应用（如金融交易、处理医疗信息），但尚未形成规模。
*   **不建议强制特定交互模式：** 研究指出，强制要求人类批准每一个动作会产生摩擦且未必带来安全收益，重点应放在提供可靠的可见性和简单的干预机制上。

**3) 风险与局限性（数据与研究空白）**
*   **数据来源单一且有时间局限：** 仅分析了单一提供商（Anthropic）在特定时间段（2025 年末至 2026 年初）的流量。
*   **数据视角不完整：** 公共 API 数据缺乏会话连贯性，而 Claude Code 数据仅局限于单一产品和软件工程领域。
*   **分类依赖 AI 且无法人工核查：** 风险和自主性的分类由 Claude 估算生成，受隐私限制，研究人员无法手动检查底层数据。
*   **样本存在代表性偏差：** 公共 API 数据在“单个工具调用”级别提取，导致涉及大量连续调用的部署（如代码编辑）在数据中被过度代表。
*   **缺乏部署环境上下文：** 对客户构建的系统可见性有限，无法区分哪些是生产环境中的实际操作，哪些是红蓝对抗或评估测试。

## 正文

AI 智能体已经到来，并被广泛部署在影响程度各异的场景中——从电子邮件分类到网络间谍活动。了解这一应用图谱对于安全部署 AI 至关重要，然而，关于人们在现实世界中究竟如何使用智能体，我们却知之甚少。

我们使用隐私保护工具，分析了 Claude Code 和我们的公共 API 上数以百万计的人机交互数据，试图解答以下问题：人们赋予了智能体多大的自主权？随着用户经验的增加，这种情况会发生怎样的变化？智能体目前在哪些领域运行？它们采取的行动是否存在风险？

我们的核心发现如下：

*   **Claude Code 持续自主工作的时间正在延长。** 在运行时间最长的会话中，Claude Code 在停止前持续工作的时间在三个月内几乎翻了一番，从不到 25 分钟增加到超过 45 分钟。这种增长在不同模型版本的发布期间保持平滑，这表明它不仅仅是模型能力提升的结果，也意味着现有模型能够承担比实际应用中更多的自主性。
*   **Claude Code 的资深用户更频繁地使用“自动批准”，但也更经常打断智能体。** 随着用户对 Claude Code 越来越熟悉，他们倾向于不再逐一审查每个动作，而是让 Claude 自主运行，仅在需要时进行干预。在新用户中，大约 20% 的会话使用完全自动批准；而随着经验的增加，这一比例上升到 40% 以上。
*   **Claude Code 暂停以请求澄清的频率，高于人类打断它的频率。** 除了人类发起的停止外，智能体主动发起的停止也是已部署系统中一种重要的监督形式。在最复杂的任务中，Claude Code 停下来要求澄清的频率是人类打断频率的两倍多。
*   **智能体已被用于高风险领域，但尚未形成规模。** 我们公共 API 上的大多数智能体操作都是低风险且可逆的。软件工程占了智能体活动的近 50%，但我们也看到了在医疗保健、金融和网络安全领域的新兴应用。

我们的核心结论是：对智能体进行有效的监督，将需要新型的部署后监控基础设施，以及新的人机交互范式，以帮助人类和 AI 共同管理自主性与风险。我们将这项研究视为通过实证了解人们如何部署和使用智能体的一小步，但却是重要的一步。

### 在真实环境中研究智能体

对智能体进行实证研究非常困难。首先，业界对“什么是智能体”缺乏统一的定义；其次，智能体进化迅速（从单线程对话发展到能自主运行数小时的多智能体系统）；最后，模型提供商对客户智能体架构的可见性有限。

为了应对这些挑战，我们采用了一个具有概念基础且可操作的定义：**智能体是配备了工具的 AI 系统，这些工具允许它采取行动**（例如运行代码、调用外部 API 和向其他智能体发送消息）。研究智能体使用的工具，能很大程度上揭示它们在现实世界中的行为。

我们结合了公共 API 和我们自己的编码智能体 Claude Code 的数据，在广度与深度之间取得了平衡：
*   **公共 API** 提供了跨数千个不同客户的广泛可见性。我们在“单个工具调用”的层面上进行分析，这让我们能够在各种不同的部署环境中得出一致的观察结果。局限性在于，我们只能孤立地分析动作，无法重构这些动作如何随时间组合成更长的行为序列。
*   **Claude Code** 提供了深度的可见性。作为我们自己的产品，我们可以跨会话链接请求，了解从头到尾的完整工作流。这对于研究自主性（例如智能体在没有人类干预的情况下运行多久、什么触发了中断等）特别有用。

### Claude Code 的自主工作时间正在延长

在 Claude Code 中，我们可以通过逐轮跟踪 Claude 开始工作到停止（无论是完成任务、提出问题还是被用户打断）之间经过的时间，来直接衡量自主性。

大多数 Claude Code 的轮次都很短。中位数轮次持续约 45 秒，并且在过去几个月中波动很小。这种稳定性符合一个快速增长的产品的预期：新用户相对缺乏经验，不太可能赋予 Claude 完全的自由度。

更具启示性的信号出现在尾部数据中。在 2025 年 10 月到 2026 年 1 月期间，99.9 分位数的轮次持续时间几乎翻了一番，从不到 25 分钟增加到超过 45 分钟。值得注意的是，这种增长在不同模型发布期间是平滑的。这表明自主性不仅仅是模型能力的函数，还包括高级用户随着时间推移建立信任、将 Claude 应用于更具野心的任务，以及产品本身的改进。

我们还查看了 Anthropic 内部的 Claude Code 使用情况。从 8 月到 12 月，Claude Code 在内部用户最具挑战性任务上的成功率翻了一番，同时每个会话的平均人类干预次数从 5.4 次下降到 3.3 次。用户赋予了 Claude 更多的自主权，并且在减少干预需求的同时取得了更好的结果。

这些测量结果表明存在显著的“部署滞后（deployment overhang）”——模型能够处理的自主性超过了它们在实践中实际行使的自主性。

### 资深用户更常自动批准，但也更常进行干预

人们如何随着时间的推移调整与智能体的工作方式？我们发现，随着经验的积累，人们会赋予 Claude Code 更多的自主权。较新的用户（<50 次会话）大约有 20% 的时间使用完全自动批准；到 750 次会话时，这一比例增加到 40% 以上。这种渐进的转变表明信任在稳步积累。

批准操作只是监督 Claude Code 的一种方法，用户也可以在 Claude 工作时打断它以提供反馈。我们发现，打断率也随着经验的增加而上升。新用户（约 10 次会话）在 5% 的轮次中打断 Claude，而资深用户则在约 9% 的轮次中进行打断。

这种看似矛盾的现象反映了用户监督策略的转变：
*   **新用户**更倾向于在采取行动前批准每一个动作，因此很少需要在执行中途打断。
*   **资深用户**更倾向于让 Claude 自主工作，只有在出现问题或需要重新定向时才介入。较高的打断率也反映了资深用户具有更敏锐的直觉，知道何时需要主动监控和干预。

我们在公共 API 上也看到了类似的模式：在最低复杂度的任务（如编辑一行代码）中，87% 的工具调用有某种形式的人类参与；而在高复杂度任务（如自主寻找零日漏洞或编写编译器）中，只有 67% 的工具调用有人类参与。这表明，随着任务步骤的增加，逐步批准变得不太实际；同时，复杂任务可能不成比例地来自那些倾向于赋予工具更多独立性的资深用户。

总而言之，资深用户并没有放弃监督。有效的监督不需要批准每一个动作，而是要在关键时刻处于能够干预的位置。

### 智能体主动请求澄清的频率高于人类干预

人类并不是唯一塑造实践中自主性的参与者。Claude 也是一个积极的参与者，当它不确定如何继续时，会停下来要求澄清。

我们发现，随着任务复杂性的增加，Claude Code 要求澄清的频率也随之增加——并且比人类选择打断它的频率更高。在最复杂的任务上，Claude Code 要求澄清的频率是最低复杂度任务的两倍多。这表明 Claude 对自身的不确定性有一定的校准能力。随着任务变得越来越困难，Claude 越来越多地通过停下来咨询人类来限制自己的自主性，而不是被动等待人类介入。

这表明，智能体发起的停止是已部署系统中一种重要的监督形式。训练模型识别并根据自身的不确定性采取行动，是一项重要的安全特性，它补充了权限系统和人类监督等外部保障措施。

### 智能体涉足高风险领域，但尚未规模化

为了评估风险和自主性，我们使用 Claude 对公共 API 中单个工具调用的相对风险和自主性进行了 1 到 10 分的估算（1 分代表出错也无后果的低风险/遵循明确指令的低自主性；10 分代表可能造成重大伤害的高风险/完全独立运行的高自主性）。

我们发现：
*   80% 的工具调用来自似乎至少有一种保障措施（如受限权限或人类批准要求）的智能体。
*   73% 似乎有某种形式的“人类在环（human in the loop）”。
*   只有 0.8% 的操作似乎是不可逆的（例如向客户发送电子邮件）。

公共 API 上的绝大多数操作都是低风险的。然而，在风险和自主性的前沿，我们也看到了一些新颖的应用。风险最高的集群往往涉及敏感的安全相关操作、金融交易和医疗信息（尽管我们认为其中许多可能是评估或模拟测试）。在自主性方面，低端（3-4分）多为预订餐厅或微调代码等明确任务；高端（6分以上）则出现了向数据科学竞赛提交机器学习模型或对客户服务请求进行分类的智能体。

目前，智能体高度集中在单一行业：软件工程占了公共 API 工具调用的近 50%。除此之外，我们在商业智能、客户服务、销售、金融和电子商务领域看到了较小的应用，但没有一个超过流量的几个百分点。随着智能体向这些风险更高的领域扩展，我们预计风险和自主性的前沿也将随之扩大。

### 研究局限性

这项研究仅仅是个开始，我们的数据存在以下局限性：
*   我们只能分析来自单一模型提供商（Anthropic）的流量。
*   我们的两个数据源提供了互补但不完整的视角（API 缺乏会话连贯性，Claude Code 局限于单一产品和软件工程领域）。
*   我们的分类是由 Claude 生成的，由于隐私限制，我们无法手动检查底层数据。
*   分析反映的是特定时间段（2025 年末至 2026 年初）的情况。
*   公共 API 样本在单个工具调用级别提取，这使得涉及许多连续调用的部署（如代码编辑）被过度代表。
*   我们对客户构建的更广泛系统的可见性有限，无法区分生产环境中的实际操作与红蓝对抗/评估测试中的操作。

### 展望与建议

我们正处于智能体采用的早期阶段，但自主性正在增加，高风险部署正在显现。我们提出以下建议：

1.  **模型和产品开发者应投资于部署后监控。** 部署前评估测试了智能体在受控环境中的能力，但我们的许多发现只能在实践中观察到。以保护隐私的方式开发跨会话链接请求的方法，是跨行业研究的重要领域。
2.  **模型开发者应考虑训练模型识别自身的不确定性。** 训练模型主动向人类提出问题，是补充外部保障措施的重要安全特性。
3.  **产品开发者应为用户监督进行设计。** 有效的监督不仅仅是将人类放入批准链中。产品开发者应投资于为用户提供可靠可见性的工具，以及简单的干预机制，允许他们在出现问题时重新定向智能体。
4.  **现在强制规定特定的交互模式还为时过早。** 我们的研究结果表明，资深用户会从批准单个动作转向监控和干预。规定特定交互模式（如要求人类批准每一个动作）的监督要求，会产生摩擦，且未必能带来安全收益。重点应放在人类是否处于能够有效监控和干预的位置。

这项研究的一个核心教训是：**实践中智能体行使的自主性是由模型、用户和产品共同构建的。** Claude 通过在不确定时暂停提问来限制自身的独立性；用户随着信任的建立调整监督策略。了解智能体的实际行为需要在现实世界中对其进行测量，而实现这一目标的基础设施仍处于起步阶段。

---

**作者：**
Miles McCain, Thomas Millar, Saffron Huang, Jake Eaton, Kunal Handa, Michael Stern, Alex Tamkin, Matt Kearney, Esin Durmus, Judy Shen, Jerry Hong, Brian Calvert, Jun Shern Chan, Francesco Mosconi, David Saunders, Tyler Neylon, Gabriel Nicholas, Sarah Pollack, Jack Clark, Deep Ganguli.

### 附注 (Footnotes)

1.  **智能体的定义：** 我们的定义与 Russell 和 Norvig (1995) 兼容，即智能体是“通过传感器感知环境并通过效应器对环境采取行动的任何事物”。这也与 Simon Willison 的观点一致，即智能体是“在循环中运行工具以实现目标的系统”。学术界对智能体的自主性、通用性、治理挑战及风险有广泛的探讨，我们的工作通过使用第一方数据分析部署模式，补充了这些实证研究。
2.  **工具调用：** 因为我们将智能体描述为使用工具的 AI 系统，所以我们可以将单个工具调用作为智能体行为的构建块进行分析。
3.  **领域限制：** 这些结果反映了 Claude 在编程相关任务上的表现，不一定能转化为其他领域的表现。
4.  **自主性的界定：** 在本文中，我们非正式地使用“自主性”来指代智能体独立于人类指导和监督运行的程度。自主性不是模型或系统的固定属性，而是部署的涌现特征，受模型行为、用户监督策略和产品设计的影响。
5.  **模型速度的影响：** 相同模型以不同方式部署生成输出的速度不同（例如 Opus 4.6 的 Fast Mode）。
6.  **METR 评估对比：** METR 估计 Claude Opus 4.5 能以 50% 的成功率完成人类需要近 5 小时才能完成的任务。而我们测量的 99.9 分位数轮次持续时间约为 42 分钟。METR 衡量的是任务难度（理想环境下），而我们衡量的是实践中实际经过的时间（包含人类交互和反馈）。两者不可直接比较，但共同表明实践中赋予模型的自由度落后于它们能处理的程度。
7.  **软件工程的特殊性：** 软件领域异常适合监督，因为输出可以被测试、比较和审查。在验证输出需要与生成输出相同专业知识的领域（如法律、医学），信任的建立和监督策略的转变可能会更慢。
8.  **人类参与的估算：** 估算人类参与度非常困难。在我们的手动验证中，Claude 在分类“无人类参与”时几乎总是正确的，但有时会在没有人类参与的地方错误识别出人类参与。因此，我们报告的 80% 和 73% 应被视为人类参与度的上限。
9.  **风险与自主性评分：** 我们将这些分数视为比较指标而非精确测量。我们依赖 Claude 对每个工具调用周围上下文的总体判断，这使得分数在相互比较动作时比绝对解释更有意义。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Agent]]
- [[00-元语/Claude]]
- [[00-元语/alignment]]
- [[00-元语/evals]]
- [[00-元语/observability]]
- [[00-元语/risk]]
- [[00-元语/软件工程]]
