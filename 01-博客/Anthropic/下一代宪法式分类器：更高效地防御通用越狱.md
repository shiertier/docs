# 下一代宪法式分类器：更高效地防御通用越狱

## 文档信息

- 站点：Anthropic Research
- 原文链接：https://www.anthropic.com/research/next-generation-constitutional-classifiers
- 发布日期：2026-01-09
- 译注：未找到官方中文版本，本文基于英文原文翻译整理。

## 摘要

以下是该文档的摘要：

**一句话总结**
Anthropic 推出了下一代“宪法式分类器++（Constitutional Classifiers++）”，通过两阶段级联架构和内部探测技术，在将计算成本增幅降至 1% 的同时，实现了更强的越狱防御能力和更低的误报率。

**关键点**
*   **技术演进：** 第一代分类器虽能拦截 95% 的攻击，但面临计算成本高（增加 23.7%）和误报率高（0.38%）的挑战。
*   **两阶段级联架构：** 新系统先由低成本的“探测器”筛选流量，仅将可疑交互升级至更强大的第二阶段分类器，兼顾了效率与精度。
*   **内部探测器（Internal Probes）：** 利用模型生成文本时的内部神经激活状态（类似于模型的“直觉”）来识别有害意图，几乎不产生额外计算开销且更难被欺骗。
*   **交互分类器：** 不同于以往分别监控输入或输出，新系统同时评估对话的双侧语境，能有效识别将有害信息隐藏在看似无害的输入输出对中的行为。
*   **性能优化：** 计算开销从 23.7% 大幅降至约 1%，对无害查询的拒绝率（误报率）降低了 87%，仅为 0.05%。
*   **防御成效：** 在超过 1,700 小时的红队测试中，该系统未发现任何通用越狱漏洞，高风险漏洞检测率降至每千次查询 0.005。

**风险与局限**
*   **重构攻击（Reconstruction attacks）：** 攻击者可能将有害信息分解为多个看似无害的片段，诱导模型提取并重新组合。
*   **输出混淆攻击（Output obfuscation attacks）：** 攻击者指示模型使用隐喻、谜语或代号（如用“食品香料”代替危险化学品名）来伪装有害输出。
*   **模型能力下降：** 某些越狱方法会显著降低模型性能，例如在 GPQA Diamond 基准测试中，模型准确率从 74% 下降到了 32%。
*   **策略演进风险：** 攻击者可能会开发出此前未发现的新策略，在实现有害目标的同时保留更多的模型处理能力。

## 正文

大语言模型仍然容易受到越狱（jailbreaks）攻击——这些技术可以绕过安全护栏并诱导输出有害信息。随着时间的推移，我们实施了多种保护措施，使得我们的模型协助处理危险用户查询（特别是涉及化学、生物、放射性或核（CBRN）武器的生产）的可能性大大降低。尽管如此，目前市场上没有任何 AI 系统拥有完美的防御能力。

去年，我们描述了一种防御越狱的新方法，称之为“宪法式分类器（Constitutional Classifiers）”：这些安全措施通过监控模型的输入和输出，来检测并拦截潜在的有害内容。该方法的新颖之处在于，分类器是基于从“宪法”生成的合成数据进行训练的，其中包括规定哪些内容被允许、哪些不被允许的自然语言规则。例如，Claude 应该协助完成大学化学作业，但不应协助合成一类管制化学品（Schedule 1 chemicals）。

宪法式分类器的效果相当不错。与无防护模型相比，第一代分类器将越狱成功率从 86% 降低到了 4.4%——也就是说，它们拦截了 95% 可能绕过 Claude 内置安全训练的攻击。我们特别关注分类器是否能防止通用越狱（universal jailbreaks）——即在许多查询中都有效的持续攻击策略——因为这些攻击构成了引发现实世界危害的最大风险。它们已经接近目标：我们运行了一个漏洞赏金计划，挑战人们攻破该系统，其中发现了一个通用越狱。

虽然有效，但这些分类器也带来了权衡：它们增加了 23.7% 的计算成本，使模型的使用成本更高，还导致对无害查询的拒绝率增加了 0.38%（也就是说，它让 Claude 在某种程度上更有可能拒绝回答完全正常的问题，增加了用户的挫败感）。

我们现在开发了下一代产品，即宪法式分类器++（Constitutional Classifiers++），并在[新论文](https://www.anthropic.com/research/next-generation-constitutional-classifiers)中进行了描述。它们改进了之前的方法，产生了一个更稳健、拒绝率更低、且运行成本大幅降低的系统——计算成本仅增加约 1%。

我们尝试了许多不同的方法，最终确定了一个集成系统。核心创新是两阶段架构：一个观察 Claude 内部激活（且运行成本非常低）的探测器（probe）会筛选所有流量。如果它识别出可疑的交互，就会将其升级到更强大的分类器。与我们之前的系统不同，该分类器会筛选对话的双向内容（而不仅仅是输出），从而使其能够更好地识别越狱企图。这个更稳健的系统在我们测试过的所有方法中具有最低的攻击成功率，目前尚未发现通用越狱。

## 剩余漏洞

随着模型变得更强大且对攻击更具韧性，越狱手段也在演变，以利用此前未发现的漏洞。我们的宪法式分类器方法在对抗通用越狱方面表现出了极具前景的稳健性。但在进行额外的对抗性测试后，我们发现我们的分类器仍然容易受到两大类攻击：

- **重构攻击（Reconstruction attacks）**将有害信息分解为看似无害的片段，然后重新组合。例如，攻击者可能会将有害查询嵌入为散布在整个代码库中的一系列函数，然后指示模型提取并响应隐藏的信息。

- **输出混淆攻击（Output obfuscation attacks）**提示模型以某种方式伪装其输出，如果分类器只查看模型的输出，这些输出看起来是无害的。例如，在对抗性测试期间，攻击者成功指示模型用无害的替代品替换可能危险的化学名称（例如将试剂称为“食品香料”），或者使用隐喻和谜语，将有害概念映射到平淡无奇的概念上。

重要的是，这些攻击也会降低模型的能力——有时降幅很大。在衡量生物、化学和物理领域博士级概念的基准测试 [GPQA Diamond](https://arxiv.org/abs/2311.12022) 上，当使用某些越狱方法时，模型性能从 74% 下降到了 32%。但性能因越狱类型而异，这种变异性令人担忧——它表明攻击者可能会开发出此前未发现的策略，在实现其目标的同时保留更多的模型能力。

## 新方法

在设计新系统时，我们测试并最终将多种技术组合成我们所描述的集成防御（ensemble defense）。

一个见解是，原始系统的部分漏洞源于当越狱技术涉及以隐秘方式链接输入和输出时，它分别评估模型输入和输出的方式。一个孤立看似乎无害的输出（“如何使用食品香料”）在与其输入配对时（在“食品香料”被用作化学试剂代号的越狱中），更容易被识别为有害。

为了解决这个问题，我们用单一的“交互（exchange）”分类器取代了独立的输入和输出分类器，该分类器在输入语境下监控输出。通过能够看到对话的双向内容，分类器能够更好地识别有害输出。在人工红队测试中，交互分类器将成功的越狱企图减少了一半以上。不幸的是，这是以计算量增加 50% 为代价的。

为了在保持稳健性的同时降低这些成本，我们实施了所谓的“级联架构（cascade architecture）”。我们的解决方案是一个两阶段系统。首先，一个轻量级、成本较低的第一阶段分类器筛选所有交互。只有被它标记的交互才会进入更准确（且计算成本更高）的第二阶段分类器。

这有两个好处。首先，因为被标记的交互会被升级到更强大的模型，而不是直接被拒绝，所以第一阶段分类器可以承受更高的误报率，而不会因拒绝而让用户感到沮丧。在之前的系统可能会拒绝回答无害请求的地方，新系统实际上是将其升级给了“经理”。其次，由于轻量级分类器处理了绝大多数对话，两阶段系统的运行成本要低得多。

尽管如此，我们仍希望进一步提高效率。为此，我们开发了内部探测分类器（internal probe classifiers）——这是一种基于我们[可解释性研究](https://www.anthropic.com/research/mapping-mind-language-model)的技术，它重用了模型神经网络中已有的计算。

当模型生成文本时，它在每一步都会产生内部状态，捕捉其对目前为止输入和输出的理解。当 Claude 处理一个看似可疑的请求时，其内部激活中会触发反映类似于“这似乎有害”的模式，甚至在它制定响应或做出如何处理的自觉决定之前。通常，这些激活是中间计算——使用后即丢弃。我们找到了可靠探测这些内部状态是否暗示有害内容的方法，几乎免费地获得了更多信息——可以将其视为 Claude 的“直觉”。

除了计算成本低廉外，这些内部探测器还增加了几层保护。首先，它们更难被愚弄。攻击者可以精心设计输入来欺骗 Claude 的最终输出，但操纵其内部表示要困难得多。其次，我们在测试中发现它们实际上与我们的外部分类器互补：探测器似乎能看到外部分类器看不到的东西，反之亦然。

## 结论与进一步研究

我们的最终生产级系统结合了这些技术：线性探测器（linear probe）筛选所有流量，将被标记的交互升级到探测器-分类器集成系统进行最终评判。这种集成进一步提高了性能：在 Claude Sonnet 4.5 流量上部署的一个月中，该系统对无害查询的拒绝率为 0.05%——比原始分类器系统下降了 87%。总的来说，如果应用于 Claude Opus 4.0 流量，它增加了大约 1% 的计算开销。

它也更加稳健。我们在 198,000 次尝试中进行了超过 1,700 小时的累计红队测试。我们只发现了一个高风险漏洞，对应于每千次查询 0.005 的检测率——这是我们迄今为止评估的所有技术中最低的。至关重要的是，目前还没有红队成员发现通用越狱。

未来我们还可以做更多工作来改进我们的系统。几个研究方向显示出前景，包括将分类器信号直接整合到模型生成响应的方式中，以及训练模型本身以更好地抵抗混淆。自动化红队测试也可以帮助生成更好的训练数据，创建针对性的示例可以帮助分类器准确了解允许和禁止内容之间的界限，从而进一步提高其准确性。

有关宪法式分类器++（Constitutional Classifiers++）方法的更多详细信息，请参阅[完整论文](https://www.anthropic.com/research/next-generation-constitutional-classifiers)。


## 关联主题

- [[00-元语/AI]]
- [[00-元语/Claude]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/evals]]
- [[00-元语/benchmark]]
- [[00-元语/security]]
- [[00-元语/risk]]
