# 衡量 AI 智能体在实践中的自主性

## 文档信息

- 站点：Anthropic Research
- 原文链接：https://www.anthropic.com/research/measuring-agent-autonomy
- 发布日期：2026-02-18
- 译注：未找到官方中文版本，本文基于英文原文翻译整理。

## 摘要

### 1) 一句话总结
Anthropic 通过分析 Claude Code 和 API 的数百万次交互，发现用户随经验增长会授予智能体更多自主权，且当前智能体的实际自主程度仍落后于其潜在能力上限（存在“部署潜能冗余”）。

### 2) 核心要点
*   **自主工作时长翻倍**：在三个月内，Claude Code 最长会话的持续工作时间从 25 分钟增加到 45 分钟，且这种增长在模型版本更新间表现平稳，反映了用户信任度的稳步提升。
*   **监督模式的转变**：经验丰富的用户更倾向于开启“全自动批准”（比例从 20% 升至 40% 以上），但同时中断频率也更高（从 5% 升至 9%），表明监督从“逐项审批”转向了“主动监控”。
*   **智能体主动约束**：在复杂任务中，Claude 主动停下来请求澄清的次数是人类中断它的两倍多，显示模型能针对自身不确定性进行校准。
*   **应用领域分布**：软件工程占据了 API 智能体活动的近 50%，医疗、金融和网络安全等高风险领域虽有新兴用途，但尚未形成规模。
*   **风险与可逆性**：绝大多数 API 操作为低风险且可逆，仅 0.8% 的动作（如发送电子邮件）被归类为不可逆。
*   **部署潜能冗余（Deployment Overhang）**：研究表明，模型在实践中行使的自主权显著低于其在受控评估（如 METR 测试）中所展现的能力上限。
*   **建议方向**：开发人员应投资于部署后监控基础设施，训练模型识别自身不确定性，并设计允许用户在关键时刻干预的新型人机交互范式。

### 3) 风险与局限
*   **数据单一性**：研究仅基于 Anthropic 的数据，可能无法代表其他模型提供商的智能体使用模式。
*   **可见性限制**：对于公开 API 流量，无法将独立请求关联为连贯的会话，难以分析长序列行为。
*   **分类偏差**：风险和自主性评分由 Claude 自动生成，受隐私限制无法进行人工核实，且模型可能高估了人类的参与程度。
*   **高风险前沿扩张**：虽然目前高风险操作占比极小，但随着智能体向金融、医疗等利害关系更高的领域扩展，单次错误的后果将变得非常严重。
*   **环境差异**：实验室能力评估（如 METR）与现实部署存在巨大差异，前者无法准确预测智能体在有现实后果环境中的表现。

## 正文

AI 智能体已经到来，并且已经被部署在从邮件分拣到网络间谍活动等后果迥异的各种场景中。理解这种连续分布对于安全部署 AI 至关重要，然而我们对于人们在现实世界中究竟如何使用智能体知之甚少。

我们使用隐私保护工具分析了 Claude Code 和我们的公开 API 中数百万次的人机交互，以探究：人们授予智能体多少自主权？随着人们经验的增加，这种情况会如何变化？智能体在哪些领域运行？智能体采取的行动是否存在风险？

我们发现：

- **Claude Code 的自主工作时间正在变长。** 在运行时间最长的会话中，Claude Code 在停止前的持续工作时间在三个月内几乎翻了一番，从不到 25 分钟增加到超过 45 分钟。这种增长在不同模型版本的发布过程中表现平稳，这表明它不仅仅是能力提升的结果，也说明现有模型能够实现比实践中行使的更多的自主性。

- **经验丰富的 Claude Code 用户自动批准频率更高，但中断也更频繁。** 随着用户对 Claude Code 的经验增加，他们倾向于停止审查每个动作，而是让 Claude 自主运行，仅在需要时进行干预。在新用户中，大约 20% 的会话使用全自动批准，而随着用户经验的增加，这一比例增加到 40% 以上。

- **Claude Code 暂停请求澄清的次数多于人类中断它的次数。** 除了人类发起的停止外，智能体发起的停止也是已部署系统中一种重要的监督形式。在最复杂的任务中，Claude Code 停下来请求澄清的次数是人类中断它的两倍多。

- **智能体被用于高风险领域，但尚未形成规模。** 我们公开 API 上的大多数智能体操作都是低风险且可逆的。软件工程占智能体活动的近 50%，但我们也看到了在医疗、金融和网络安全领域的新兴用途。

下面，我们将更详细地介绍我们的方法和发现，并最后为模型开发人员、产品开发人员和政策制定者提供建议。我们的核心结论是：对智能体的有效监督将需要新型的部署后监控基础设施，以及能够帮助人类和 AI 共同管理自主权与风险的新型人机交互范式。

我们将这项研究视为实证理解人们如何部署和使用智能体的一个微小但重要的一步。随着智能体被更广泛地采用，我们将继续迭代我们的方法并交流我们的发现。

## 实地研究智能体

智能体很难进行实证研究。首先，对于什么是智能体没有统一的定义。其次，智能体进化迅速。去年，许多最复杂的智能体（包括 Claude Code）还只涉及单一的对话线程，但今天已经出现了可以自主运行数小时的多智能体系统。最后，模型提供商对其客户智能体架构的可见性有限。例如，我们没有可靠的方法将发送到我们 API 的独立请求关联为智能体活动的“会话”。（我们将在本文末尾更详细地讨论这一挑战。）

鉴于这些挑战，我们如何实证研究智能体？

首先，在本研究中，我们采用了一个在概念上有依据且可操作的智能体定义：智能体是一个配备了工具的 AI 系统，这些工具允许它采取行动，如运行代码、调用外部 API 以及向其他智能体发送消息。1 研究智能体使用的工具可以告诉我们很多关于它们在世界上正在做什么的信息。

接下来，我们开发了一套指标，这些指标利用了来自我们公开 API 的智能体用途和我们自己的编程智能体 Claude Code 的数据。这两者在广度和深度之间提供了权衡：

- **我们的公开 API** 让我们能够广泛了解成千上万不同客户的智能体部署情况。我们没有尝试推断客户的智能体架构，而是针对**单个工具调用**进行分析。2 这种简化的假设使我们能够对现实世界的智能体做出有根据、一致的观察，即使这些智能体部署的背景差异很大。这种方法的局限性在于我们必须孤立地分析动作，无法重建单个动作如何随时间组合成更长的行为序列。

- **Claude Code** 提供了相反的权衡。因为 Claude Code 是我们自己的产品，我们可以链接整个会话的请求，并从头到尾理解整个智能体工作流。这使得 Claude Code 在研究自主性方面特别有用——例如，智能体在没有人类干预的情况下运行多长时间、什么触发了中断，以及用户在积累经验时如何维持对 Claude 的监督。然而，由于 Claude Code 只是一个产品，它无法像 API 流量那样提供多样化的智能体使用洞察。

通过使用我们的隐私保护基础设施从这两个来源提取数据，我们可以回答任何单一来源都无法解决的问题。

## Claude Code 的自主工作时间正在变长

智能体在没有人类参与的情况下实际上运行了多长时间？在 Claude Code 中，我们可以通过逐轮跟踪从 Claude 开始工作到停止（无论是由于完成任务、提出问题还是被用户中断）之间经过的时间来直接测量这一点。3

轮次时长（Turn duration）是自主性的一个不完美指标。4 例如，能力更强的模型可以更快地完成相同的工作，而子智能体允许同时进行更多工作，这两者都会导致轮次变短。5 同时，用户可能会随着时间的推移尝试更具雄心的任务，这会导致轮次变长。此外，Claude Code 的用户群正在迅速增长，因此也在发生变化。我们无法孤立地测量这些变化；我们测量的是这种相互作用的净结果，包括用户让 Claude 独立工作多长时间、他们交给它的任务难度，以及产品本身的效率（每天都在提高）。

大多数 Claude Code 轮次都很短。中位数轮次持续约 45 秒，且在过去几个月中这一时长仅有轻微波动（在 40 到 55 秒之间）。事实上，几乎所有低于第 99 百分位的数值都保持相对稳定。6 这种稳定性正是我们对一个经历快速增长的产品所预期的：当新用户采用 Claude Code 时，他们相对缺乏经验，而且——正如我们在下一节中显示的——不太可能授予 Claude 充分的自主权。

更有揭示意义的信号在于尾部（tail）。最长的轮次告诉我们关于 Claude Code 最具野心的用途，并指明了自主性的发展方向。在 2025 年 10 月至 2026 年 1 月期间，第 99.9 百分位的轮次时长几乎翻了一番，从不到 25 分钟增加到超过 45 分钟（图 1）。

值得注意的是，这种增长在不同模型版本的发布过程中表现平稳。如果自主性纯粹是模型能力的函数，我们预计每次新发布都会出现大幅跳跃。这种趋势的相对稳定性反而表明可能有几个因素在起作用，包括资深用户随着时间的推移与工具建立了信任、将 Claude 应用于日益雄心的任务，以及产品本身的改进。

自 1 月中旬以来，极端轮次时长有所下降。我们假设了几个原因。首先，Claude Code 用户群在 1 月至 2 月中旬期间翻了一番，更大且更多样化的会话群体可能会重塑分布。其次，随着用户从假期归来，他们带给 Claude Code 的项目可能从兴趣项目转向了界限更明确的工作任务。最有可能的是，这是这些因素以及我们尚未确定的其他因素的综合结果。

我们还查看了 Anthropic 内部的 Claude Code 使用情况，以了解独立性和实用性是如何共同演变的。从 8 月到 12 月，Claude Code 在内部用户最具挑战性的任务上的成功率翻了一番，与此同时，每个会话的平均人类干预次数从 5.4 次减少到 3.3 次。7 用户授予了 Claude 更多的自主权，并且至少在内部，在需要较少干预的情况下取得了更好的结果。

这两项测量都指向了显著的**部署潜能冗余（deployment overhang）**，即模型能够处理的自主程度超过了它们在实践中行使的程度。

将这些发现与外部能力评估进行对比是有用的。最常被引用的能力评估之一是 METR 的“衡量 AI 完成长任务的能力”，该评估估计 Claude Opus 4.5 可以以 50% 的成功率完成人类需要近 5 小时才能完成的任务。相比之下，Claude Code 中第 99.9 百分位的轮次时长约为 42 分钟，中位数则短得多。然而，这两个指标不具有直接可比性。METR 评估捕捉的是模型在没有人类交互且没有现实后果的理想化环境中的能力。我们的测量捕捉的是实践中发生的情况，即 Claude 会停下来请求反馈，且用户会进行中断。8 此外，METR 的五小时数据衡量的是任务难度——人类完成该任务需要多长时间——而不是模型实际运行了多长时间。

无论是能力评估还是我们的测量，单独看都无法提供智能体自主性的完整图景，但结合在一起，它们表明在实践中授予模型的自主权落后于它们所能处理的程度。

## 经验丰富的 Claude Code 用户自动批准频率更高，但中断也更频繁

随着时间的推移，人类如何调整他们与智能体合作的方式？我们发现，随着人们使用 Claude Code 的经验增加，他们会授予它更多的自主权（图 2）。新用户（会话数 <50）使用全自动批准的比例约为 20%；到 750 个会话时，这一比例增加到 40% 以上。

这种转变是渐进的，表明信任在稳步积累。同样重要的是要注意，Claude Code 的默认设置要求用户手动批准每个动作，因此这种转变的一部分可能反映了用户在熟悉 Claude 的能力后，配置产品以匹配他们对更高独立性的偏好。

批准动作只是监督 Claude Code 的一种方法。用户还可以在 Claude 工作时中断它以提供反馈。我们发现中断率随着经验的增加而增加。新用户（约 10 个会话）在 5% 的轮次中中断 Claude，而更有经验的用户在约 9% 的轮次中中断（图 3）。

中断和自动批准都随着经验的增加而增加。这种明显的矛盾反映了用户监督策略的转变。新用户更有可能在每个动作执行前予以批准，因此很少需要在执行过程中中断 Claude。经验丰富的用户更有可能让 Claude 自主工作，仅在出现问题或需要重新定向时介入。较高的中断率也可能反映了那些对何时需要干预有更敏锐直觉的用户正在进行积极监控。我们预计每轮中断率最终会随着用户进入稳定的监督风格而达到平台期，事实上，在最有经验的用户中，曲线可能已经趋于平缓（尽管在较高会话数下置信区间变宽使得这一点难以确认）。9

我们在公开 API 上也看到了类似的模式：在极低复杂度的任务（如编辑一行代码）中，87% 的工具调用有某种形式的人类参与，而在高复杂度任务（如自主寻找零日漏洞或编写编译器）中，这一比例仅为 67%。10 这可能看起来违反直觉，但有两个可能的解释。首先，随着步骤数量的增加，逐步批准变得不切实际，因此在复杂任务上监督每个动作在结构上更难。其次，我们的 Claude Code 数据表明，经验丰富的用户倾向于授予工具更多的独立性，而复杂任务可能不成比例地来自经验丰富的用户。虽然我们无法直接测量公开 API 上的用户资历，但总体模式与我们在 Claude Code 中观察到的一致。

综合来看，这些发现表明经验丰富的用户并不一定是在放弃监督。中断率随经验与自动批准同步增加的事实表明存在某种形式的积极监控。这加强了我们之前提出的一个观点：有效的监督并不需要批准每一个动作，而是要处于能够在关键时刻进行干预的位置。

## Claude Code 暂停请求澄清的次数多于人类中断它的次数

当然，人类并不是影响自主性在实践中展开的唯一参与者。Claude 也是一个积极的参与者，当它不确定如何进行时，它会停下来请求澄清。我们发现，随着任务复杂度的增加，Claude Code 请求澄清的次数会增加——而且频率高于人类选择中断它的频率（图 4）。

在最复杂的任务中，Claude Code 请求澄清的次数是极低复杂度任务的两倍多，这表明 Claude 对其自身的不确定性有一定的校准。然而，重要的是不要夸大这一发现：Claude 可能没有在正确的时刻停止，它可能会问一些不必要的问题，而且它的行为可能会受到产品功能（如计划模式 Plan Mode）的影响。无论如何，随着任务变得越来越难，Claude 越来越多地通过停止咨询人类来限制自己的自主权，而不是要求人类主动介入。11

表 1 显示了 Claude Code 停止工作以及人类中断 Claude 的常见原因。

**Claude Code 停止工作的原因是什么？**

这些发现表明，智能体发起的停止是已部署系统中一种重要的监督方式。训练模型识别并根据自身的不确定性采取行动是一项重要的安全属性，它补充了权限系统和人类监督等外部保障措施。在 Anthropic，我们训练 Claude 在面对模糊任务时提出澄清问题，我们也鼓励其他模型开发人员这样做。

## 智能体被用于高风险领域，但尚未形成规模

人们使用智能体做什么？这些部署的风险有多大？这些智能体的自主程度如何？风险是否会与自主性进行权衡？

为了回答这些问题，我们使用 Claude 以 1 到 10 的等级评估公开 API 中单个工具调用的相对风险和自主性。简而言之，风险评分为 1 表示如果出现问题不会产生后果的操作，风险评分为 10 则涵盖可能造成重大伤害的操作。我们以相同的等级对自主性进行评分，低自主性意味着智能体似乎在遵循明确的人类指令，而高自主性意味着它在独立运行。13 然后，我们将相似的操作分组到聚类中，并计算每个聚类的平均风险和自主性评分。

表 2 提供了处于风险和自主性极端的聚类示例。

**具有高风险或高自主性的工具使用聚类**

鉴于我们对这些智能体的可见性有限，这些动作有可能是模拟的（例如，我们无法确定 Claude 下达的金融交易是否真的被执行了，而且我们相信许多“数据外泄”其实是评估测试）。虽然这些高风险动作在整体流量中所占份额很小，但单次错误的后果仍然可能非常严重。

我们发现 80% 的工具调用来自似乎至少拥有一种保障措施（如受限权限或人类批准要求）的智能体，73% 似乎以某种方式有人类参与，只有 0.8% 的动作似乎是不可逆的（例如向客户发送电子邮件）。14

为了可视化所有聚类中风险和自主性的联合分布，我们根据每个聚类在这两个维度上的平均得分进行绘图。图 5 中的每个点对应一个相关动作的聚类，其位置由其平均风险和自主性决定。

公开 API 上的绝大多数动作都是低风险的。但是，虽然大多数智能体部署相对良性，我们也看到了一些处于风险和自主性前沿的新颖用途。15 风险最高的聚类（同样，我们预计其中许多是评估测试）往往涉及敏感的安全相关动作、金融交易和医疗信息。虽然风险集中在量表的低端，但自主性的变化范围更广。在低端（自主性评分为 3-4），我们看到智能体为人类完成小型、范围明确的任务，如预订餐厅或对代码进行微调。在高端（自主性评分高于 6），我们看到智能体向数据科学竞赛提交机器学习模型或分拣客户服务请求。

我们还预计，在风险和自主性极端情况下运行的智能体将变得越来越普遍。今天，智能体集中在单一行业：软件工程占我们公开 API 工具调用的近 50%（图 6）。除了编程之外，我们还看到了一些跨商业智能、客户服务、销售、金融和电子商务的小型应用，但没有一个应用占流量的比例超过几个百分点。随着智能体扩展到这些领域（其中许多领域的利害关系比修复漏洞更高），我们预计风险和自主性的前沿将会扩大。

这些模式表明我们正处于智能体采用的早期阶段。软件工程师是第一批大规模构建和使用智能体工具的人，图 6 表明其他行业也开始尝试使用智能体。16 我们的方法允许我们监控这些模式如何随时间演变。值得注意的是，我们可以监控使用情况是否倾向于向更具自主性和更高风险的任务移动。

虽然我们的头条数据令人欣慰——大多数智能体动作都是低风险且可逆的，而且通常有人类参与——但这些平均值可能会掩盖前沿领域的部署情况。采用在软件工程中的集中，结合新领域中不断增长的尝试，表明风险和自主性的前沿将会扩大。我们将在本文末尾的建议中讨论这对模型开发人员、产品开发人员和政策制定者的意义。

## 局限性

这项研究仅仅是一个开始。我们只提供了智能体活动的部分视图，并且我们想坦诚地说明我们的数据能告诉我们什么，以及不能告诉我们什么：

- **我们只能分析来自单一模型提供商（Anthropic）的流量。** 基于其他模型构建的智能体可能会表现出不同的采用模式、风险状况和交互动态。

- **我们的两个数据源提供了互补但不完整的视图。** 公开 API 流量为我们提供了跨数千个部署的广度，但我们只能孤立地分析单个工具调用，而不是完整的智能体会话。Claude Code 为我们提供了完整的会话，但仅针对一个绝大多数用于软件工程的产品。我们的许多最强有力的发现都基于 Claude Code 的数据，可能无法推广到其他领域或产品。

- **我们的分类是由 Claude 生成的。** 我们为每个维度提供了一个退出类别（例如“无法推断”、“其他”），并在可能的情况下针对内部数据进行验证（详见附录），但由于隐私限制，我们无法手动检查底层数据。某些保障措施或监督机制也可能存在于我们无法观察到的上下文之外。

- **此分析反映了一个特定的时间窗口（2025 年底至 2026 年初）。** 智能体领域变化迅速，随着能力的增长和采用的演变，模式可能会发生变化。我们计划随着时间的推移扩展此分析。

- **我们的公开 API 样本是在单个工具调用级别提取的**，这意味着涉及许多连续工具调用的部署（如具有重复文件编辑的软件工程工作流）相对于通过较少动作完成目标的部署而言，被过度代表了。这种采样方法反映了智能体活动的数量，但不一定反映智能体部署或用途的分布。

- **我们研究了 Claude 在公开 API 上使用的工具以及围绕这些动作的上下文，但我们对客户在公开 API 之上构建的更广泛系统的可见性有限。** 在 API 级别看起来自主运行的智能体可能在下游有人类审查，而我们无法观察到。特别是，我们的风险、自主性和人类参与分类反映了 Claude 从单个工具调用的上下文中可以推断出的内容，并没有区分在生产环境中采取的动作与作为评估或红队演练一部分采取的动作。几个风险最高的聚类似乎是安全评估，这突显了我们对每个动作周围更广泛背景的可见性限制。

## 展望未来

我们正处于智能体采用的早期阶段，但自主性正在增加，更高风险的部署正在出现，特别是随着像 Cowork 这样的产品使智能体变得更加触手可及。下面，我们为模型开发人员、产品开发人员和政策制定者提供建议。鉴于我们才刚刚开始测量现实世界中的智能体行为，我们避免给出强硬的处方，而是强调未来工作的领域。

**模型和产品开发人员应投资于部署后监控。** 部署后监控对于理解智能体的实际使用方式至关重要。部署前评估测试了智能体在受控环境中的能力，但我们的许多发现无法仅通过部署前测试观察到。除了了解模型的能力外，我们还必须了解人们在实践中如何与智能体交互。我们在这里报告的数据之所以存在，是因为我们选择构建了收集这些数据的基础设施。但还有更多工作要做。我们没有可靠的方法将发送到我们公开 API 的独立请求链接成连贯的智能体会话，这限制了我们对第一方产品（如 Claude Code）之外的智能体行为的了解。以保护隐私的方式开发这些方法是跨行业研究和协作的一个重要领域。

**模型开发人员应考虑训练模型识别其自身的不确定性。** 训练模型识别其自身的不确定性并主动向人类反馈问题是一项重要的安全属性，它补充了人类批准流和访问限制等外部保障措施。我们训练 Claude 这样做（我们的分析显示 Claude Code 提出问题的频率高于人类中断它的频率），我们也鼓励其他模型开发人员这样做。

**产品开发人员应为用户监督而设计。** 对智能体的有效监督不仅仅是将人类置于批准链中。我们发现，随着用户获得使用智能体的经验，他们倾向于从批准单个动作转向监控智能体的行为并在需要时进行干预。例如，在 Claude Code 中，经验丰富的用户自动批准更多，但中断也更多。我们在公开 API 上看到了相关的模式，即人类参与似乎随着目标复杂性的增加而减少。产品开发人员应投资于能够让用户对智能体行为具有可靠可见性的工具，以及允许他们在出现问题时重新定向智能体的简单干预机制。这是我们在 Claude Code 中持续投资的方向（例如，通过实时转向和 OpenTelemetry），我们也鼓励其他产品开发人员这样做。

**现在强制执行特定的交互模式还为时过早。** 我们确实有信心提供指导的一个领域是“不应强制执行什么”。我们的研究结果表明，经验丰富的用户会从批准单个智能体动作转向在需要时进行监控和干预。规定特定交互模式的监督要求（例如要求人类批准每个动作）会产生摩擦，而不一定能产生安全效益。随着智能体和智能体测量科学的成熟，重点应该放在人类是否能够有效地监控和干预，而不是要求特定形式的参与。

这项研究的一个核心教训是，智能体在实践中行使的自主性是由模型、用户和产品共同构建的。Claude 通过在不确定时暂停提问来限制自己的独立性。用户在与模型合作的过程中建立信任，并相应地调整他们的监督策略。我们在任何部署中观察到的情况都是由这三种力量共同产生的，这就是为什么它不能仅通过部署前评估来完全表征。理解智能体的实际行为需要对它们进行现实世界的测量，而执行此操作的基础设施仍处于起步阶段。

Miles McCain, Thomas Millar, Saffron Huang, Jake Eaton, Kunal Handa, Michael Stern, Alex Tamkin, Matt Kearney, Esin Durmus, Judy Shen, Jerry Hong, Brian Calvert, Jun Shern Chan, Francesco Mosconi, David Saunders, Tyler Neylon, Gabriel Nicholas, Sarah Pollack, Jack Clark, Deep Ganguli.

如果您想引用此博文，可以使用以下 Bibtex 键：

## 附录

我们在本文的 PDF 附录中提供了更多详细信息。

## 脚注

1. 我们的定义与 Russell 和 Norvig (1995) 的定义兼容，他们将智能体定义为“任何可以被视为通过传感器感知其环境并通过执行器作用于该环境的事物”。我们的定义也与 Simon Willison 的定义兼容，他写道，智能体是一个“循环运行工具以实现目标”的系统。

虽然全面的文献综述超出了本文的范围，但我们发现以下工作有助于构建我们的思考。Kasirzadeh 和 Gabriel (2025) 提出了一个四维框架，用于从自主性、效能、目标复杂性和通用性方面表征 AI 智能体，构建了映射不同类别系统治理挑战的“智能体概况”。Morris 等人 (2024) 根据性能和通用性提出了 AGI 的级别，将自主性视为一个可分离的部署选择。Feng、McDonald 和 Zhang (2025) 根据用户角色定义了五个级别的自主性，从操作员到观察员。Shavit 等人 (2023) 提出了治理智能体系统的实践，而 Mitchell 等人 (2025) 则认为，鉴于风险随自主性而扩展，不应开发完全自主的智能体。Chan 等人 (2023) 主张在广泛部署之前预测智能体系统的危害，强调了奖励黑客、权力集中和集体决策侵蚀等风险。Chan 等人 (2024) 评估了智能体标识符、实时监控和活动日志如何增加 AI 智能体的可见性。

在实证方面，Kapoor 等人 (2024) 批评智能体基准测试忽视了成本和可重复性；Pan 等人 (2025) 调查了从业者，发现生产环境中的智能体往往很简单且受人类监督；Yang 等人 (2025) 分析了 Perplexity 的使用数据，发现生产力和学习任务占主导地位；Sarkar (2025) 发现经验丰富的开发人员更有可能接受智能体生成的代码。在 Anthropic，我们还研究了专业人士如何在内部和外部将 AI 融入他们的工作中。我们的工作通过使用跨 API 和 Claude Code 的第一方数据分析部署模式来补充这些努力，使我们能够获得在外部难以观察到的关于自主性、保障措施和风险的可见性。

2. 因为我们将智能体表征为使用工具的 AI 系统，所以我们可以将单个工具调用分析为智能体行为的构建块。为了了解智能体在世界上正在做什么，我们研究它们使用的工具以及这些动作的上下文（例如动作发生时的系统提示词和对话历史）。

3. 这些结果反映了 Claude 在编程相关任务上的表现，不一定能转化为其他领域的表现。

4. 在本文中，我们有些非正式地使用“自主性”来指代智能体独立于人类指导和监督运行的程度。自主性极低的智能体完全执行人类明确要求的操作；自主性极高的智能体在很少或没有人类参与的情况下，自行决定做什么以及如何做。自主性不是模型或系统的固定属性，而是部署的一种涌现特征，受模型行为、用户监督策略和产品设计的影响。我们不尝试给出一个精确的形式化定义；有关我们在实践中如何操作和测量自主性的详细信息，请参见附录。

5. 此外，以不同方式部署的同一模型可以以不同的速度生成输出。例如，我们最近发布了 Opus 4.6 的快速模式（Fast Mode），其输出生成速度比常规 Opus 快 2.5 倍。

6. 有关其他百分位数的轮次时长，请参见附录。

7. 具体来说，我们使用 Claude 将每个内部 Claude Code 会话分为四个复杂度类别，并确定任务是否成功。在这里，我们报告最难类别任务的成功率。

8. METR 的五小时数据是任务难度的衡量标准（人类完成该任务需要多长时间），而我们的测量反映的是实际经过的时间，这受模型速度和用户计算环境等因素的影响。我们不尝试对这些指标进行推理，包含此对比是为了向可能熟悉 METR 发现的读者解释为什么我们在此报告的数字要低得多。

9. 这些模式来自交互式的 Claude Code 会话，绝大多数反映了软件工程。软件异常适合监督式管理，因为输出可以被测试、轻松比较，并在发布前进行审查。在验证智能体输出需要与生成输出相同的专业知识的领域，这种转变可能会更慢或采取不同的形式。中断率上升也可能反映了经验丰富的用户正在完成更具挑战性的任务，这自然需要更多的人类输入。最后，Claude Code 的默认设置将新用户推向基于批准的监督（因为默认情况下动作不会被自动批准），因此我们观察到的一些转变可能反映了 Claude Code 的产品设计。

10. 复杂度和人类参与度都是通过让 Claude 在完整上下文中分析每个工具调用（包括系统提示词和对话历史）来估计的。完整的分类提示词可在附录中找到。定义人类参与特别困难，因为即使人类没有积极引导对话，许多转录内容也包含来自人类的内容（例如，被审核或分析的用户消息）。在我们的手动验证中，当 Claude 将工具调用分类为没有人类参与时，它几乎总是正确的，但它有时会在没有人类参与的地方识别出人类参与。因此，这些估计应被视为人类参与度的上限。

11. 从某种意义上说，停下来问用户一个问题本身就是一种能动性（agency）。我们使用“限制自己的自主权”来表示 Claude 选择寻求人类的指导，而它本可以继续独立运行。

12. 这些聚类是通过让 Claude 分析每次中断或暂停以及周围的会话上下文，然后将相关原因分组在一起生成的。我们手动合并了一些密切相关的聚类，并为了清晰起见编辑了它们的名称。显示的聚类并非详尽无遗。

13. 我们将这些评分视为比较指标而非精确测量。我们没有为每个级别定义严格的标准，而是依靠 Claude 对每个工具调用周围上下文的一般判断，这使得分类能够捕捉到我们可能未预料到的考虑因素。权衡之处在于，这些评分在将动作相互比较时比在绝对意义上解释任何单一评分更有意义。有关完整提示词，请参见附录。

14. 有关我们如何验证这些数据以及我们的精确定义的更多信息，请参见附录。特别是，我们发现 Claude 经常高估人类参与度，因此我们预计 80% 是具有直接人类监督的工具调用的上限。

15. 我们的系统还会自动排除不符合我们聚合最小值的聚类，这意味着只有少数客户使用 Claude 执行的任务不会出现在此分析中。

16. 软件工程中的采用曲线是否会在其他领域重复是一个悬而未决的问题。软件相对容易测试和审查——你可以运行代码并查看它是否工作——这使得信任智能体并捕捉其错误变得更容易。在法律、医学或金融等领域，验证智能体的输出可能需要付出巨大努力，这可能会减缓信任的建立。


## 关联主题

- [[00-元语/AI]]
- [[00-元语/Agent]]
- [[00-元语/Claude]]
- [[00-元语/benchmark]]
- [[00-元语/evals]]
- [[00-元语/observability]]
- [[00-元语/risk]]
