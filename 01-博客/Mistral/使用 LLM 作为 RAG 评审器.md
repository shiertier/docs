# 使用 LLM 作为 RAG 评审器

## 文档信息

- 发布日期：2026-01-13
- 原文链接：https://mistral.ai/news/llm-as-rag-judge

## 摘要

### 1) 一句话总结
本文介绍了如何结合 Mistral 模型的结构化输出功能，利用“LLM 作为裁判”方法和 RAG 三元组框架来高效、可靠地评估检索增强生成（RAG）系统的性能。

### 2) 关键要点
* **RAG 评估挑战**：RAG 系统通过结合数据检索来减少幻觉，但评估其性能不仅需要检查生成内容的连贯性，还需验证检索源信息的准确性和相关性。
* **LLM 作为裁判**：在缺乏人类评估员的大规模场景中，使用一个“裁判 LLM”根据自定义量表（数字、二元或定性）对“生成器 LLM”的回答进行评分是一种高效的解决方案。
* **RAG 三元组框架**：由 TruLens 引入的评估框架，用于端到端评估 RAG 系统的可靠性和上下文完整性，包含三个核心指标。
* **指标一：上下文相关性 (Context Relevance)**：检查检索到的文档是否与用户的查询高度契合。
* **指标二：事实依据 (Groundedness)**：验证生成的回答是否准确地基于检索到的上下文数据。
* **指标三：回答相关性 (Answer Relevance)**：评估最终生成的回答在多大程度上解决了用户的原始查询。
* **结构化输出 (Structured Outputs)**：Mistral API 的新功能，可强制 LLM 以机器可读的结构化格式（如特定模式）输出结果。
* **方案结合**：将结构化输出与 RAG 三元组结合，可构建出高度可靠、易于机器读取和解析的“LLM 作为裁判”评估流程。
* **实践资源**：Mistral 官方提供了一份完整的 Jupyter Notebook 代码示例，供开发者实现用于 RAG 评估的“LLM 作为裁判”。

### 3) 风险与缺口
* **幻觉风险**：LLM 存在生成看似自信但实际上是“幻觉”内容的风险。
* **传统方法的局限性**：传统评估方法往往忽略了在源头验证检索信息是否相关且准确的细微差别。
* **评估资源匮乏**：许多领域在衡量模型性能时，面临缺乏明确的定量指标、评估数据以及人类评估员的缺口。

## 正文

使用 Mistral 模型将 LLM 作为裁判（结合结构化输出）

大型语言模型 (LLM) 正迅速成为创建广泛使用的应用程序的必备工具。但是，确保这些模型按预期运行说起来容易做起来难。评估 LLM 系统不仅要验证输出是否连贯，还要确保回答具有相关性并满足必要的要求。

## RAG 系统的兴起

检索增强生成 (RAG) 系统已成为提升 LLM 能力的一种流行方式。通过将 LLM 与数据检索系统相结合，LLM 生成的回答不仅连贯，而且能以相关且最新的信息为事实依据。这有助于减少模型听起来自信满满但实际上可能在产生“幻觉”的情况。

然而，评估这些 RAG 系统的性能并非易事。这不仅仅关乎 LLM 生成的输出听起来是否正确，还关乎在源头验证检索到的信息是否相关且准确。传统方法往往会忽略这些细微差别，因此一个全面的评估框架显得尤为重要。

退一步讲，许多领域都面临着同样的问题：可能缺乏明确的定量指标或评估数据来衡量性能，而衡量成功的标准可能更加偏向定性且微妙。

### LLM 作为裁判：使用 LLM 评估其他 LLM

在必须大规模运行评估且缺乏人类评估员的场景中，“LLM 作为裁判 (LLM As A Judge)”已成为评估 LLM 系统回答的流行解决方案。

“LLM 作为裁判”的典型工作方式是创建一个“裁判 LLM”，在获得“生成器 LLM”的回答后，指示其根据给定的量表对该回答进行评分。这个量表可以是数字（例如 1-10 或 0-3 的量表）、二元（例如 True 或 False）或定性的（例如“优秀”、“良好”、“一般”、“差”）。然后可以对评估数据集中所有问题的评分进行平均，从而得出一个整体的加权分数。

对于“LLM 作为裁判”的指令提示词 (instruction prompt)，您可以创建自定义标准，让 LLM 据此评估回答。例如，您可以创建特定的评分标准，检查回答是否准确、相关且有事实依据。Mistral 的模型可以有效地用于 LLM 系统的生成器和裁判组件。

### RAG 三元组：更全面的评估框架

RAG 三元组 (The RAG Triad) 是一种用于评估 RAG 系统的流行评估框架，旨在检查 LLM 基于数据源生成的回答的可靠性和上下文完整性。它由 TruLens 引入，并且为了更定性地评估 RAG 系统，业界还创建了几个类似的框架（如 RAGAS）。

RAG 三元组侧重于三个关键领域：

1. **上下文相关性 (Context Relevance)**：该指标检查检索到的文档是否与用户的查询高度契合。它确保用于生成回答的信息是相关且适当的，从而为连贯且有用的回答奠定基础。

2. **事实依据 (Groundedness)**：该指标验证生成的回答是否准确地基于检索到的上下文。它确保 LLM 的输出是符合事实且可靠的，从而降低产生那些令人头疼的幻觉的风险。

3. **回答相关性 (Answer Relevance)**：该指标评估最终回答在多大程度上解决了用户的原始查询。它确保生成的回答是有帮助且切题的，符合用户的意图并提供有价值的见解。

通过使用 RAG 三元组，评估人员可以全面了解 LLM 的性能，发现潜在问题，并确保回答准确、可靠且在上下文中合理。该框架有助于在人类、AI 和知识库之间建立值得信赖的交互。

这在端到端评估 RAG LLM 系统时特别有用，因为它包含了对检索和生成步骤的评估。这不仅有助于评估 LLM 的回答，还能评估其相对于检索到的源数据的正确性。

### 结构化输出：一种实用的解决方案

自定义结构化输出 (Structured Outputs) 是我们 API 最近推出的一项功能，它允许您控制 LLM 的输出格式，并以结构化的机器可读方式提供值，从而获得更高的可靠性。

Mistral 的模型结合结构化输出，提供了一种实用的方法来实现 RAG 三元组并应对常见的评估挑战。通过为数值设定明确的标准和模式 (schema)，结构化输出可帮助您构建一个更可靠的“LLM 作为裁判”，用于评估 LLM 系统。

结构化输出建立了一个完美的框架，用于定义新的、可重用的标准，并强制执行一种易于机器读取以进行评估的格式。

## 结论

评估 LLM 系统对于开发可靠的 AI 应用程序至关重要。RAG 三元组与 Mistral 的结构化输出一起，为使用 LLM 作为裁判评估 LLM 性能提供了一个强大的框架。通过关注上下文相关性、事实依据和回答相关性，我们可以确保 LLM 的回答更加准确和有意义。这种方法提升了用户体验和 AI 驱动应用程序的可靠性，从而与 AI 系统建立更值得信赖的交互。

### 完整代码

您可以在此处找到用于 RAG 的“LLM 作为裁判”的完整代码：https://github.com/mistralai/cookbook/blob/main/mistral/evaluation/RAG_evaluation.ipynb

### 联系我们

有兴趣与 Mistral AI 团队进行更多定制合作吗？请联系我们获取解决方案支持。

## 分享本文

## 更多来自 Mistral AI 的内容

- 新闻

- 模型

- AI 服务

## AI 的下一章由您书写。

AI 的下一章由您书写。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/rag]]
- [[00-元语/evals]]
