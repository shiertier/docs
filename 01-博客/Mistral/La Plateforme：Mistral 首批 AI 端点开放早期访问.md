---
title: "La Plateforme：Mistral 首批 AI 端点开放早期访问"
发布日期: 2026-02-24
作者: "Mistral"
来源: "Mistral"
原文链接: "https://mistral.ai/news/la-plateforme"
译注: "未找到官方中文版本，本文基于英文原文翻译整理。"
---

## 摘要

**1) 一句话总结**
Mistral AI 开放了其开发者平台 La Plateforme 的 Beta 测试，首批提供三个不同性价比的文本生成端点以及一个嵌入端点。

**2) 关键点**
*   **模型技术**：提供基于公开网络数据预训练及标注数据指令微调的模型，采用高效微调和直接偏好优化（DPO）技术进行对齐。
*   **Mistral-tiny 端点**：最具成本效益，基于可下载的 Mistral 7B Instruct v0.2 模型，仅支持英语，MT-Bench 得分为 7.6。
*   **Mistral-small 端点**：基于最新的 Mixtral 8x7B 模型，支持英、法、意、德、西五种语言及代码，MT-Bench 得分为 8.3。
*   **Mistral-medium 端点**：质量最高的原型模型，支持上述五种语言及代码，MT-Bench 得分为 8.6。
*   **Mistral-embed 端点**：专为检索设计的嵌入模型，维度为 1024，MTEB 检索得分为 55.26。
*   **开发者工具**：API 兼容主流聊天接口规范，提供 Python 和 Javascript 客户端库，支持通过系统提示词进行高级别的内容审核。
*   **生态合作**：与 NVIDIA 合作整合了 TensorRT-LLM 和 Triton，使稀疏专家混合模型（MoE）兼容 TRT-LLM。
*   **访问权限**：已开放注册，平台容量正逐步提升，业务团队可协助加速获取访问权限。

**3) 风险/不足**
*   在平台稳定并过渡到完全自助服务的过程中，用户可能会遇到一些系统不完善的情况。

## 正文

# La Plateforme

我们的首批 AI 端点现已开放抢先体验。

Mistral AI 为开发者带来了最强大的开源生成式模型，以及在生产环境中部署和定制这些模型的高效方法。

今天，我们开放了首批平台服务的 Beta 测试权限。我们从简单的开始：La Plateforme 提供了三个用于根据文本指令生成文本的聊天端点，以及一个嵌入端点。每个端点都有不同的性能与价格权衡（性价比）。

前两个端点 mistral-tiny 和 mistral-small 目前使用的是我们已发布的两个开源模型；第三个端点 mistral-medium 使用的是一个性能更高的原型模型，我们正在部署环境中对其进行测试。

我们提供模型的指令微调版本。我们致力于整合最有效的对齐技术（高效微调、直接偏好优化），以创建易于控制且使用体验良好的模型。我们在从公开网络提取的数据上对模型进行预训练，并根据标注数据进行指令微调。

Mistral-tiny。这是我们最具成本效益的端点，目前提供 Mistral 7B Instruct v0.2，这是 Mistral 7B Instruct 的一个全新小版本更新。
Mistral-tiny 仅支持英语。它在 MT-Bench 上获得了 7.6 分。
可以在此处下载该指令微调模型。

Mistral-small。该端点目前提供我们最新的模型 Mixtral 8x7B，我们的博客文章中对其进行了更详细的描述。
它精通英语/法语/意大利语/德语/西班牙语以及代码，在 MT-Bench 上获得了 8.3 分。

Mistral-medium。这是我们质量最高的端点，目前提供一个原型模型，
根据标准基准测试，该模型目前位列可用服务模型的前列。它精通英语/法语/意大利语/德语/西班牙语以及代码，在 MT-Bench 上获得了 8.6 分。下表比较了 Mistral-medium、Mistral-small 的基础模型与竞争对手端点的性能。

Mistral-embed，我们的嵌入端点，提供了一个具有 1024 嵌入维度的嵌入模型。我们的嵌入模型在设计时充分考虑了检索能力。它在 MTEB 上获得了 55.26 的检索分数。

我们的 API 遵循由我们最尊敬的竞争对手最初提出的流行聊天接口规范。我们提供 Python 和 Javascript 客户端库来查询我们的端点。我们的端点允许用户提供系统提示词，以便在对此有重要需求的应用中，对模型输出设置更高级别的内容审核。

随着我们逐步提升容量，从今天起任何人都可以注册使用我们的 API。我们的业务团队可以帮助评估您的需求并加速访问权限的获取。在我们稳定平台以实现完全自助服务可用性的过程中，可能会遇到一些不完善之处，敬请谅解。

我们感谢 NVIDIA 在整合 TensorRT-LLM 和 Triton 方面对我们的支持，并与我们并肩工作，使稀疏专家混合模型（sparse mixture of experts）能够兼容 TRT-LLM。

## 分享本文

## 更多来自 Mistral AI 的内容

- 新闻

- 模型

- AI 服务

## AI 的新篇章由你书写。

AI 的新篇章由你书写。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
