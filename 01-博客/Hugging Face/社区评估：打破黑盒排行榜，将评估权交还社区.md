---
title: "社区评估：打破黑盒排行榜，将评估权交还社区"

来源: "https://huggingface.co/blog/community-evals"
---

## 摘要

**1) 一句话总结**
Hugging Face 推出了一种去中心化且透明的社区评估机制，允许用户通过拉取请求（PR）公开提交模型评估分数，并在基准数据集和模型卡片之间实现数据的自动汇总与互联。

**2) 关键要点**
*   **行业痛点**：当前基准测试（如 MMLU、GSM8K）得分已趋于饱和，但与模型在真实世界中的表现存在明显鸿沟，且不同来源报告的分数往往不一致。
*   **去中心化机制**：推出透明的评估报告机制，初期从 MMLU-Pro、GPQA 和 HLE 等 4 个精选基准测试开始。
*   **基准测试端**：数据集仓库可注册为基准测试，通过 `eval.yaml`（基于 Inspect AI 格式）定义评估规范以确保可复现，并自动汇总全站结果展示排行榜。
*   **模型端**：评估分数以 YAML 格式存储在模型仓库的 `.eval_results/` 目录中，同步显示在模型卡片和基准数据集中。
*   **社区参与**：任何人均可通过 PR 提交评估结果（带有“社区”标签），无需等待模型作者合并即可展示，并可链接至论文、第三方平台或评估日志。
*   **作者权限**：模型作者有权关闭社区提交的分数 PR 并隐藏特定结果。
*   **数据开放与透明**：所有分数通过 Hub API 开放，方便构建定制化仪表盘；基于 Git 的底层架构确保了评估添加与修改的历史记录完全透明。

**3) 风险与不足**
*   该机制无法彻底解决基准测试分数饱和的问题。
*   无法消除基准测试得分与模型现实世界表现之间的鸿沟。
*   不能阻止模型在测试集上进行训练（数据污染）。
*   该功能目前仍处于 Beta 阶段。

## 正文

**核心摘要：** Hugging Face 上的基准数据集现在可以托管排行榜了。模型可以存储自身的评估分数，且所有内容相互关联。社区成员可以通过 PR（拉取请求）提交结果，经过验证的徽章将证明这些结果具备可复现性。

### 评估机制的现状与困境

让我们认清 2026 年模型评估的现状：MMLU 的准确率已经饱和并超过 91%，GSM8K 突破了 94%，HumanEval 也已被攻克。然而，根据实际使用反馈，一些在基准测试中表现优异的模型，仍然无法可靠地浏览网页、编写生产级代码，或在不产生幻觉的情况下处理多步任务。基准测试分数与真实世界表现之间存在着明显的鸿沟。

此外，报告的基准分数本身也存在差异。从模型卡片（Model Cards）、学术论文到各大评估平台，不同来源报告的结果往往不一致。这导致社区缺乏一个统一、权威的真实数据来源。

### 我们推出了什么

**去中心化且透明的评估报告机制。**

我们将通过去中心化的报告方式，允许整个社区公开报告基准测试分数，从而引领 Hugging Face Hub 上的模型评估走向新的方向。初期我们将从 4 个精选基准测试开始，并随着时间的推移扩展到更多相关的基准测试。

*   **对于基准测试（Benchmarks）：** 数据集仓库现在可以注册为基准测试（MMLU-Pro、GPQA 和 HLE 现已上线）。它们会自动汇总来自整个 Hub 的报告结果，并在数据集卡片中展示排行榜。基准测试通过 `eval.yaml`（基于 Inspect AI 格式）定义评估规范，确保任何人都可以复现。报告的结果必须与任务定义保持一致。
*   **对于模型（Models）：** 评估分数存储在模型仓库的 `.eval_results/*.yaml` 文件中。它们会显示在模型卡片上，并同步到基准数据集中。模型作者的结果和**公开的 PR 结果**都会被汇总。模型作者也有权关闭分数 PR 并隐藏特定结果。
*   **对于社区（The Community）：** 任何用户都可以通过 PR 为任何模型提交评估结果。这些结果会以“社区（community）”的标签展示，无需等待模型作者合并或关闭。社区可以链接到论文、模型卡片、第三方评估平台或 `inspect` 评估日志等来源，并像讨论普通 PR 一样讨论这些分数。由于 Hub 是基于 Git 的，因此添加评估、修改内容的时间等历史记录都将一目了然。

### 为什么这很重要

去中心化的评估机制将整合社区中（如模型卡片和论文中）已经存在的分数。通过公开这些数据，社区可以在此基础上进行汇总、追踪，并深入了解整个领域的评估现状。同时，所有分数都将通过 Hub API 开放，方便开发者聚合数据并构建定制化的排行榜和仪表盘。

社区评估并不会取代现有的基准测试，因此传统的排行榜和带有公开结果的封闭评估仍然至关重要。但我们认为，基于可复现的评估规范为行业贡献开放的评估结果同样重要。

这并不能彻底解决基准测试饱和的问题，也无法消除基准与现实之间的鸿沟，更不能阻止模型在测试集上进行训练。但它通过公开“评估内容、评估方式、评估时间以及评估者”，让整个博弈过程变得透明可见。

最重要的是，我们希望将 Hub 打造成一个积极构建和分享可复现基准测试的平台，特别是聚焦于那些能对前沿（SOTA）模型提出更大挑战的新任务和新领域。

### 如何开始

*   **阅读文档：** 查阅官方文档以了解更多关于评估结果的信息。
*   **添加评估结果：** 将您执行的评估以 YAML 文件的形式发布到任何模型仓库的 `.eval_results/` 目录中。
*   **查看分数：** 浏览基准数据集上的最新得分。
*   **注册新基准测试：** 在您的数据集仓库中添加 `eval.yaml`，并联系我们以加入精选列表。

*注：该功能目前处于 Beta 阶段。我们坚持公开构建，欢迎随时提供反馈。*

## 相关文档

- [[01-博客/Hugging Face/RTEB：检索评测新标准.md|RTEB：检索评测新标准]]；关联理由：延伸思考；说明：两文都围绕“基准分数与真实表现存在鸿沟”提出改进路径，前者聚焦检索场景，后者聚焦社区化评估治理；
- [[01-博客/Hugging Face/IBM与加州大学伯克利分校：使用IT-Bench和MAST诊断企业级智能体失败原因.md|IBM与加州大学伯克利分校：使用IT-Bench和MAST诊断企业级智能体失败原因]]；关联理由：观点一致；说明：两文都强调要打破单一分数黑盒，转向可追溯、可解释的评估信息；

## 关联主题

- [[00-元语/evals]]
- [[00-元语/benchmark]]
- [[00-元语/community]]
- [[00-元语/llm]]
