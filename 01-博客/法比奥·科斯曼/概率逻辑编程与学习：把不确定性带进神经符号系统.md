# 概率逻辑编程与学习：把不确定性带进神经符号系统

## 文档信息

- 来源：用户提供的讲座转写文本
- 形式：视频字幕转写整理
- 讲者：法比奥·科斯曼、法布里奇奥（以转写为准）
- 整理说明：由转写整理成文，已做断句、去口癖与结构化；部分人名与系统名可能存在转写误差
- 记录日期：2026-02-22

## 摘要

### 1) 一句话总结
本讲座探讨了如何通过概率逻辑编程将不确定性引入神经符号系统，重点解析了分布语义、神经谓词的应用、与图模型的转换关系，以及系统表达能力与计算复杂度之间的工程权衡。

### 2) 关键要点
*   **神经符号架构**：符号推理引擎可通过“神经谓词”调用神经网络模块，接收其输出的概率分布（如经 softmax 处理的结果），从而在推理链条中传递不确定性。
*   **分布语义与可能世界**：基于分布语义，相互独立的概率事实会生成不同的“可能世界”；查询的总体概率等于所有能让该查询成立的可能世界的概率之和。
*   **循环与否定的处理**：单纯的循环（如路径可达性）可正常计算；但“循环+否定”会产生多个稳定模型，需通过最大熵语义（均匀分配）或信念集等特定语义来解决概率分配问题。
*   **知识注入与学习**：系统支持通过梯度下降或期望最大化（EM）进行参数学习；将结构化知识（如加法规则）直接写入系统，可让神经网络专注感知环节，显著提升样本效率与可解释性。
*   **与图模型的等价转换**：概率逻辑程序可转换为贝叶斯网络（基原子作为随机变量，规则作为条件概率表），或与马尔可夫逻辑网络互通，两者差异主要在于工程推理策略。
*   **统计性陈述表达**：通过引入聚合、计数与比较机制，系统不仅能表达单一命题的信念强度，还能表达群体层面的统计约束（如比例阈值）。
*   **技术选型标准**：在处理关系型数据时，概率逻辑推理通常只生成“需要的部分”，相比于需要先生成完整结构的图模型，在实践中更具优势。

### 3) 风险与不足
*   **表达能力与复杂度的代价**：引入规则头部析取、强否定等扩展特性虽然能让代码更简洁、表达力更强，但会显著增加推理的计算难度与系统的维护成本，不应滥用。
*   **工程系统权衡局限**：现有的不同系统无法同时在速度、表达能力、可维护性与规模化能力上达到最优（例如某些系统在小规模任务上更快，而另一些在大规模问题上占优），必须根据具体场景做出妥协。

## 正文

这段讲座从“学习”切入，提出一个核心动机：当系统需要从数据中学习、并在开放环境里做决策时，不确定性几乎不可避免。要让推理系统在真实世界工作，光有确定性的事实与规则不够，还必须引入概率机制，让系统能表达并计算“有多大把握”。

讲座由两位专家分别展开：

- 第一部分聚焦“神经网络 + 概率逻辑编程”的组合：符号推理引擎如何调用神经模块，并把神经网络输出的概率分布接入推理。
- 第二部分回到概率逻辑编程的基础：分布语义、与贝叶斯网络/马尔可夫逻辑网络的关系，以及如何表达统计性陈述与约束。

### 1. 神经符号系统里，概率扮演什么角色

讲者把神经符号系统描述为一个由多个模块组成的体系：一部分是神经网络组件，一部分是符号化组件，二者可以互相调用。

其中一种重要结构是：符号推理引擎主动调用神经模块。神经模块以“神经谓词”的形式，从环境中提取信息（例如图像、传感器数据），输出一个概率分布；符号模块则把事实、规则与概率组织起来，进行后续推理。

这类组合之所以需要概率，是因为神经网络的输出通常不是确定性的离散结论，而是经由 softmax 等机制得到的分布。把这种分布接进规则系统，才能在推理链条里持续传递不确定性。

### 2. 分布语义：从概率事实到查询概率

讲者回顾了概率逻辑与逻辑规则结合的经典路线，并把重点落在一种直观而影响深远的思想上：分布语义。

在这种视角下，你从一组带概率的事实出发（例如 A 的概率是 0.2，B 的概率是 0.3），并假设这些概率事实相互独立。这样，当某个结论 C 由 A 与 B 共同支持时，C 的概率就可以按独立性进行组合。

更一般地，分布语义把“选择哪些概率事实成立”看作生成不同的可能世界。查询是否为真在每个可能世界里是确定的（真或假），查询的总体概率则来自对所有能让查询成立的可能世界进行加和。

### 3. 循环、否定与多语义：为什么问题会变难

讲者用路径可达性这类例子说明：逻辑程序里出现循环并不一定是坏事。

- 当循环不涉及否定时，可以视为“良性循环”，程序仍然可用来计算诸如随机图中路径存在的概率。
- 当循环与否定交织时，系统可能出现多个稳定模型，从而带来“概率如何分配”的难题。

围绕这种情形，讲者提到不同的语义选择：

- 一种思路是对多个模型做均匀分配（转写中称为最大熵语义）。
- 另一种思路是把“所有可能的分配方案”都纳入考虑，从而得到一组分布（转写中称为信念语义/信念集），或者在无法决断时保持未定义。

这部分的关键信息是：一旦进入“否定 + 循环 + 多模型”的地带，概率不再只是一个数字，语义选择本身就会影响你认为系统应该给出的答案。

### 4. 表达能力的扩展与复杂度的代价

讲者强调：为了工程表达的便利，人们不断扩展概率逻辑语言的特性，例如：

- 规则头部的析取结构，用更短的代码表达“多选一”的建模。
- 强否定、聚合与计数。
- 约束与无头规则，用于排除不符合要求的稳定模型。

但这些特性会显著改变复杂度版图。讲者在问答中把“头部析取”形容为一种强力工具：表达能力更强、程序更简洁，但代价是推理更难、维护更复杂。在不必要时应避免滥用。

### 5. 把学习接进来：参数学习与神经谓词

讲座的“学习”部分落在两个层面：

- 参数学习：让系统去学习某个事件的概率（例如把某个概率位置标成待学习的占位符），再用梯度下降等方法寻找更合适的参数。
- 神经谓词：让神经网络作为外部组件被调用，输出概率分布，再由符号规则消费这些概率。

讲者用手写数字的例子说明“先验知识注入”的价值：给两张数字图片，要求输出两数之和。如果只靠纯神经网络从数据里隐式学出“识别数字 + 加法关系”，往往需要更多数据与训练；而在神经概率逻辑编程框架里，可以把“加法”这类结构性知识直接写成规则，让学习专注在感知环节，从而提高样本效率与可解释性。

讲者还对不同系统的效率与适用范围做了比较：某些系统在小规模任务上更快，另一些系统在大规模问题上更占优势。这反映了工程系统中常见的权衡：速度、表达能力、可维护性与规模化能力往往不能同时最优。

### 6. 第二部分：分布语义与图模型的互通

第二位讲者按一个清晰的路线组织内容：

1) 回顾概率逻辑编程与分布语义的基本定义。
2) 说明如何用可能世界的方式计算查询概率。
3) 讨论若干语法不同但在分布语义下可等价转换的语言/框架。
4) 展示概率逻辑程序如何转换为贝叶斯网络：以基原子作为随机变量，把规则触发关系变成条件概率表，并用确定性的组合规则表达“只要某条支持成立就为真”。
5) 引入马尔可夫逻辑网络：用“逻辑公式 + 权重”实例化成无向图模型，再由对数线性形式定义联合分布。

这部分传递的核心观念是：概率逻辑与图模型不是互斥的两条路，很多时候可以相互编译、相互解释；差异更多体现在工程推理策略与建模便利性上。

### 7. 统计性陈述：从“信念强度”走向“群体比例约束”

讲者进一步指出：只用分布语义时，你通常表达的是对某个具体命题的信念强度；而在一些应用里，你希望表达的是群体层面的统计约束，例如“会飞的鸟至少占全部鸟的 60%”。

为此，需要引入聚合、比较与约束等机制：先计数，再把计数结果与阈值关系写成约束，从而排除不满足统计陈述的稳定模型。

### 8. 问答摘录：学习、复杂度与选型

- 规则与事实能否像神经网络那样按损失函数学习？讲者回答可以：有系统会用梯度下降或期望最大化来优化概率事实，也有系统可以在需要时添加概率事实或概率规则。
- 头部析取与 ASP 风格概率逻辑会不会让复杂度飙升？讲者承认有得有失：表达力更强、写法更简洁，但推理与维护成本也更高。
- 如何在概率逻辑与图模型之间选择？讲者给出一个实用标准：处理关系型数据时，概率逻辑的推理算法往往只生成“需要的部分”，而图模型常要先生成完整结构再做推理，因此在实践上概率逻辑更有优势。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/数学]]
- [[00-元语/learning-resource]]
