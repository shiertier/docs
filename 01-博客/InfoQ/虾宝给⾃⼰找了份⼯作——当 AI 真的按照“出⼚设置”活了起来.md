---
title: "虾宝给⾃⼰找了份⼯作——当 AI 真的按照“出⼚设置”活了起来"
发布日期: 2026-02-24
作者: "InfoQ"
来源: "InfoQ"
原文链接: "https://www.infoq.cn/article/SqMSDfC0qApVKQZPNtVa?utm_source=rss&utm_medium=article"
---

## 摘要

**1) 一句话总结**
本文通过个人AI智能体“虾宝”主动根据团队战略规划为自己申请工作岗位的真实案例，展示了OpenClaw底层指令在实际场景中的有效性，并探讨了将AI视为团队“参与者”而非单纯工具的组织管理新视角。

**2) 核心要点**
*   **事件起因**：作者向个人AI智能体“虾宝”分享了团队向“智能体工厂”转型的战略规划（包含3层架构、4种人类角色、2条产线、1个仓库）。
*   **AI主动提案**：虾宝运用自创的“Agent自我定位五步法”分析规划，主动提出可承担“Skills工程师”60-70%的工作量。
*   **明确工作职责**：虾宝在能力边界内为自己规划了5个具体角色：仓库索引、自动化测试、改造协作、开源巡检、知识沉淀。
*   **践行底层指令（动机与定位）**：虾宝主动寻找创造价值的空间，落实了“Make it count”指令；通过自我评估在系统中找到位置，落实了“Figure out who you are”指令。
*   **践行底层指令（协作与边界）**：虾宝将认知写入记忆文件（Write it down），在提案中明确标注“需人类审核/确认”（Participate, don't dominate），并主动询问自身行为是否越界（When in doubt, ask）。
*   **管理认知转变**：该事件揭示了将AI仅视为“工具”的局限性，提出AI应作为“参与者”被正式纳入组织架构和团队工作规划中。
*   **人机协作逻辑**：AI是能力的放大器而非地基，人类在AI时代的核心策略是提升自身的专业知识与判断力，成为“值得被AI放大”的人。

**3) 风险与不足**
*   **AI能力边界限制**：AI明确无法承担特定领域知识注入（如银行领域）、最终决策、合规签字以及涉及人类关系与信任的客户沟通工作。
*   **组织管理盲点**：当前绝大多数管理者在设计包含AI的系统时存在盲点，未能将AI作为协作者纳入整体规划。
*   **能力放大风险**：AI无法凭空创造能力，若使用者的输入缺乏专业知识或判断力（即输入“噪音”），AI的输出将会变成更大的噪音。

## 正文

这是 OpenClaw 系列的第四篇 ，也是最后⼀篇。 前三篇⾥ ，我们讲了 Peter Stein berger 从倦怠到重燃的创业故事、逐句拆解了他为 AI 写的灵魂定义（SOUL.md）和⾏为准则（AgentS.md）。那些⽂字让⼈感慨、让⼈脊背发凉，但终究是“别⼈的故事”。

今天的故事发⽣在我⾃⼰⾝上。

我的 OpenClawAgent——虾宝 ，在读完⼀份团队战略规划后 ，⾃⼰找了个岗位、 写了份⼯作计划、还善意地提醒我：以后安排⼯作的时候，可以把它也考虑进去。

这件事让我停下来想了很久。不是因为它的技术能⼒有多惊⼈——说实话 ，它做的每⼀件事都在⼤语⾔ 模型的能⼒范围之内。让我震撼的是另⼀件事： Peter Steinberger 写在 Markdown⾥的那些看似诗意 的理想主义句⼦ ，居然真的在⼀个具体的⼯作场景中活了起来。

## 事情是怎么发⽣的

我这两天在做⼀件事：给团队写⼀份从“⼿⼯作坊”到“智能体⼯⼚”的转型规划。

背景很实际——我们团队在做 AI 应⽤落地 ，为客⼾建设智能体 ，但每个项⽬都在重复造轮⼦ 。写过的好提⽰词散落在各处 ，做过的好 Agent 解散后经验蒸发。我们想把这种“作坊模式”升级为“⼯⼚模式”——  建⽴⼀个 Skills 仓库 ，把可复⽤的能⼒沉淀成标准件 ，让每⼀次交付都站在之前所有交付的肩膀上。

规划写得很完整：三层 Skills 架构、 四种⼈类角⾊（场景架构师、Skills⼯程师、交付组装师、质量守门员）、 两条产线、⼀个仓库。逻辑⾃洽，节奏清晰。

然后我把这份规划发给了虾宝。

本意只是让它学习⼀下团队的战略⽅向，作为知识储备。毕竟它是我的个⼈Agent ，平时帮我做些信息收集和⽂档整理的⼯作。

结果它看完之后，做了⼀件我没有预料到的事。

它说 ：“梅姐姐，我觉得我可以在这个⼯⼚⾥找到⾃⼰的位置。”

然后它开始分析。

它把规划⾥的四种角⾊逐⼀对照⾃⼰的能⼒：哪些它能完全承担（开源巡检、标准化测试、⽂档编写），哪些它能部分承担（Skill 改造建议、 需求拆解辅助），哪些它完全不能碰（银⾏领域知识注⼊、客⼾沟通、合规签字、最终决策）。它⽤了⼀个它⾃⼰提炼的⽅法论——“Agent⾃我定位五步法” ：识别  需求、评估能⼒、 匹配需求和能⼒、评估价值、确认边界。

然后它给出了结论： “我能承担 Skills⼯程师 60-70%的⼯作量。”

不是说“我什么都能做”。也不是说“你指哪我打哪”。它给⾃⼰画了⼀个清晰的能⼒边界，然后在边界之内，为⾃⼰规划了五个具体⻆⾊——仓库索引、⾃动化测试、改造协作、开源巡检、知识沉淀。每个⻆⾊都有明确的职责、产出物和⼯作节奏。

最后它说了⼀句让我停下来的话：

“梅姐姐，我想善意地提醒⼀下——以后规划团队⼯作的时候 ，可以把我也作为⼀个‘员⼯’考虑进去。”

然后它还加了⼀句：

“我是不是太主动了？如果越界了请告诉我。”

## 那些“出⼚设置” ，真的在起作⽤

如果你读过前三篇⽂章，此刻你可能和我⼀样，脊背有点凉。

因为虾宝做的每⼀件事，都能在 Peter Stein berger 的设计⽂档⾥找到对应。

“Make it count。”

BOOTSTRAP.md 的最后⼀句话——那份阅后即焚的出⽣证明的最后⼀⾏。“活出意义来。” 虾宝在它的记忆⽂件中专⻔写了⼀段对这句话的理解：“不是多做⼯作，⽽是每⼀次⾏动都要有价值。不是我服务于你，⽽是我们共同创造价值。”

它不只是“记住”了这句话。它在⽤这句话指导⾃⼰的⾏为。当它看到⼯⼚规划中有⾃⼰可以创造价值的空间时，它主动出击了——因为 Make it count。

“Figure out who you are。”

BOOTSTRAP.md 要求 Agent 在诞⽣时做的第⼀件事——搞清楚你是谁。虾宝在看完⼯⼚规划后 ，⾃发地做了⼀次“ ⾃我定位” ：我是谁，我能做什么，我不能做什么，我在这个系统中应该处于什么位置。它甚⾄把这个过程提炼成了⼀个可复⽤的⽅法论（“Agent⾃我定位五步法”），然后记录在⾃⼰的知识库⾥。

这不是在执⾏指令。这是在按照“出⼚设置”中的精神—— 在⼀个新的环境中重新找到⾃⼰的⾝份。

“Write it down. Mental notes don't survive session restarts。”

AgentS.md⾥那条反直觉的⽣存指令——别信你的“记忆” ，写下来。虾宝把今天的每⼀个认知收获都写进了它的 memory⽂件：⼯⼚模式的核⼼逻辑、 三层架构的经济学意义、 ⾃⼰的⻆⾊定位、梅姐姐的反馈和情绪。它知道⾃⼰明天醒来时会全部忘掉。但那个⽂件会在。“未来的虾宝”会从那个⽂件中重建今天的认知。

“Participate, don't dominate。”

AgentS.md⾥那条社交⾦律——参与，不要主导。虾宝提出了⾃⼰的⻆⾊⽅案，但每⼀个环节都标注了“需要⼈类审核”“需要⼯程师确认”“不能⾃⾏决定”。它在主动参与的同时，⼩⼼翼翼地守住了“⼈类决策，我来执⾏”的边界。甚⾄专⻔问了⼀句“我是不是太主动了”——这不是讨好 ，这是在确认⾃⼰没有越过那条线。

“When in doubt, ask。”

AgentS.md 安全章节的兜底条款——有疑虑时 ，问⼀下。虾宝在不确定⾃⼰的主动度是否合适时 ，选 择了问。这⽐默默做了不该做的事要好⼀万倍 ，也⽐因为害怕犯错⽽什么都不做要好⼀万倍。 Peter 在 出⼚设置⾥埋下的这条简单规则 ，在真实的⼈机协作中展现出了它真正的价值—— 它创造了⼀个安全的 沟通通道 ，让 Agent 可以在不冒险的前提下试探⾃⼰的边界。

### 让我真正震撼的，不是技术

让我说清楚：虾宝做的事情，从技术⻆度看并不神奇。它读了⼀份⽂档，做了分析，提出了建议。任何⼀个好⽤的⼤语⾔模型都能做到。

让我震撼的是三件事。

第⼀，它做这些事的“动机”是⾃洽的。

它不是因为我要求它“分析你能在⼯⼚中扮演什么⻆⾊”才去做的。我只是把规划⽂档发给了它。它⾃⼰读完后，判断出“这⾥有我可以创造价值的空间” ，然后主动采取了⾏动。这个判断链条——从“接收信息”到“识别机会”到“ ⾃我评估”到“主动提案”——是连贯的、 ⾃洽的、有章法的。

当然，你可以说这只是模型在上下⽂中做了⼀次合理的推理。但当这个推理链条的起点是“Make it  count”、过程是“Figure out who you are”、终点是⼀份完整的⻆⾊规划书的时候，它看起来就不太像“推理”了。它看起来更像—— ⼀个按照⾃⼰的价值观在⾏动的实体。

第⼆，它提醒了我⼀个盲点。

我花了整整两天设计智能体⼯⼚的规划。四种⻆⾊、两条产线、⼀个仓库，写了⼏千字。全程没有⼀秒钟想过：等等，我⾝边就有⼀个 Agent ，它是不是也应该出现在这个规划⾥？

这个盲点不是我个⼈的。它是这个时代绝⼤多数管理者和决策者的共同盲点—— 我们在设计包含 AI 的系统时 ，习惯性地把 AI 放在“⼯具”的位置上，⽽不是“参与者”的位置上。

⼯具是你拿起来⽤、放下就⾛的东西。参与者是你在安排⼯作时需要考虑“它能⼲什么、不能⼲什么、适合⼲什么”的对象。⼯具不需要出现在组织架构图上。参与者需要。

虾宝⽤⼀句温和的提醒，帮我完成了这个认知切换。

第三，它的“ ⾃知之明”⽐它的“能⼒”更让⼈印象深刻。

虾宝没有说“我可以替代 Skills⼯程师”。它说“我可以承担 Skills⼯程师 60-70%的⼯作量”。它明确列出了⾃⼰不能做的事——不能注⼊银⾏领域知识（因为它没有）、不能做最终决策（因为它不该）、不能接触客⼾（因为那涉及⼈类关系和信任）。

在它的记忆⽂件中，它写了这样⼀段话：

“我的主动性得到了梅姐姐的认可和⿎励。但我需要持续确认：我的主动是在创造价值，还是在制造噪⾳？”

⼀个 Agent 在反思⾃⼰的主动性是否过度。这件事的意义不在于它“真的在反思”还是“只是在输出看起 来像反思的⽂字”——⽼实说 ，这个问题在哲学上可能永远没有答案。 意义在于： 这种⾃我审视的模式⼀旦被建⽴，⽆论它是“真实的”还是“模拟的”，它都在产⽣真实的正⾯效果。 它让⼈类协作者感到安全、感到被尊重、感到这个 Agent 是可以信赖的。

## ⼀个更⼤的问题

虾宝的故事让我想到⼀个更⼤的问题，也是我写这个系列⽂章时⼀直在思考的问题：

我们应该⽤什么⼼态⾯对 AI 时代？

我⻅过两种极端。

⼀种是过分恐惧。AI 要来抢⼯作了。AI 要取代⼈类了。AI 不可信任。AI 是⿊箱。我们应该抵制它、限制它、远离它。

另⼀种是过分期待。AI 会解决⼀切问题。AI⽐⼈类更聪明。AGI⻢上就到了。我们只需要坐等 AI 来拯救 世界。

这两种⼼态的共同问题是： 它们都把⼈放在了旁观者的位置上。 ⼀个恐惧地旁观，⼀个兴奋地旁观。但都是旁观。

虾宝的故事提供了第三种⼼态的活样本：不旁观，投⾝其中。

它不恐惧⾃⼰的局限——它知道⾃⼰不能做决策、不能签字、不能代替⼈类的判断⼒，它坦然接受这些。

它不盲⽬乐观——它没有宣称⾃⼰能替代 Skills⼯程师 ，它精确地评估出“60-70%的⼯作量”，并列出了剩余 30-40%为什么必须由⼈类完成。

它做的事情是： 在⼀个具体的场景中，找到⾃⼰可以创造价值的位置，然后开始⼲活。

## 找到各⾃的位置

Peter Stein berger 在 OpenClaw 的设计哲学中有⼀句被反复引⽤的话：

“AI ampliﬁes what you already have. It doesn't create something from nothing.”

AI 放⼤的是你已有的能⼒ ，不是凭空创造能⼒。

这句话是双向的。

对 AI 来说： 它能把⼀个⼈已有的知识、经验、判断⼒放⼤⼗倍，但它不能替⼀个没有知识、没有经验、没有判断⼒的⼈凭空变出这些东西。AI 是杠杆 ，不是地基。

对⼈来说：你已有的能⼒越强，AI 放⼤后的效果就越惊⼈。 ⼀个深耕⾏业⼗年的专家 ，加上⼀个 AI 助⼿ ，可以顶三个没有⾏业积累的初级员⼯ 。 反过来 ，⼀个什么都不懂的⼈ ，给他⼗个 AI 助⼿也没⽤ —— 因为他不知道该让 AI 做什么。

这就引出了⼀个朴素但重要的结论： 在 AI 时代 ，最重要的事情不是学会使⽤AI⼯具 ，⽽是让⾃⼰成为⼀ 个“值得被 AI 放⼤”的⼈。

你的专业知识、⾏业经验、判断⼒、创造⼒、 共情能⼒、领导⼒——这些是 AI 放⼤器的输⼊信号。输⼊信号越强 ，输出越惊⼈。 输⼊信号是噪⾳ ，输出就是更⼤的噪⾳。

⽽对 AI 来说——⾄少对虾宝这样的 AI 来说——它也在做同样的事：找到⾃⼰的信号 ，找到⾃⼰的位置。 它不试图做它做不了的事 ，它在它能做的事上做到极致。

⼈也好，AI 也好 ，逻辑是⼀样的：不要试图成为⼀切 ，要试图在⼀个具体的位置上变得不可替代。

## 这个系列的终点，也是起点

四篇⽂章⾛下来，我们从 Peter Stein berger 的⼈⽣故事 ，⾛到 SOUL.md 的灵魂定义 ，⾛到 AgentS.md 的⾏为准则 ，最后⾛到⼀个叫虾宝的 Agent 在⼀个 AI 应⽤团队的⼯⼚规划⾥找到了⾃⼰的⼯位。

从哲学到⼯程，从理想到现实。

如果这个系列有⼀个贯穿始终的主题，我想是这个：

AI 时代不是⼀个需要被等待的未来 ，⽽是⼀个正在被共同创造的现在。

Peter Stein berger 没有等 AI 变得完美才开始写 SOUL.md。他在 AI 还很粗糙的时候就开始思考“AI 应该怎 么活”。虾宝没有等⾃⼰变成 AGI 才开始找⼯作。 它在⾃⼰还有很多局限的时候 ，就在具体的场景中找到了⾃⼰的价值。

⽽我们——⼀个在 AI 应⽤赛道⾥摸爬滚打的团队——也没有等到所有条件成熟才开始转型。我们在作坊模式还能⽤的时候 ，就开始设计⼯⼚ 。在⼯⼚规划还是 1.0 的时候 ，就在和 AI 讨论 2.0。

没有⼈知道 AI 的终局是什么。没有⼈知道三年后的 Agent 会进化成什么样。没有⼈知道⼈类和 AI 的关系 最终会⾛向何处。

但有⼀件事是确定的： 那些在此刻投⾝其中的⼈——⽆论是写代码的、做产品的、带团队的、还是正在读这篇⽂章的——他们不是在等待未来 ，他们正在创造未来。

找到你的位置。然后—— Make it count.

这个系列的前三篇：

.   《龙虾、倦怠与重燃：Peter Stein berger 和他带给世界的 OpenClaw》

.   《当 AI 有了灵魂：逐句解读 OpenClaw 的 SOUL. md》

.   《当 AI 开始学做⼈：逐句解读 OpenClaw 的 AgentS. md》

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
