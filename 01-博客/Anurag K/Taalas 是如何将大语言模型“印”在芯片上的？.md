---
title: "Taalas 是如何将大语言模型“印”在芯片上的？"
---

## 摘要

**1) 一句话总结**
初创公司 Taalas 开发了一款固定功能 ASIC 芯片，通过将大语言模型的权重直接物理硬连线到硅片上并采用片上 SRAM，打破了传统 GPU 的“内存墙”，实现了比 GPU 快 10 倍、成本和能耗均降低 10 倍的高效推理。

**2) 关键点**
*   **极致性能：** 该芯片运行 Llama 3.1 8B（3/6 bit 量化）模型的推理速度达 17,000 Tokens/秒，相比传统 GPU 系统，速度提升约 10 倍，拥有成本和耗电量均降低 10 倍。
*   **固定功能架构：** 芯片为固定功能 ASIC，类似于游戏卡带，只能容纳并运行单一模型，无法重写。
*   **打破“内存墙”：** 摒弃了传统 GPU 在计算核心与显存（VRAM/HBM）之间频繁传输数据的低效模式，将模型的 32 层结构按顺序直接物理蚀刻在芯片上。
*   **单晶体管创新：** 采用创新的硬件方案，使用单个晶体管即可同时存储 4-bit 数据并执行相关的乘法运算。
*   **流水线数据流：** 计算结果不再存入外部内存，而是作为电信号顺着物理导线直接流入下一层晶体管，直至生成最终 Token。
*   **拥抱 SRAM：** 放弃外部 DRAM/HBM，使用片上 SRAM 来存储 KV Cache（上下文窗口）和微调所需的 LoRA 适配器，同时规避了 DRAM 的供应链风险。
*   **敏捷制造方案：** 采用包含通用逻辑门的基础芯片设计，只需定制顶部的两层掩膜（Masks）即可映射特定模型，Llama 3.1 8B 芯片的开发周期仅为两个月。

**3) 风险/不足**
*   **缺乏灵活性：** 芯片无法重写，一旦制造完成便只能运行固化在其中的单一模型。
*   **硬件周期滞后于软件迭代：** 尽管两个月的芯片开发周期在硬件领域极快，但在“一周等于一年”的 AI 模型快速迭代背景下，仍显得相对缓慢。

## 正文

一家名为 Taalas 的初创公司最近发布了一款 ASIC 芯片，能够以每秒 17,000 个 Token 的推理速度运行 Llama 3.1 8B（3/6 bit 量化）模型。这速度相当于一秒钟写满约 30 页 A4 纸。他们声称，与基于 GPU 的推理系统相比，其拥有成本降低了 10 倍，耗电量减少了 10 倍，且速度比目前最先进的推理技术快 10 倍左右。

作为一名拥有软件背景且对大语言模型（LLM）感兴趣的爱好者，我最初很难理解如何将 LLM 直接“印”在芯片上。在查阅了多篇博客、LocalLLaMA 论坛的讨论以及相关硬件概念后，我发现这比想象中更加有趣。

### 基础概念：固定功能 ASIC

Taalas 成立仅两年半，这是他们的首款芯片。该芯片是一种固定功能 ASIC（专用集成电路）。它就像 CD-ROM、游戏卡带或印刷书籍一样，只能容纳一个模型，且无法重写。

### 传统 GPU 的低效与“内存墙”

LLM 由连续的层（Layers）组成。例如，Llama 3.1 8B 有 32 层，每一层的任务是进一步细化输入数据，其本质上是巨大的权重矩阵（即模型的“知识”）。

在传统的 GPU 上，处理过程如下：
* 用户输入的提示词被转换为数字向量（即 Embeddings）。
* 输入向量进入计算核心。
* GPU 从 VRAM/HBM（显存）中提取第 1 层的权重，进行矩阵乘法，然后将中间结果（激活值）存回 VRAM。
* 接着，GPU 提取第 2 层的权重和上一步的结果，进行计算，再次存入 VRAM。
* 这个循环会一直持续到第 32 层，仅仅为了生成**一个** Token。
* 为了生成下一个 Token，GPU 必须重新重复这完整的 32 层旅程。

这种不断来回的数据传输会导致内存总线产生延迟，并消耗大量能源。这就是所谓的“内存带宽瓶颈”，有时也被宽泛地称为冯·诺依曼瓶颈或“内存墙”。

### Taalas 的破局之道：物理硬连线

Taalas 完全避开了这堵“内存墙”。他们直接将 Llama 3.1 的 32 层按顺序刻在了芯片上。本质上，模型的权重就是蚀刻在硅片上的物理晶体管。

此外，他们声称发明了一种硬件方案（我们可以称之为“魔法乘法器”）：**使用单个晶体管即可存储 4 位（4-bit）数据并执行相关的乘法运算。**

现在的工作流程变成了：
* 用户输入转化为向量后，直接流入构成第 1 层的物理晶体管。
* 通过“魔法乘法器”完成乘法运算。
* 结果不再存入 VRAM，而是作为电信号顺着物理导线（据理解是通过流水线寄存器）直接流入第 2 层的晶体管。
* 数据在硅片中持续流动，直到生成最终的输出 Token。

### 内存管理：放弃 DRAM，拥抱 SRAM

Taalas 不使用外部的 DRAM 或 HBM，而是使用少量的片上 SRAM。

* **为什么选 SRAM？** 出于成本和复杂性的考虑，制造商通常不会将 DRAM 与逻辑门混合在一起（这也是 GPU 需要独立 VRAM 的原因）。此外，SRAM 目前没有面临供应链危机，而 DRAM 则不然。
* **SRAM 的用途：** Taalas 使用片上 SRAM 来存储 KV Cache（当前对话的临时内存/上下文窗口），并用于存放微调所需的 LoRA 适配器。

### 制造成本与周期

为每个模型制造定制芯片听起来非常昂贵。但 Taalas 的解决方案是：设计一款包含海量、通用逻辑门和晶体管网格的基础芯片。

为了将特定模型映射到芯片上，他们只需要定制顶部的两层/掩膜（Masks）。虽然这仍然需要时间，但比从头开始构建芯片要快得多。

他们花了两个月的时间开发出适用于 Llama 3.1 8B 的芯片。在“一周等于一年”的 AI 领域，这听起来很慢；但在定制芯片领域，这已经是极其惊人的速度了。

对于那些受限于没有大型 GPU、只能在笔记本电脑上运行本地模型的人来说，这种硬件的尽早量产无疑是非常值得期待的。

## 相关文档

- [[01-博客/Simon Willison/Taalas 推出定制硬件：以每秒 1.7 万 Token 运行 Llama 3.1 8B|Taalas 推出定制硬件：以每秒 1.7 万 Token 运行 Llama 3.1 8B]]；关联理由：同一事件；说明：两篇都围绕 Taalas 同一代芯片发布与 17,000 Token/s 性能声明展开。
- [[01-博客/QbitAI/24人初创团队硬刚英伟达：新芯片HC1推理速度达每秒17000个Token|24人初创团队硬刚英伟达：新芯片HC1推理速度达每秒17000个Token]]；关联理由：解说；说明：该文补充了结构化 ASIC、两层掩膜映射与成本对比等背景，可作为本文技术细节延伸。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/lora]]
- [[00-元语/benchmark]]
