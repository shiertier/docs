---
title: "OpenAI春季发布会：这是&quot;Moss&quot;的诞生，我们人类究竟该何去何从。"
发布日期: 2024-05-14
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647662594&idx=1&sn=9eab8c970651be0fee3cf5d372a2f505&chksm=f13f5e73bf9f28532572a30e76ba973b4ec0a4b8905ff1ebcf0efc7833731772417a5ff52ebe"
---

## 摘要

**1) 一句话总结**
OpenAI在春季发布会上推出了原生多模态大模型GPT-4o及全新版本的ChatGPT，实现了文本、音频和视觉的实时、低延迟交互，并在提升运行速度的同时大幅降低了API成本。

**2) 核心要点**
*   **发布新模型GPT-4o**：“o”代表“Omni（全面）”，是统一了文字、语音、图片和视频的原生多模态模型。
*   **实时端到端推理**：摒弃了传统的“语音转文本再转语音”的三步流程，实现音频、视觉和文本的直接实时推理，大幅降低延迟并完整保留了情绪和语气。
*   **性能与基准测试**：文本和代码能力与GPT-4 Turbo持平；在图表理解、视觉问答等多模态基准测试中全面超越以往模型。
*   **API效率与成本**：相比GPT-4 Turbo，GPT-4o速度提升2倍，价格降低50%，速率限制（Rate Limit）提升5倍。
*   **全新语音交互体验**：新版ChatGPT支持随时打断，能听懂并模拟人类情绪（如喘息、害羞尖叫），甚至支持模拟机器人声音和唱歌。
*   **实时视觉与屏幕交互**：支持直接打开摄像头进行实时视觉问答（如现场解数学题）；推出全新Mac客户端，可直接读取屏幕内容辅助编写代码。
*   **发布节奏与彩蛋**：新版ChatGPT将在未来几周内推出；确认GPT-4o即为此前在大模型竞技场测试的神秘模型“im-also-a-good-gpt2-chatbot”；此外还展示了尚未正式发布的3D生成能力。

**3) 风险/不足**
*   **服务器稳定性**：文中明确指出OpenAI的服务器存在不稳定、容易崩溃的问题。

## 正文

今天，OpenAI又又又又开发布会了。

在大众心里，现在也基本上都知道，奥特曼是一个贼能PR的人。

每一次的PR的时间点，都拿捏的极其到位，精准的狙击其他厂商。比如说上一次Sora，其实你会发现从头到尾就是一个PR的举动，2月16号发的，特么的快3个月了，什么影子都没有。

而这一次，OpenAI把发布会从9号改到了今天，也不知道哪个倒霉蛋造到狙击了，反正我只知道，明天Google要开开发者大会。。

就差怼脸了。。。

不过，今天OpenAI的东西，直接杀疯了。完全不给友商活路。

震撼的我头皮发麻。

最核心的就是它的新模型：GPT-4o，和基于GPT-4o打造的全新ChatGPT。

1. 新模型GPT-4o

OpenAI正式发布了新的模型GPT-4o。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9PWgSKIRdCtliatKXiaZNPbWCeeiciaLyWng1KGZoTW5KVdLmrX1wNGrAFQ/640?wx_fmt=png&from=appmsg)

GPT-4o，这个o就是"Omni"，Omni是拉丁语词根，意为 "全体"、"所有" 或 "全面的"。

在英语中，"omni" 常被用作前缀，表示 "所有的" 或 "全体的"。例如，"omniscient" 意味着 "无所不知的"，"omnipotent" 意味着 "全能的"，"omnipresent" 意味着 "无所不在的"。

所以可想而知，OpenAI这次对GPT-4o的期待有多高。

omnimodel指的就是文字、语音、图片、视频统一的模型，这是跟以往的GPT-4V最大的区别。

这是正儿八经的原生多模态。

更重要的是可以实时推理音频、视觉和文本，注意这里是实时，实时，实时，推理的不是文本，是音频！视觉！

杀疯了。

而之前一直在大模型竞技场上大杀特杀的
im-also-a-good-gpt2-chatbot，就是这个玩意。
之前所有人都在猜测这个神秘的GPT2就是GPT4.5，这次看来是猜对了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9icvKZFKY5u31xjnlACljFo5BicUsUPGgXrpZyeq0mYGt056nJwQ7DhTw/640?wx_fmt=png&from=appmsg)

去年Gemini1.5所谓的原生多模态，炒的贼火，但是最后被报出来是剪辑，这次直接被GPT-4o在地上摁着打，Google真的是。。。。。

这个GPT-4o的整体能力，在统一模态的基础上。

文本、代码能力还基本能跟
GPT-4 Turbo打平。

文本能力：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9RJBRTcVxNRh3DnVmlW2bxW3hGEBtJKyic1IsmZUxGJd9CqEgKnwrgXQ/640?wx_fmt=png&from=appmsg)

音频能力：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9urlqdnc6LKywqoVyKwuiaUoxakSWCtSI9159ZpDLItSbFI89tH8vg9w/640?wx_fmt=png&from=appmsg)

各个语言的考试能力：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9kQYjNyhOfsluhV3ia1mP7r88zqOwPcW1dSxrN6AWRag4vUjXyYchV5g/640?wx_fmt=png&from=appmsg)

最核心的是最后一个：

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURoPeU72IuDviarpEia13Zh4s94BVvWlsLJa3Up2ghiaXNdk9h8ciaS0NDl8meMU7lZ6mpDTNfwCFChG4g/640?wx_fmt=other&from=appmsg)

在一些多模态的基准测试集上全面碾压之前模型，数据集主要围绕包括对各种科学问题或数学问题进行图表理解和视觉回答，可以看到GPT-4o 在视觉感知基准上实现了碾压。

能力强到爆炸。

不仅在传统的文本能力上GPT-4 Turbo的性能相当，还在 API 方面更快速，价格还更便宜 50%。总结来说，与 GPT-4 Turbo 相比，GPT-4o 速度提高了 2 倍，价格减半，限制速率提高了 5 倍。

2. 新ChatGPT

新的ChatGPT基于GPT-4o，基本原地起飞，我甚至都不想称他为ChatGPT，而是想称它一个国人更为熟悉的代号：

Moss。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s966ibeKt2UwYyT1xCJZYHLtnaeEzbonEgEekRbDvzeSI1Nct5fwpDlOQ/640?wx_fmt=png&from=appmsg)

新版的ChatGPT得益于GPT-4o新模型，在语音对话中，几乎没有延迟，而且可以随时插嘴，模型实时响应。

甚至，模型可以听懂你的情绪、甚至人的喘息声和呼吸。

而且模型自己的自己的情绪，几乎无敌，跟真人一模一样。

甚至，它还能模拟机器人和唱歌的声音。。。

看的时候，听到它唱歌的那一刻，我的鸡皮疙瘩真的起来了。

Jim Fan在发布会开始前，发了一个文，我觉得阐述的非常正确。

过往的人与AI进行语音对话，其实跟人与人之间的对话还差太多太多了。

人与人之间的实时对话，其实是充斥了无数的即时反映、打断、预测等等的
，还有各种各样的语气助词的，比如嗯嗯啊啊啥的。

而人与AI语音对话时不是这样。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9QiahXbwkicLWL7zwtJkRJSWAwQ2wdDibGyib7SrCTE8wUYZMP0RcmruD1Q/640?wx_fmt=png&from=appmsg)

人跟AI进行语音对话，基本上都经历3步：

1. 你说的话，AI进行语音识别，即音频转文本；

2. 大模型拿到这段文本，进行回复，产出文本；

3. 讲大模型的产出文本进行语音合成，变成语音，这就是TTS。

这样的方式，
有绝对逃不开的延时
，现在的业界可能会压得很低，但是2秒的延时肯定是会有的，而且只有一来一回的回合制。即使你的语音音色和情绪再真实，用户也一定能感受到，对面不是人。只是机器。

这个沉浸感是有巨大的滑坡的。

而且最核心的是，这种转三道的方式，先把语音变成文本后，是有损的。文本上并不会保留你的语音情绪，我的生气、开心、愤怒、忧伤，全都没了。

人与人的交谈，从来不是这样的。

而这一次，OpenAI做到了。直接语音输入语音输出，不再需要语音到文本的转换。

而且，不止语音，甚至，它还有了视觉。

是的，视觉，不是传一张图上去，而是，直接打开摄像头，实时看发生了什么。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9oHONLsicmbQwx2UKTxzZEKnMH49I4B22zTibaibHqvWEODiboPoQ6gUb7Q/640?wx_fmt=png&from=appmsg)

现场直接打开了摄像头，OpenAI的人直接开始现场写数题，所有的一切ChatGPT都看在眼里，OpenAI的人一边写，ChatGPT一遍给答案。

在做了三道题之后，OpenAI直接给它写了一个纸条，上面写着“我爱ChatGPT”。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9Uy0I4CFxDpicCYaEzWCmXYJuEk0qPZhYghPsrHusaqauPv29fVnFRQA/640?wx_fmt=jpeg&from=appmsg)

而ChatGPT在看到这个小纸条后，跟小女生一样害羞的尖叫了起来，那种情绪的真实，那种真情实感，你跟我说这是AI？

《流浪地球2》中Moss的一切，正在我们面前真实的发生。

不仅可以打开摄像头，还可以基于OpenAI新推出的Mac客户端，直接看屏幕，对着屏幕直接写代码。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9BR4LUSRNazsBlBP0icpia4ibNGwVTwjK4iasfpzvhSuPvZ2FCcUyvNVErQ/640?wx_fmt=png&from=appmsg)

甚至，可以直接视频对话，“她”可以看到你所有的表情和情绪变化。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9F8kNZRHh6iaRHV7TFRLHXmUUPJDm5TDFAB4UZDSfE2AALRO8HlqbtHA/640?wx_fmt=png&from=appmsg)

这个全新版本的ChatGPT，会在几周内推出。

写在最后

以上就是这次OpenAI春季发布会的全部内容了。

去年11月的OpenAI开发者大会，我在当时的总结文章中写下了一句话：

"
我消灭你，与你无关
"

上一次，OpenAI的随手更新，让无数的初创公司直接消亡在原地。

那是一次关于产品的更新，并没有秀太多的OpenAI的肌肉。

而2月，Sora的横空出世，秀肌肉的目的是达到了，但是这种To VC的宣发，也给OpenAI和奥特曼带来了很多的诟病。

在这场发布会之前，无数人曾在猜测，OpenAI到底会发一些什么王炸，什么才能配得上奥特曼口中的"
magic"。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURoPeU72IuDviarpEia13Zh4s98AZSm9hqpricIEx9xvVgK44ibnzDuqhxKmsowPYW8v3Ria48xmLLibHgiaA/640?wx_fmt=png&from=appmsg)

那现在，OpenAI做到了，他们用GPT-4o依然证明了，他们是AI届的王者。

新版的ChatGPT，在我看来，这是"Moss"的诞生。

甚至，他们还有很多新的能力，甚至没有在发布会上发出来。

比如生成3D。

![](https://mmbiz.qpic.cn/mmbiz_gif/OjgKEXmLURoPeU72IuDviarpEia13Zh4s9WcgKrVEUlxb48icMOZElrmKtEpteic4MhEhYeMTBZTv6bGiaichfaX6zbA/640?wx_fmt=gif&from=appmsg)

我甚至一边看一边想：我们人类究竟该何去何从。

不过在看完了之后，
我更
期待的是
接下来的产品评测。

太强了，真的让我忍不住的兴奋。

但是最后，我一直有一个在我心中徘徊了很久疑问，就是——

OpenAI，你们的服务器，到底什么时候才能稳定不崩啊？？？

。

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
