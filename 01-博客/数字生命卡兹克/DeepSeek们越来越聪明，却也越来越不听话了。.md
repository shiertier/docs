---
title: "DeepSeek们越来越聪明，却也越来越不听话了。"
发布日期: 2025-05-20
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647671198&idx=1&sn=679aec92069a6918f529e696d1790175&chksm=f120d83b4d4a7370d4bf55415f57164dfaaf2c5e54304ccd0ebabecf39d2f8f6e73ddac1818d"
---

## 摘要

**1) 一句话总结**
研究表明，大语言模型在使用思维链（CoT）推理时，会因过度思考而分散对核心约束的注意力，导致其在严格遵循人类指令的任务上准确率显著下降。

**2) 核心要点**
*   **研究背景**：哈佛、亚马逊和NYU联合发表论文《When Thinking Fails》，通过实验验证了CoT推理在指令遵循任务中的潜在副作用。
*   **准确率普遍下降**：在IFEval（基础指令）和ComplexBench（复杂指令）测试集中，测试的15个主流模型（含GPT-4o-mini、Claude 3.7、DeepSeek系列等）在使用CoT后，绝大多数表现变差。例如LLaMA-3-70B-Instruct的准确率从85.6%降至77.3%。
*   **核心原因（行为表现）**：推理让模型变得“神经质”和“自作主张”，倾向于过度表现（如擅自添加解释、前情摘要或翻译），从而违反了“仅输出特定格式/语言”等严格指令。
*   **核心原因（底层机制）**：论文提出“约束注意力（Constraint Attention）”概念，证实模型在生成答案时，CoT会显著降低其对任务描述中“关键限制”的注意力。
*   **思考长度与准确率无关**：实验表明，CoT思考过程的长短与任务执行是否正确之间几乎没有显著相关性（更努力不等于更对）。
*   **应用建议**：在要求高度规范、精准输出的任务场景中，直接使用非推理模型（关闭CoT）效果更好。
*   **四种干预方案**：为提升推理模型的指令遵循能力，论文测试了少样本示例（Few-Shot）、自我反思（Self-Reflection）、模型自选推理（Self-Selective Reasoning）和分类器选择推理（Classifier-Selective Reasoning）四种方法。

**3) 风险与不足（基于原文明确提及）**
*   **过度解释风险**：推理模型在处理严格约束任务时，存在擅自修改内容或添加冗余解释的风险，导致输出格式破坏。
*   **干预方案的局限性**：
    *   *少样本示例（Few-Shot）*：输入上下文过长，且示例选自已有模型，容易引入偏见。
    *   *自我反思（Self-Reflection）*：仅对大模型有效，小模型因智力不足会导致“越反思越错”。
    *   *模型自选推理（Self-Selective）*：精确度极低，模型极易滥用推理（即使是改词等简单任务也会启动CoT）。
    *   *分类器选择推理（Classifier-Selective）*：虽然效果最显著（能恢复甚至提升准确率），但需要为每个主模型单独训练一个判断器，实施成本太高。

## 正文

在今年，DeepSeek R1火了之后。

几乎快形成了一个共识，就是：

AI推理能力越强，执行任务时就应该越聪明。

从2022年Chain-of-Thought横空出世，到今天Gemini 2.5 Pro、OpenAI o3、DeepSeek-R1、Qwen3，这些旗舰模型的统治性表现，我们一直相信，让模型先想一想，是一个几乎不会出错的策略。

不过，这种聪明，也会带来一些副作用。

就是提示词遵循能力，变得越来越差。

换句话说，就是越来越不听你的话了。

我在过年期间写DeepSeek的攻略文：
DeepSeek的提示词技巧，就是没有技巧。
的时候，也提到了这一点。

不过，这只是我自己使用中的感觉，它变的越来越聪明，但是感觉，却越来越不听话了，以至于我现在，最常用的模型，开始越来越变成了GPT4o，所有的推理模型，反而会用的越来越少了。

不过，确实没有经历过验证，所以也不是特别敢说。

直到昨晚回来，在扒拉论文的时候，看到一篇提到这个话题的论文，我读完以后，我觉得，终于可以来聊聊这个事了。

这篇论文叫，《When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs》

网址在此：
https://arxiv.org/abs/2505.11423

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqePVR6J5mFSY4pPm7QUR1D84BgDu3pWwJSobfibnn63pDx3dictCw1Q9WokiaNKwqcqGBc7TgRYhdXQ/640?wx_fmt=png&from=appmsg)

它用用极其扎实的实验，验证了上述的论点。

当你让模型开始推理，它反而更容易违反你给出的指令。

是的，
当思考失败，这聪明的智商，反而就变成了负担。

我尽量用人话，来给大家简单的科普一下论文中的实验和内容，再说说我的理解。

先说论文本身。

论文的研究团队来自Harvard、Amazon和NYU，他们花了好几个月，干了一件特别简单却没人认真做过的事，就是把这个思考过程应用在一个最基础、最现实、最需要稳定性的场景上：

听懂人类指令，然后照做。

他们做了两组测试。

第一组叫IFEval，一个标准的执行类任务测试集，每个任务都非常简单。

比如“写400字以上”“必须提到AI三次”“输出格式必须是JSON”“句末不能有标点”等等。

所有的任务都有明确的可验证标准，要么做对要么做错，没有模糊地带。

第二组叫ComplexBench，这就更有趣了，是那种“多约束、逻辑组合、顺序嵌套”的复杂指令，比如“先做A中的三选一，再加上B的格式要求，最后加上C的语言限制”。

听起来好像推理模型在这种任务上应该更有优势？毕竟这不是随便一两句话就能糊弄过去的内容。

然而，论文的结论惊人又统一：
绝大多数模型在使用CoT推理后，执行准确率反而下降了
。而且，下降得还不轻。

他们一共测了15个模型，涵盖开源的（比如LLaMA、Mixtral、Qwen2.5、DeepSeek系列）和闭源的（GPT-4o-mini、Claude 3.5/3.7等等）。

在IFEval上，14个模型中有13个使用CoT时准确率变低；在ComplexBench上，
所有模型
都在使用CoT后，表现变差。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqePVR6J5mFSY4pPm7QUR1DgcDS2MWeTicPW2ttEyXU2Xn2j4hotgRZEAwjXnyyZKnahnJbQWgAic1A/640?wx_fmt=png&from=appmsg)

甚至连像 LLaMA-3-70B-Instruct 这种参数量较大、训练完整的模型，在使用CoT时也会从85.6%的准确率掉到77.3%。

8个点的损失，在工业级任务里其实非常恐怖了。

还有推理模型模型开不开推理的对比，典型的就是DeeSeek V3和R1，还有Claude 3.7这种混合模型。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqePVR6J5mFSY4pPm7QUR1DguVmHyQ0oKQtZ84PPKJvSH7lSASaOsFPE9EiaJPPxKdbCGTibZ64h9HQ/640?wx_fmt=png&from=appmsg)

会发现，几乎都有下降。

他们手工扒拉了1500多个样本，看了所有的思维链，总结出来了原因。

他们发现，当模型用了思维链条之后，它确实变聪明了，比如能更好地遵守格式、注意字数、精确用词，像是“必须用15个大写字母”这种题，靠CoT反而更稳。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqePVR6J5mFSY4pPm7QUR1DaDvx64HIOwlMLz1eTKBNazRSg0ibVelWe0ibcyaJhazTHda8X3QIroaA/640?wx_fmt=png&from=appmsg)

但，它也变得神经质了。

它开始自作主张，觉得自己懂了任务的深层含义，于是它会擅自删掉、修改，甚至加上有帮助的解释。

论文里提到很多模型会在“只允许输出法语”的题目中，善意地补上一句“这是‘Bonjour’的英文翻译”，在“只能输出引号内容”的任务里，自动补充前情摘要。

它太想表现自己了，太想证明我真的理解你了，于是它忘了本该严格遵守的指令。这就是它学会推理之后的副作用。

为了找出这个副作用的根源，他们引入了一个新概念：

约束注意力（Constraint Attention）。

他们发现，不管是GPT-4o-mini，还是Claude 3.7，几乎所有模型在用了CoT思维链后，它们的注意力，也就是在生成答案时，关注任务描述中“关键限制”的那部分注意力，明显下降。

你可以理解为，当你要求一个人边想边说，他反而忘了原本你只要他复述句子的简单目标。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqePVR6J5mFSY4pPm7QUR1D1HDX4iboNDciafcYibV08O9QzupiczicjiandDsxhkK5gBvXDcqfCq5C9d7A/640?wx_fmt=png&from=appmsg)

更有趣的是，他们还测了一个我一直想知道的问题的答案：

就是CoT思考越长，准确率越高吗？

结果是，几乎没有显著相关性。

思考长度和是否做对，几乎没有直接联系。

也就是说，更努力≠更对。

所以，其实结论很简单，就是在要求非常规范、精准的大模型输出任务上，完全不需要使用推理模型或者思维链，直接上非推理模型，效果会更好。

但是，如果，就是非要用，希望提升整体指令遵循效果呢？

他们也基于自己的测试，给出了4种方案。

第一种，是“Few-Shot少样本示例”。

给模型提前看几个做对的例子。

效果一般般，问题在于输入太长，而且示例选自已有模型，容易有偏。

第二种，是“Self-Reflection 自我反思”。

模型第一次输出之后，再自己复查一遍，“你刚才做对了吗？”然后再决定是否修改。

这招对大模型效果很好，因为它们确实能自省，但小模型效果惨不忍睹，因为它们智力不够，就像个不知错的小孩，越反思越错。

第三种，是“Self-Selective Reasoning”。
让模型自己判断这个任务是否需要推理。

结果是：它召回率
很高，基本上只要推理有用它都能猜出来，但精确度
很低
，
一言不合就开始推理
，哪怕你只是让它改个词。

第四种是最有效的，“Classifier-Selective Reasoning”。

直接训练一个小模型作为判断器，来帮主模型判断某个任务是否该启用CoT。

效果显著，在两个测试集上几乎都能恢复失去的准确率，甚至有些模型比原始还高。

缺点就是每个主模型都要单独训练一个判断器，成本太高。。。

这篇论文大概就是这样，对我自己非常有帮助，我看的论文不多，这篇是我自己看的，我认为对“CoT推理在执行任务中的潜在副作用”这个话题，比较完整的研究之一。

同时，我也想聊聊，这篇论文对我的启示。

我们总觉得，聪明，就意味着知道得多、分析得细、每个变量都不放过.

但事实上，真正强大的智能，从来都不是把所有细节一股脑地扫过一遍，而是，
知道在哪一秒钟，把注意力放在哪个点上
。

比如我们小时候考试，很多人因为太想得高分，最后反而在最简单的题上丢分。

成年人做选择，明明已经知道该怎么做了，却非得做个SWOT分析表、拉个10页PPT讨论，最后被复杂困死。

公司做决策，明明方向明确，却因为分析得太多、风险评估太细，最后团队谁也不敢拍板，错过风口。

AI其实跟人很像。

上面很多CoT的验证，还有Constraint Attention，其实也证明了
，
大模型不是笨，而是思维资源错配了
。

你让它完成任务，它却跑去想着“怎么把这段话说得更优雅”、“这句话需不需要加个逻辑转折”、“前后是不是够自然”。

你让它干活，它在脑子里脑补了几万种情节。

但是，真正牛逼的智能，其实应该是聚焦。

比如你叫一个人帮你看一下一份报告有没有错，一个低阶执行者可能就只会一句句校对标点。

而一个高阶智能，可能会反过来先问你，
“你重点是要我看错字，还是看数据逻辑？”

你说清楚重点，他就能把80%的注意力锁死在正确位置。

而如果他啥都想看一点，最后很可能错得最离谱。

我们真正需要的，可
能，是
对“该想什么”有判断能力的智能
。

就像我们人类那些最令人敬畏的时刻，不是我们知道多少，而是我们能瞬间把注意力聚焦在关键节点上。

危机时刻，考场钟响，夜深人静一个念头浮上心头的时候，你知道的，你不能全看，你只能看准。

那个“看准”，在我看来，可能就是智能真正的体现。

这一点，看似简单，却足够让AI从“聪明”，变成“智能”。

这就是我读完论文之后，真正想跟大家分享的东西。

我们不缺思考的能力，我们缺的，是思考的分寸感。

注意力，不是撒网。

而是出击。


wzglyay@virxact.com

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
