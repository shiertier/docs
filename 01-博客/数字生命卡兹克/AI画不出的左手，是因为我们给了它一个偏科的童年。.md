---
title: "AI画不出的左手，是因为我们给了它一个偏科的童年。"
发布日期: 2025-12-10
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647677718&idx=1&sn=504dc084b39ae7678bdb6c096a42fb7b&chksm=f1b4125afbd6cafc1a76e801bf9965e8bdbeffd257e1435549edd658a687ac266a59aafd9f23"
---

## 摘要

**1) 一句话总结**
主流AI文生图模型普遍无法准确生成“左手写字”等特定左右空间关系的图像，其根本原因在于训练数据集存在严重的分布偏差（缺乏完整性与平衡性），导致模型死记硬背了人类社会的固有偏见，而非真正理解了空间逻辑。

**2) 核心要点**
*   **多模型测试失败**：包括NanoBananaPro、Gemini、ChatGPT、Grok等在内的多个头部大模型，在面对“左手写字”或“左右手分别拿特定物品”的提示词时均稳定翻车，强制生成右手动作。
*   **理论依据**：论文《Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation》（现象空间的偏差阻碍文生图模型的泛化）解释了该现象的成因。
*   **核心指标定义**：决定模型能否举一反三理解空间关系的关键不在于数据量，而在于数据分布的两个指标：**完整性（Completeness）**（每种物体在每个位置都至少出现过）和**平衡性（Balance）**（不同组合出现的比例相对均匀）。
*   **模型认知机制**：当训练数据缺乏平衡性（如某物体永远在上方）时，模型会将其视为绝对真理；只有当不同排列组合均匀出现时，模型才能真正掌握可互换的空间位置关系。
*   **现实数据映射**：由于现实世界中绝大多数人是右撇子，图像网站中关于“写字”的标签几乎全部对应右手。模型在海量偏见数据中学习，将“写字”直接等价于“右手拿笔”。
*   **偏见的本质**：AI的认知盲区本质上是人类自身偏见和数据采集缺陷的镜像，反映了大规模图像语料对少数者（如左撇子）现象的忽视。

**3) 风险/不足**
*   **模型准确率暴跌风险**：实验数据表明，一旦训练集在视觉空间关系上的完整度和平衡度下降，模型测试的准确率会随之大幅下跌，部分组合的正确率甚至永远无法突破50%。
*   **空间顺序错乱**：在处理缺乏平衡性的空间关系时，模型极易出现“物体生成正确，但左右/上下位置完全颠倒”的典型错误。
*   **样本采集盲区（统计偏差）**：由于历史摄影习惯和文化习惯，现有大规模图像语料库存在巨大的统计偏差，导致少数群体（如左撇子、非一线城市用户、老年用户等）的真实场景和需求在数据集中被遗漏。

## 正文

昨天刷到了一条非常有意思的推特。

是我关注的一个博主，Howie.Serious发的。

他发了一个很有趣的点，就是即使是世界上现在最牛逼的NanoBananaPro，在世界知识如此屌爆的情况下，AI，还是没有办法生成左手写字的图片。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVlPXibgyoItFbEa41Micokk9g7pRWeZ3HLic53Xs2vzq9ib1TN70e88hchQ/640?wx_fmt=png&from=appmsg)

这事特别有意思。

我立马用Gemini上的NanoBananPro试了下。

果然翻车了，而且是非常稳定的翻车。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVV9GBr0tUS6pAsJTmKWpblkFt9vdss4gibECCskEkMYqLrCmd9Bic27acw/640?wx_fmt=png&from=appmsg)

我又直接用Lovart跑了十几种张图，只对了2次，其他的，全错。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVLhe5rSdghK6S8XGnf0NKPrOdvJxuicZulr2F8ggNEib5zKc4TdrXngtA/640?wx_fmt=png&from=appmsg)

我又去试了其他的大模型，包括chatgpt、seedream，grok，也在这个小小的提示词上全军覆没。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVV8yoU3P5nX2n35AMTbT7ibkJuvLaCVmIcibJQcd6wYRATCKNdRwyE7x5w/640?wx_fmt=jpeg&from=appmsg)

刷刷刷给我生成了一堆右手，让我都有点混乱了，我那一瞬间都在怀疑是不是我自己分不清左右了。。。

我又尝试了一些进阶版。

比如，右手拿着苹果左手写字。

这个已经非常明确了吧，我已经给他做限制了。

还是会生成右手写字左手拿苹果的图。。。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVagH4W2Mnw58Lk5iaF0NB8T4prUHicfnNoWPqHWtnRnrqW8W2GKTgkmTw/640?wx_fmt=png&from=appmsg)

GPT直接给我玩鬼畜了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVeVeicOEeg6uZXfYkNdz4sjAsZwN6oyDyYicVboFVT2w7xn3mRA0jTYDQ/640?wx_fmt=png&from=appmsg)

甭管是谁，就算是蜘蛛侠来了也没用，也得用右手。。。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVW4MibCNhfQSpKIrvCa7xVS67pfZXBBnUiaOibQhnzoNOr0HbXM76hqAbA/640?wx_fmt=png&from=appmsg)

非常的倔强。。。

在好奇之下，我又试了一些其他的case。

比如，让一个人左手拿着橘子右手拿着苹果。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVmNaxkTSVQXe2INaUibJsxD5LdfkIDArzb2kSyH9tW7FbCCf8hSxG4iaw/640?wx_fmt=png&from=appmsg)

翻车。

穿个不同颜色的写字，翻车。。。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVV0Bxv4DeHy6icTKV5UkfAYTfmmiauxR8j5VtNZ49SxVb40La8j0ImibeUg/640?wx_fmt=png&from=appmsg)

左手举起魔法棒，翻车。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVOzGAqmwsJZYQhbSVt9B7s5rLYicic3FTnThF9iciaEzdiaVWuFScnS9Bjdg/640?wx_fmt=png&from=appmsg)

左手拎着一只鸡，右手拎着大高达，翻车。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVt3K7uYicGU06Lf0cXLm4vudpF6YGqlAbMGHLAwoicSVQ2ZuUBVybVMjA/640?wx_fmt=png&from=appmsg)

全都翻车，翻了个大车。

至此，确实发现，AI完全分不清左右手和左右脚。

但是，如果你让他去纯粹的画空间关系，确实是没啥问题。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVJbVia92ocuIbYGH1YzZbr5LQjGaXicbNKRsjNtsFWV53eWhiawPKujtKA/640?wx_fmt=png&from=appmsg)

但只要一涉及到左手左脚，就直接原地爆炸。

这个话题太有意思了。

我非常好奇的想知道，到底是为什么？

在DeepReasearch之后，还真找到了一个蛮有趣的可以解释这个事的论文，叫《Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation》，中文名翻译过来是，现象空间的偏差，会阻碍文生图模型的泛化。

而这个影响的核心，其实就是偏见。

跟我之前写过的一篇
AI们数不清六根手指，这事没那么简单
很像。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURricGs1p5gPUZ95TFaXGKv7I6Ft7fvCntZUyNQIR97n4gkmUGz5uBZtqgBNwN9VGiaKrd2AV9OoXDVQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

那篇文章，聊的是视觉模型，在理解的时候，分不清图中的人有几根手指。

而这次，是在生成的时候，分不清左右。

其实本质的逻辑都是相同的，就是因为数据集的偏见。

这篇论文，大意就是一句话：

AI之所以分不清左右，不是因为它逻辑不行，而是因为它的老师，也就是我们投喂给它的海量图片数据，本身就存在巨大的压倒性的偏见。

他们做了一个实验。

干的第一件事，是把一张图给拆解成filler和role两种要素，你可以把它们理解为，主体和关系。

拿猫追老鼠的图来举例，主体就是猫和老鼠，而关系就是，谁是追的那一方，谁是被追的那一方。

确定好这两种要素之后，他们找来了几十个小图标。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVV8QYiasAzx6gfBJiboshnDgoEuZiaRRYjgFlxbziaH0NSFW7c0H9jzp9dzw/640?wx_fmt=png&from=appmsg)

两两图标分为一组，规定好它们的主体和关系，用Unicode字符画在 32×32 的小方块里。

所有的画面，只干一件事：

两个东西，上下叠放。

比如，把名为蛋糕和橡皮的小图标放在同一张图上，上面是蛋糕的图标，下面是橡皮的图标。

然后附上一句话，这张图是一个蛋糕在一个橡皮上面。

以此类推，就有了一堆测试图片加一堆文本。

然后，他们又分了一些数据集。

有些训练集里，每个物体都当过上面的、也当过下面的。

有些训练集里，猫可能几乎永远在上面，狗也可能几乎永远在下面。

还有些训练集，更狠一点，某些物体从来没当过上面，只当过下面。

接着，他们把这些图片和文字打包成数据集丢给模型去训练，看它学了这些东西之后，能不能理解上下位置关系。

按照我们对模型训练的常规理解呢，这件事儿的关键在于，样本量要够大。

只要数据规模够大，智能就会自然长出来，对吧？

但是他们的实验数据发现，其实，完全不是这样的。

决定模型能不能举一反三的，其实看的不是数据的数量，看的是，数据怎么分布。

他们有定义了两个指标，一个叫
Completeness（完整性），就是
每种东西，是否都至少在每个位置上出现过一次。

举个例子，圆在上、三角在下是一种组合，圆在下、三角在上是另一种组合，这两种情况都要在数据里出现过，完整度才能算是及格。

但只有完整度还不够，还得看另一个指标，叫Balance（平衡性），对，就是跳舞里面的那个Balance。

它其实指的就是，不同组合出现在数据中的具体比例。

只有圆在上和三角在上这两种情况，在数据里的分布情况大差不差时，平衡度才过关。

反之，如果九张图都是圆在上，只有一张图是三角在上，对模型来说，就是平衡度极差、世界观极度倾斜的情况了。

这样一来，模型就会天然的把圆在上这件事当成一个真理。

只有当一个训练集里，不同的排列组合都出现过，并且每种组合在上在下的情况都出现得差不多时，模型才会开窍：

原来谁在上谁在下不是恒定的，是可以互攻的。

哦说错了，是可以互换的。。。

那一刻，模型才是真正掌握了上下的位置关系，而不是死记硬背几种固定搭配。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVViaSGCUwFotFucMKPT06JAjY1ibhbKMqPuiaanpibLDgJrLNiazNtTWGtXZQ/640?wx_fmt=png&from=appmsg)

右边的表格里，两个CPL代表完整度，也就是圆和三角在上的情况是不是都出现了，BLC代表平衡度，也就是圆和三角在上的情况分布是否均匀。

左边的纵轴是模型测试的准确性。

你会发现，当完整度和平衡度都是百分百的时候，模型测试的正确率几乎也是百分百，也就是蓝色的散点。

而当完整度和平衡度越来越差的时候，模型正确率也会不断下跌，到了完整度和平衡度最低的灰色散点这里，准确率就没上过百分之四十。

论文后半段，他们还做了一个和现实世界更接近的实验，用的是一个叫what’sup的基准数据集，里面都是自然图片，专门用来描述两个物体的位置关系。

然后，在这个数据集里面抽取子集。

有的子集完整度和平衡度都很高，有的相反。

接着，他们让模型去生成数据集里没有的物品左右关系图片。

得到的结果非常稳定：

视觉这边的完整度和平衡度，一旦掉下去，测试集的准确率就一路跟着往下掉，有的组合甚至永远突破不了50%。

更经典的是最常见的一类错误：

两个物体都画对了，但顺序反了。

你让它画盘子在罐头左边，它给你的图看起来很协调，但仔细一看，变成了，可乐罐在盘子左边。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpp0jod4LNSfDo5Z1G9lyVVFqgPxz6unJxBgRzxhB4WhFXn3FxkznHru02VeIuOS4rzb6wDXCXHaw/640?wx_fmt=png&from=appmsg)

这个错误就非常符合我今天在左右手的case中测试出来的结果。。。

所以现在可以给AI分不清左右这件事，初步给一个解释：

AI不是分不清左边右边，而是在它受过的童年教育里，现象空间本来就是偏的。

就比如写字这个案例，因为现实世界里，写字的大部分人就是右撇子。

图像网站的标注里，“writing”“student writing notes”这些tag背后，也几乎清一色是右手写字。

当一个模型在海量图像上长大，它看到的写字几乎等价于右手写字。

所以当你说左手写字的时候，它脑子里的激活模式是这样的：

“写字？写字这事我懂啊，写字不就是等于一个人 + 一本本子 + 一只手拿笔吗，啥玩意？你要左手写字？你有毒吧，这个世界还有人用左手写字？我就没见过。”

然后，啪的一下，给你画了个右手写字。

其实有点像一个极端偏科的学生。

他做了十万道“2+3=5”这样的题，但是从来没见过“3+2=5”。

所以，当你你问他“3+2等于几”，他直接就宕机了。

左手写字，就是3+2那一侧的世界。

其他的失败的case其实也差不多。

当然，这也不怪AI，因为它的见识就是我们给的。

如果训练集里，左撇子的踪迹本来就少，模型学不出来，是不是应该怪模型？

还是我们，根本就没把这个世界里那些少数者的现象，认真地采集进去？

以前我做用户研究的时候，其实最怕的，就是招样本招得不均匀。

比如你明明是个普适性的APP，但是只找一线城市上班族深度访谈，当然得不出老人怎么用你的APP的这个结果。

只看IOS用户的数据，当然也看不到千元安卓机上的使用体验。

只做所谓的可用性测试，不实地去做田野调研，就根本不可能看到用户在真实场景中的那些小动作、小走神、小偷懒。

AI一直在模仿的，其实就是我们自己的偏见。

我们的大规模图像语料，是过去几十年的人类摄影习惯和文化习惯的快照。

如果这个世界90%的人都是右撇子，摄影师拍照的时候又喜欢把笔、杯子、道具放在某个视角更舒服的位置，那模型看到的世界，就会是一块巨大的统计偏差。

如果用一句很正确的话来说。

就是，我们根本没给模型一个公平的童年。

但反过来，我们如果看自己呢？

好像，我们本身，也会被各种各样的训练集规训。

成功的概念是有房有车财务自由，人生的捷径是考功上岸。

我们和AI的区别只不过在于，AI是用几百亿张图、几万亿 token，迅速堆叠起来一个模型的失误。

而人类是用几十年的生活和经验积累，逐渐走到一条自己不那么想走的岔路上。

人类和AI，现在好像，都无法看到自己认知以外的东西。

如果说技术的发展会逼着AI公司们，去重新设计那个属于AI的训练集，增加它的完整度和平衡度，让它泛化。

那我们，是不是也可以，增加一下自己体验的厚度？

当我们对模型说，你不能永远只会用右手写字。

你也得试试左手。

那在我们的生活里，有没有哪一些左手的可能性，其实一直都在，但我从来没有看见过？

我相信，肯定会有的。


wzglyay@virxact.com

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
