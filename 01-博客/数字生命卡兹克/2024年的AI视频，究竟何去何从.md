---
title: "2024年的AI视频，究竟何去何从"
发布日期: 2024-02-04
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647661342&idx=1&sn=7b130099238cad803db59dda407f1593&chksm=f19be03b35820f8e0b82cbb2fe059d15503a20d46f04a420437b48567978ca107a5ab2189d88"
---

## 摘要

### 1) 一句话总结
2023年AI视频迎来了爆发式增长，但要实现对影视行业的真正颠覆，未来的核心在于通过构建世界模型或“AI 3D+视频”技术，解决AI缺乏物理规律理解的根本痛点。

### 2) 核心要点
* **行业爆发与现状**：2023年是AI视频突破之年（以Runway Gen2发布为标志），目前已有数十家初创公司（如Pika、Pixverse等）推出产品。
* **大厂集中入局**：预计2024年上半年，Meta、Google、腾讯、字节、阿里、商汤等头部科技企业的AI视频产品将迎来涌现式喷发。
* **产品功能局限**：当前绝大多数产品仅能生成3-4秒的视频，且缺乏可控性，仅有少数产品（如Runway的运动笔刷、Pika的区域修改）支持局部控制。
* **行业进度评估**：当前AI视频距离全面为影视行业降本增效的终极目标，进度仅在5%到10%左右。
* **核心突破方向**：AI视频走向成熟必须解决“物理规律”问题，即实现符合现实逻辑的人与人、人与物、物与物的交互。
* **技术路线一（世界模型）**：以Runway为代表，试图让AI学习并理解现实世界的物理常识与规律。
* **技术路线二（AI 3D + AI视频）**：以商汤、Midjourney为代表，通过AI生成3D模型，并借助成熟的游戏3D物理引擎来解决轨迹与动作的交互问题。

### 3) 风险与不足
* **AI视频通用缺陷**：目前普遍存在画面一致性差、生成时长短、运动幅度受限以及可控性不足的问题。
* **世界模型的数据收集瓶颈**：机器缺乏人类的抽象化能力，难以从海量且充满噪点的视频数据中剔除冗余信息并抽离出关键的物理规律数据。
* **AI 3D路线的技术难点**：虽然避开了世界模型的难题，但仍面临3D建模精度、贴图、骨骼绑定以及渲染等方面的技术坑点。

## 正文

前几天在A16Z上看到了一篇文章：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqaT8uZK76al3CmcEMSjxKpKPmbKfMQYx8JzAk5taCgro5VoUzCbyXWg/640?wx_fmt=png&from=appmsg)

看完了有一点感触，正好我自己也在各种不同的场合表达过过AI视频现状的看法和展望。

那我想，不如就写一篇文章，结合A16Z的观点，来一起聊聊这个话题。

2023为什么是AI视频突破的一年，AI视频还缺少什么，以及对2024年AI视频的展望。

2023是AI视频爆发的一年，从最开始时，只有Runway的Gen1和wonder studio勉强可看，但是基本不可用。

直到8月，RunwayGen2正式推出。

Nicolas Neubert做了《创世纪》AI预告片，我做了《流浪地球3》的AI预告片，AI生成式视频，正式走向大众眼前。

直到2024年初，已经有数十家AI视频公司成立，且推出产品，而更有不计其数的大厂的产品正在路上：比如Meta、Google、腾讯、字节、阿里、商汤等等等等。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqK24KVS5X2Y23pwNduibWZ0ugtibrGO5ElQibYbaBpzEoVfMCeFFMgCtbQ/640?wx_fmt=png&from=appmsg)

A16Z总结了一张图2023年关键节点图，我觉得总结的非常完整。

今年，我自己也整体体验了非常多的产品，除了最常用的Runway、PIKA、Pixverse之外，还有SVD、Genmo、
Moonvalley、domo、Morph等等等等。太多了，也太卷了。

同样的，这里直接放A16Z总结的图。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqHTC9Eib9ycqKW1QDtMyYIMJYqSTKXoPOyw2awCUMPb1SGfEvkvaVc4A/640?wx_fmt=png&from=appmsg)

但是，绝大多数的产品都并不完善，在可控性上，只有为数不多的几家产品支持了，比如Runway的运动笔刷、比如PIKA的区域修改，大部分的产品都只有文生图和图生图，生成3～4秒的视频，然后就没了。

也可以从图中看出，大部分的公司，都是小公司，大厂的动作有点慢，不过2024年上半年，大厂的AI生成式视频产品，可能会是涌现式喷发。

但是不要忘了，AI视频的第一次正式全员亮相，是在去年八月。

距离今天，也仅仅过了6个月而已。

说说AI视频的问题。

很多人在交流的时候，经常问我一个问题是，你觉得AI生成式视频，距离终点的进度走到了多少？或者说，你觉得能全面给行业降负还有多久。

虽然我觉得这个问题太大太空，但是我每次还是会说，现在的进度大概是5%到10%的地步吧。

离你们心中那个饼，还有90%的路。

在我心中，有一个最核心的问题需要解决：

物理规律
。

这个问题不解决，我不认为会对现在的影视行业会造成很大的冲击。

众所周知，视频里面，是包含大量的交互镜头的，人与人、人与物体、物体与物体的交互，等等。

比如我的玻璃杯子我放在空中，让他垂直落下，他应该掉在地上然后碎掉，或者高度不够的话，杯子会在地上弹起。

但是你让AI去做的话，你就会发现，变形变得你妈妈都不认识了。

![](https://mmbiz.qpic.cn/mmbiz_gif/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqeO8saEFaTCmYL2Kom6r5LyM9mtvtUwDMEvK5103gX9x4t0K0kK3lSQ/640?wx_fmt=gif&from=appmsg)

或者一个人物戴着头盔转下头，头盔没了，脸也不要了。

![](https://mmbiz.qpic.cn/mmbiz_gif/OjgKEXmLURqAYcH9a9JDp81nCViaW2voq5SlicoWwEIsgpg5QbTFwsYUQiaMfjyTVgrFjUqiaia7icYlTjwyMWbysRTA/640?wx_fmt=gif&from=appmsg)

这就是一个很离谱的事情，AI并不懂这个世界的物理规律，而一个不懂物理规律的视频，你认为他的上限能高到哪里去呢？

影视里，我们的最喜欢看的，是关系，是交互。

《海边的曼彻斯特》里有很多苦衷的两人之间的亲吻相拥。

《环太平洋》里机甲一拳将怪兽砸到海中的快感。

《复仇者联盟4》里钢铁侠那一个响指的浪漫。

没有这些，那一切都是空镜，一切都是转场，这样的镜头，怎么好看呢？

所以我们需要让AI视频，拥有物理引擎。

现在就我知道的，有两条路线，一条以Runway为代表，做世界模型；一条以商汤为代表，做3D。

先说世界模型。

Runway很久以前发了一个帖子，简单的介绍了一下世界模型。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqVeY7YpExFicCAVOicb0wATUIdL5CPVsmR9VfJib06ruHaeQ0UPEFntskQ/640?wx_fmt=png&from=appmsg)

但是不要以为他就开始干了，他的原话是：

We are building a team to tackle those challenges. If you’re interested in joining this research effort, we’d love to hear from you.

翻译过来是：
我们正在组建一个团队来应对这些挑战。如果您有兴趣加入这项研究工作，我们很乐意听取您的意见。

。。。

就，怎么说呢...

斯坦福有一个团队在我的映像中也在做，但是世界模型遇到的最大的问题是：数据收集的问题。

世界模型最麻烦的就是对物理现实的数据收集，人类的抽象能力很强，看到一个球，可能会识别出这是足球或者头；看到一个白色的物体，可能是桌子或是一堵墙。

但是现在大部分的视频数据，有大量的垃圾噪点，怎么能抽象的把大量的噪音抽离，只留关键的物理信息？这是一个非常坑的点。

自动驾驶其实就是这个方向的最落地的应用之一，但是自动驾驶要的数据，并不是那么多，来来回回就是道路、街道、人物、车子那些东西了，但是也搞了N久，现在离L4还很远。

当你要做生成式AI视频的通用世界模型的时候，那数据要的就不是那一丁点了。。那是海量的数据。

一个炸弹炸在屎上屎应该如何散开？我一巴掌打到你脸上你肌肉应该如何颤抖？鲨鱼在水中追你的时候水是如何散开的？？

这都是物理规律，我们能很容易的想象到，但是对于AI来说，太难。难的不是后期的训练，而是前期的数据收集。

在我跟大佬们的交流中，他们说现在最痛苦的点是：

"人经历过千万年的进化，对于世界的常识，是藏在基因里的，会自动把一些冗余的信息以极快的速度，一步一步剔除直到只剩到最关键的信息，然后做做一些思考推断。
但是机
器没有，机器现在不懂抽象，
所以需要人去做
类似人类抽象化过程的学习算法，来抽离关键信息，收集视频数据。"

这个点非常痛苦。

这也是我对于所谓的AI视频的通用世界模型没有那么多信心的原因。

另一条路，就是AI 3D + AI视频。

商汤的一部分技术就是走的这条路。

他们曾经发了一篇论文：https://story2motion.github.io/

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURqAYcH9a9JDp81nCViaW2voqD0fkeTEyI1c0R7zyCVsa6TibzvFQ2oNcJTY7ArwNxznkNyN1KibjqIHQ/640?wx_fmt=png&from=appmsg)

这个项目很有意思，从故事可以直接生成动作，非常连贯。

如果给一张图，能直接分离场景和主体，然后3D建模后，再用Story to motion生成轨迹和动作呢？

不需要什么世界模型，3D世界中的物理引擎，游戏行业都做了N多年了，非常成熟了。

只要能分离建模，那就没问题。

而AI 3D，我之前也写过一篇评测文章，现在正在冉冉升起的阶段：
AI时代的生成式3D大模型全面评测 - “ChatGPT时刻”的前夜

还没有那么成熟，但是按照现在这卷的速度，AI 3D，可能要不了半年，就能达到MJ的V4的程度。

当AI 3D和AI视频结合，那就是王炸。

所以我也能理解为什么MJ给自己定的星辰大海是AI视频，但在做AI视频之前，先从英伟达挖了一个专门搞3D的大佬去做AI 3D。

当然，这里面也有很多坑，3D建模的精度问题、贴图问题、骨骼问题、渲染问题等等。不过相比世界模型，我觉得这玩意的难度，还是小不少的。

总结一下，AI视频当然还有很多问题，比如一致性、比如时长、比如运动幅度、比如可控性等等。

但是我认为没有一个，像物理规律这样重要。

我期待后面大厂们加大筹码，疯狂开卷。

早一点解决这个问题。

早一点迎来，那超级的“颠覆时刻”。

我真的很想看到那一天。

。

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
