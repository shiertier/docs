---
title: "揭秘AI背后的神秘代码 - Token究竟是什么？"
发布日期: 2023-06-16
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647658746&idx=1&sn=c9ae95c3c0c1b580c4bf8055768766e9&chksm=f10cfefb037c07b1888c3bcc110adc25036801fd542df688b66a2a36e8f91518e89af7ca8d2f"
---

## 摘要

**1) 一句话总结**
本文介绍了OpenAI发布的GPT-3.5 16K版本如何解决复杂API调度中的上下文限制问题，并科普了Token的基本概念及长文本处理所面临的平方级计算量挑战。

**2) 关键要点**
* **版本更新**：OpenAI近期更新了函数功能（对Langchain及API调度中台产生巨大冲击）并推出了GPT-3.5的16K Token版本。
* **旧版痛点**：旧版GPT-3.5受限于4096 Token，在处理大量（约100个）API调度时，单次只能输入约30个API，必须拆分为4个并发对话再进行数据提取和合并，流程繁琐。
* **16K版本优势**：16K版本能够在一个对话窗口内解决所有API的调度问题，大幅简化了开发流程。
* **性能与价格**：16K版本的推理能力比3.5强几个量级，而价格仅比GPT-3.5贵一倍。
* **Token定义**：Token是自然语言处理中机器理解数据的基本单元，可以是一个字、一个词组或标点符号。
* **算力挑战原理**：在大模型Transformer算法的注意力机制（Attention）中，每个Token都需要与其他所有Token进行一次计算，计算量随Token数量呈平方级增长。
* **具体计算量**：4K Token大约需要进行1600万次（4000×4000）注意力计算，而16K Token的计算量会暴增（原文称约需256亿次计算），这是提升Token上限的核心难点。

**3) 风险/缺口**
* **并发合并可靠性低**：在受限于低Token上限（如4K）时，采用多个对话并发处理并最终合并数据的方案，大模型的可靠性较弱，容易引发额外的数据处理麻烦。

## 正文

前两天，OpenAI来了个大活，更新了函数和GPT3.5的16K版本。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURowVceohpIGOrkx7dAYOsia9cKyNcVZpIzRDhuhhNUVlEBsdf0zF7YlJZIBGcAnBd6OuZNqhDl2l8g/640?wx_fmt=png)

函数不具体说了了，对Langchain和想做API调度中台的是一锤子暴击。

重点说说GPT3.5的16K Token版本。

正好作为一个伪科普作者，也想借这个机会，给大家用大白话简单科普一下到底什么是Token，为什么想提高Token的上限这么难。

先说一下16k的价值。

众所周知，旧有的3.5是4096Token，我们在做开发时，无数的地方受制于这个4096Token。

比如说，用户对着我们韭圈儿AI发了一句话：

“帮我挑选5只去年收益排名在前20%，且最新季度持仓不含白酒的消费类基金，并全部加到我的自选，然后做一个等权组合，生成一个组合诊断分析报告。”

我们一共有100个左右的API，比如查询收益、查询持仓行业占比、筛选基金、构建组合、组合诊断、加自选是分别不同的API，我们会让GPT自主挑选使用什么API来处理这个用户的问题
。

而受制于4096，我们一次对话大概只能灌30个左右的API给GPT，另外70个API会丢失，这肯定不能这么任性瞎搞。

所以我们会同时开4个对话进行并发，最后把挑选的API合并在一起，但是大模型吗，你懂的，可靠性并不总是那么强，特别是4个对话并发最后合并，那特么就更麻烦了。。。所以我们又需要额外的做数据提取再合并等等。。。

而现在，有16K，还并发还合并还提取个屁。

一个对话窗口全部解决。省了无数的心。

同时，推理能力也比3.5强几个量级，直观感受可能叫GPT3.75更为合适。价格也仅仅只比GPT3.5贵一倍，相当香了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURowVceohpIGOrkx7dAYOsia9ibSXpBiaKV9P0oAUDXKpXlsQNI4ExYMaax5tKo81QXjZe7LXzuR9ZOXw/640?wx_fmt=png)

回到4k和16K上，为什么16K比4K难做的多，导致以OpenAI的技术，也现在才放出来16K的3.5版本？而Token到底又是个啥呢？

我先浅显易懂的解释一下到底什么是Token。

在自然语言处理和计算机科学中，“Token”是一个非常基本的概念。

简单来说，一个Token可以看作是数据的一个单元。

在自然语言处理中，Token最常见的定义是一个语言的基本组成单元，如单词、字、甚至是标点符号。

当我们要让机器理解和生成自然语言时，首先需要进行的就是“分词”（Tokenization），即将连续的文本划分成一个个Token。

我们可以用一个有趣的测试来证明字和Token的区别。

比如，我要将
“
卡兹克今晚要去按摩然后吃炸鸡
”
这句话倒写。

正常来讲人类语言应该是：

“鸡炸吃后然摩按去要晚今克兹卡”

但是对于计算机，用Token来处理就是：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURowVceohpIGOrkx7dAYOsia9K4hfSibSA8r25zWmyoSmKWllS2L8bFAicibnzTmBic4mkqNHrfc1YVZIxg/640?wx_fmt=png)

完全就可以发现，一个字可能是token，一个词组也可能是token。

Token，就是机器所理解的数据单元。

明白了Token，我们再来回答另一个问题，
为什么16K的Token版本比4K难做的多。

我用一个有趣的例子来解释。

派对。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURowVceohpIGOrkx7dAYOsia9j3MxNMwiaIonSo36ic1JvVK8me9dYsoy93GTGcsZBPo3icsicSXEPeMibjQ/640?wx_fmt=jpeg)

假设你正在举办一个盛大的派对，你邀请了4000位宾客（这就是我们的4K，同时为了方便计算，后面4k=4000,16k=16000）。

你要尽量确保每个人都有人陪伴，所以你要安排每个宾客和其他所有的宾客都至少打个招呼。你能感觉到这是个挺大的工程，对吧？

现在，试想一下，如果你的派对规模翻了4倍，你邀请了16000位宾客（也就是我们的16K）。你仍然希望每个人都被照顾，所以你又要让每个宾客和其他所有的宾客打招呼。

你能想象到这是多恐怖的工程吗？

这就是处理16K的Token比处理4K的Token难得多的原因。

在大模型的核心算法Transformer中，这种“打招呼”叫做注意力计算（attention calculation）。

在注意力机制中，每个Token都需要与其他所有Token进行一次计算。这就意味着，当Token的数量增加，需要进行的计算就会呈平方级增长。

对于4K的Token，我们需要进行大约
16百万次（4000乘以4000）
的注意力计算。

而对于16K的Token，我们需要进行大约
256亿次（16000乘以16000）
的注意力计算。

这就是为什么16k比4k难做的多的原因，同理，32K，那就更恐怖了。

希望我的这篇文章，能让大家明白一些大模型背后简单的原理。

最后。

让我们向每一个参与这场科技盛宴的开发者，研究者，创新者致敬。

他们改变了寻常事物。他们推动着人类前进。

我相信，他们，才能够
真正的改变世界。

以上，

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
