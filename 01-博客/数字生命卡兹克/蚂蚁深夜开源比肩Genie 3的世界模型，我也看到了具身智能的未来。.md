---
title: "蚂蚁深夜开源比肩Genie 3的世界模型，我也看到了具身智能的未来。"
发布日期: 2026-01-29
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647679339&idx=1&sn=9f40ca981e803254db26220dcf701a90&chksm=f162cb7dd8071ef44c52af5189d2c58dba92e4f0e54b86fd63bfc8b2dbb7d2bcc367271e2c2e"
---

## 摘要

**1) 一句话总结**
蚂蚁集团旗下灵波科技开源了支持实时交互与动态生成的世界模型LingBot-World，该模型具备稳定的长时记忆、极强的风格泛化能力及优秀的动作代理机制，为具身智能训练提供了高保真的模拟环境。

**2) 核心要点**
*   **产品定位与交互机制**：LingBot-World是对标Google Genie 3的世界模型，支持通过按键（如WASD）实时演算和动态生成世界，而非播放预渲染视频。
*   **模型规模**：模型总参数量约为28B，推理参数量约为14B。
*   **版本规划与开源状态**：
    *   **Base (Cam)**：支持相机位姿与运动轨迹控制，提供480P和720P推理配置，**目前已在GitHub开源权重与下载链接**。
    *   **Base (Act)**：支持结构化动作指令（如抬手、转身）控制，待开源。
    *   **Fast**：主打低延迟与实时交互，延迟低于1秒，可达16帧/秒，待开源。
*   **长时记忆能力**：模型具备极强的长时记忆，在长达10分钟的探索测试中，能维持建筑结构、遮挡关系及空间的一致性，视角移开后回头仍能保持原貌。
*   **训练数据与风格泛化**：混合了真实视频、游戏录像和UE合成场景进行训练，能够兼容超写实与非写实（如游戏画风）等多种视觉风格。
*   **动作代理机制**：不仅支持玩家按键的连续意图推演，还内置了基于微调视觉语言模型（VLM）的动作代理，允许AI角色在生成的空间内自主规划运动（如避障、急停、变道）。
*   **核心应用场景**：主要面向具身智能领域，为机器人的现实世界理解和长程任务训练提供低成本、高保真的试错空间。

**3) 风险与不足**
*   在超长时（如10分钟）的生成过程中，画面偶尔会出现一些变形。
*   Fast版本为实现流式生成与低延迟，其画面质量上限略低于Base版本。
*   在生成部分非写实风格物体（如案例中的大剑）时，画面仍存在轻微的不稳定现象。

## 正文

AI圈最近是卷疯了吗，模型跟不要钱一样kuku的往外发。

今天凌晨的时候。

蚂蚁在毫无预兆的情况下，他们旗下的具身智能公司，灵波科技，开源了一个非常非常离谱的世界模型。

LingBot-World。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURrcZACZZJgKcN9Nw2S7GtOKASJyRBdAgbbib4faqKe6Yxa5ib3AhibVgtQDIEx0kjx0nFxIhDzNziaHrA/640?wx_fmt=png&from=appmsg)

我其实本身是真的没有当回事的，就是因为我对世界模型还比较关注，就随手点进去看了眼。

结果，我真的有点停不下来了，我在这个页面里，花了半个小时的时候，几乎看完了所有的案例。

我是真的觉得有点离谱，几乎可以对标Google Genie 3的质量，而且，开源。

我直接放个case。

一个1分钟的，第一人称探索的视角。

我不知道你们是什么感觉，如果玩游戏很多的朋友，可能会说，这有啥稀奇的，不就是一个普通的游戏里面的那种废弃小镇场景吗，不就是第一人称在里面探索吗。

对，但是如果你知道，这一切的源头，这个世界里面所有的一切，都是根据你的方向键，用视频动态生成的。

我相信你一定会有不一样的感觉。

这是一个完完全全的，一边探索一边生成的世界。

这个视频里面的一切，都是实时交互的，实时按键实时运动的。

言出法随，指哪打哪。

我凌晨1点多，第一次看到这个demo，同时意识到，这是一个世界模型的时候，我其实是起了一些鸡皮疙瘩的。

还有这个，实时生成的巨物压迫感，真的非常的真实了。

最离谱的是这个。

一个10分钟的视频，他们让模型一个人就这么沿着古建筑群瞎逛，逛了整
整十分钟，中间确实偶尔有一些变形，但是，到最后了，这个古建筑居然没有崩掉，太离谱了。

之前测过一个叫
Odyssey的世界模型产品。

别说10分钟了，1分钟就直接崩成这样了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURrcZACZZJgKcN9Nw2S7GtOKQqWZADqvUL0to5mHjwkItj5mLOxw3RPXCEqCQg2Hach2DVrx6UHBOw/640?wx_fmt=png&from=appmsg)

而且毫无记忆能力，我只要一回头，这个世界就变了样，而且是每回头一次，它就变一次。

相当的吓人。。。

如果说要跟Sora、可灵这种视频生成模型最大的区别是什么，我那觉得，一个是预先录制的电影，另一个是可实时演算的模拟器。

视频生成模型，是他已经把整部电影拍完、剪好，加好特效，然后放给你看。

画面很精美，故事很完整，但你是纯粹的观众，只能被动接受，无法改变任何事。

而LingBot-World，你按W，它就往前生成世界，你按A，它就往左拓开空间。

你输一句“下雨了”，天空就真的变阴云密布，你说“来点烟花”，远处城堡上空立刻炸开一朵。

所有的一切，都是边走边算出来的，而不是提前渲染好放给你看。

前者是叙事的终点，后者是世界的起点。

太离谱了，要知道，这个模型，是跟之前Google Genie 3的路线一致，是可实时生成的世界模型。

老粉可能还记得，我去年写过
Genie 3
。

这篇文章到现在也是我觉得是我的一个很大的遗憾，它明明那么强，可是我没有把它写火让更多的人看到，这是我的问题。

我一直都非常关注这种可交互的实时生成的世界模型，但是坦率的讲，Genie 3之后，几乎再无同类，而且已经几个月了，Genie 3到现在也不能体验上。

但今天，不仅有了，而且，还开源，甚至他们，把论文都发出来了。

真的有点不敢相信这是我之前认知里那个蚂蚁。。。

项目网址在此：
https://technology.robbyant.com/lingbot-world

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURrcZACZZJgKcN9Nw2S7GtOKJNDicQCdLKUU4cc6f6b8xmLlGulfoYkRFqEd40UxRPpJv4hEa8q4qkg/640?wx_fmt=png&from=appmsg)

目前已经在github上开源了第一个版本，另外两个版本等待放出。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURrcZACZZJgKcN9Nw2S7GtOKksuk7J0KUN8p2Dz0R7EiaahkH5utO9LQ6vMTtNoIYYiceIqV7O2EicTFQ/640?wx_fmt=png&from=appmsg)

这三个版本我大概解释一下。

LingBot-World-Base (Cam) 代表Base系列里带Camera Poses控制的版本。

你在推理时会额外喂相机位姿或相机运动轨迹这类信号，所以它更擅长把镜头运动做得可控，适合你想明确指定推进镜头、环绕、俯仰、平移这类拍法的场景。

表里写的480P和720P也对应它当前提供的推理配置，这个版本目前已经放出权重和下载链接。

LingBot-World-Base (Act) 代表Base系列里带Actions控制的版本。

这里的 Actions 更像“动作指令”或“行为控制”，让你能用更结构化的方式去约束主体怎么动，往哪走，抬手，转身之类。

它的目标是把可控性从镜头扩展到行为层面，目前等待开源中。

LingBot-World-Fast代表Fast系列，核心取向是低延迟与实时交互，一般会通过结构改造与加速手段，把推理做得更适合流式生成和边交互边出画面。

代价通常是质量上限会比 Base 略低一点点，优势是响应更快更像实时世界模拟，延迟能低于1秒，能做到每秒16帧，
目前等待开源中。

模型参数量在28B左右，推理应该在14B。

在看完了所有的case，以及论文以后。

我给它总结了3个特点。

分别是长时记忆很稳定、风格泛化性极强、很棒的动作代理。

一. 长时记忆很稳定

说实话，我们看世界模型，最核心的一个东西，看的一定是长时记忆。

就跟我们用文本大模型一样，他能不能记住前面的那么多的信息，这个事非常的重要。

而在世界模型里，这个事，尤为重要，甚至就是第一位的。

如果没有长时记忆的模型，你可以想一想这个场景，你去厕所拉屎，打开了厕所门，进门，掀开马桶盖，一回头，厕所门没了，变成了一个不知道通往哪的过道，你再一回头，马桶也没了，变成了一个小女孩就这么瞪着你。

现在是凌晨3点20多，我写下这段话的时候，我还忍不住回头看了好几次，我说实话，我真的突然有点慌。。。

这就是没有长时记忆的问题。

可能在文字输出的时候，他不记得之前的事了，可能影响还没那么大。

但是在一个可以互动的世界里，如果没有了长时记忆，那就成了彻头彻尾的恐怖片了，我们俗称，鬼打墙。。。

而
LingBot-World解决了这个问题。

比如说这个case。

可以看到，在这个过程中，在这个廊桥上，不管你怎么走，前看后看左看右看，那些建筑也绝对一直都在，甚至高墙和后面的建筑的遮挡关系，都会随着你行走的距离而改变，当你扭头看向别处的时候，回头看，还在。

这个长时记忆，就太牛逼了。

还有这个。

从看到这个开始，然后从肚皮下面穿越过去，在差不多的时候，你可以看到，你的视角是真的穿过了这个异兽的横向面积，让你感受到很真实。

而没有长时记忆的世界模型，可能在你穿的时候，直接就在它的肚皮下面穿了将近1分多钟，甚至就在那鬼打墙了一直穿不过去，明显时间尺度和距离尺度都不对。

而Lingbot-World在这点上，就表现的非常好。

二. 风格泛化性极强
很多的世界模型，其实在风格上都比较固定。
只能搞现实世界的，就是超写实的那种，但是一旦涉及到非写实的，一般效果就非常的差了。
但是lingbot-World居然保持的相当好。
比如这个例子。
明显能感觉到，虽然大剑还稍微有一点点不稳定，但是其他的地方，保持的非常好，已经媲美一些游戏的质感了。
还有这个。
在这种画风下，整个世界模型没有崩塌，这一点其实非常的难能可贵。
核心其实在于LingBot-World在训练的时候，真实视频、游戏录像、UE合成场景，全塞在一锅里里面训了，他们搞了大量游戏世界的数据，还有UE的合成数据。
现实世界的视频负责教它物理世界大概长啥样，游戏世界负责教它人类在虚拟世界里是怎么玩的，然后合成世界则负责补齐那些现实很难系统采集的视角，比如各种极端运动轨迹、复杂相机路径、极限视角。
对模型来说，这三种东西在输入上其实是统一的。
就是都是一帧一帧的视频，外加相机位置、动作指令、文本描述。
它其实并不会像人类那样心里有一条线，说哦这是真实的，哦这边是游戏，它看到的只是不同分布的像素序列。
这一点其实有点像机器人领域的域随机化。
就是很多具身公司，在做仿真训练的时候，经常会把地面材质、光照、物体贴图全打乱，让机器人习惯各种诡异的组合，这样下放到真实世界的成功率反而更高。
Lingbot-World在这块做的相当好。
三. 很棒的动作代理
世界模型如果只会自己滚动，不会被控制，那最多也就是一个超长、有记忆的屏保。
真正好玩的是，当你把WASD和方向键绑上去，甚至把一个动作代理塞进去，在你操控的同时，里面的角色还可以自主行动和规划。
从而涌现出一些全新的事件和玩法，而不是那种单纯的，步行模拟器，只会单纯的走路和跑步而已。
比如这个。
你可以明显的看到，这个布偶随着方向的变化，而自主在房间里进行运动，在过程中还碰到了沙发，从而掉头，避免了穿模。
还有这个。
并不是千篇一律的滑行，而是有急停、有变道，有自主运动。
这一点也是完全不一样的点，是我从来没有见过的，很新，也很强。
看了下论文，LingBot-World在动作这块，主要干了两条线的事。
一条线是最直接的，你自己按键。
你按 W，它往前生成，你按S，它往后拉，你长按A，它会帮你推演侧身走一段路应该是什么样子，这个看着好像理所当然，但其实背后代表着模型已经学会了一种还蛮重要的东西。
动作不是单帧的，而是一个连续意图。
你按一下W，它不会只管下一帧往前挪一点点，而是会在内部自动帮你补出一整个往前走两三步的节奏，把腿步伐、相机抖动、视差变化协同起来。
否则你按W一下停一下，画面只会抖成帧动画。
另一条线，是他们搞的那个AI玩自己世界的动作代理。
你可以把它理解成给LingBot-World添了一个玩家。
这个玩家看不到底层张量，它只看画面和一些文字提示，然后决定“我要往哪走”“我要不要拐弯”“我要不要停下来多看一眼”。
论文里是用一个微调的视觉语言模型
来做这件事，看一帧图，输出接下来几秒钟的命令，让 LingBot-World去执行。
所以还真的挺有意思的，就真的像，我们在玩游戏的感觉。
只不过这个游戏，我们是观测者，我们决定向什么方向去，而AI，会在生成的空间，自主运动。
这一点，确实是一个非常有意思的创新。
写在最后
LingBot-World很强，很有意思，让我突然有了一种。

24年春节2月16号的时候，同样的深夜，同样的凌晨，看到Sora的那一刻。

世界模型，一直是一个全新的、未被探索、还有广阔空间的领域。

他不仅对游戏、对影视、对娱乐都有非常强的意义。

而真正我觉得最核心的场景，其实是为了具身智能，一个优秀的、泛化能力强的世界模型，也能为具身的训练，为他们对现实世界的理解和长程任务，提供低成本高保真的试错空间。

世界模型，也是AI真正由虚到实，进入我们现实空间中的必要条件之一。

而蚂蚁，居然是蚂蚁。

把这个进程，向前推了一大步。

并且直接选择开源，造福所有人。

我很少会对一个技术demo感到兴奋，而最近的兴奋，坦诚的讲，几乎都来自世界模型。

而LingBot-World让我又有了当年最开始玩AI的那种感觉。

世界模型成熟之后，来临的，必然就是井喷式的、进入我们实体世界的、随处可见的具身智能们。

而那时。

才是我心中，真正的AI时代。


wzglyay@virxact.com

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
