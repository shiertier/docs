---
title: "一手实测豆包新发布的视觉理解大模型，他们真的卷起飞了。"
发布日期: 2024-12-18
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647667358&idx=1&sn=e2df5b1173b7f22cf5ef7721e8928ad5&chksm=f19d50578415ddd5a792319a7411d0210deef3ab7fe41bf7d4e26c4709f98f87a9a0e0f96bd3"
---

## 摘要

**1) 一句话总结**
字节跳动新发布的豆包视觉理解模型调用价格大幅下调85%，并在图像计数、复杂角色识别及国内景点辨认等多项实测中表现出优于GPT-4o的准确性，大幅降低了普通用户的AI使用门槛。

**2) 核心要点**
*   **发布与定价**：字节跳动发布豆包视觉理解模型，API已在火山引擎开放，豆包App端已同步上线，模型价格直降85%，进入“厘时代”。
*   **图像计数能力**：在“数狗”测试中，GPT-4o和Claude 3.5均错误回答为11只，豆包连续5次准确识别为12只，并准确指出品种为金毛幼犬。
*   **复杂细节识别**：在手办识别测试中，GPT-4o出现严重幻觉（数量和角色均错），豆包准确识别出总数6个，并正确认出其中4个角色（正确率66%）。
*   **本土景点识别**：在《黑神话：悟空》取景地测试中，豆包与GPT-4o在多数知名景点打平，但豆包成功识别出GPT-4o未能认出的“山东济南灵岩寺塔林”。
*   **视觉常识与抗干扰**：在视觉误导测试（判断滚珠丝杆长度）中，豆包准确给出正确答案，而GPT-4o多次测试均回答错误。
*   **人脸处理权限**：相比GPT-4o因安全限制无法处理任何人脸图像，豆包支持对包含人脸的图像进行视觉理解。
*   **降低交互门槛**：视觉输入无需复杂的语言Prompt（提示词）组织能力，让不擅长文字表达的普通用户（如缺乏设计能力的电商卖家）也能直观地使用AI。

**3) 风险/不足**
*   **数学与复杂计算短板**：实测表明，豆包视觉理解模型在处理复杂的数学公式计算和做题时仍存在错误率，这是其目前的短板。
*   **部分识别仍有瑕疵**：在手办角色识别中未能做到100%完美识别；在部分冷门景点（如小西天、水陆庵）的识别上与GPT-4o一样出现了识别失败（翻车）的情况。

## 正文

人在字节火山发布会现场。

眼睁睁看着他们发了一大堆的模型升级，眼花缭乱，有一种要一股脑把字节系的AI底牌往桌上亮的感觉。

有语音的，有音乐的，有大语言模型的，有文生图的，有3D生成。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURodo4IVV1DEibNIcHO5l274tTrmFu4bDVhsyYzskpmbUlUBXxDPwpPqObibUFqqmre0ic9t6kcUszc1w/640?wx_fmt=jpeg&from=appmsg)

真的过于豪华了，字节真的是，家大业大。。。

但是看完了全场，我觉得最值得写一写，聊一聊的，还是这个：

豆包视觉理解模型。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURodo4IVV1DEibNIcHO5l274tia9Q1WPTIuzI93rszic1ApicP7qibOXxmpyLGficJ6MdL9klO3icaWJa53kA/640?wx_fmt=jpeg&from=appmsg)

效果不仅出奇的好，最关键的是，他们的价格。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURodo4IVV1DEibNIcHO5l274tQCtH1bfV1b1evfCXujFiaKaEOHAdkFtiaQtACX9moAXzreUtZHaxdgoA/640?wx_fmt=jpeg&from=appmsg)

价格直接低85%，直接把视觉理解模型拉入了“厘时代”。

字节，还是那个字节。

说实话，过去一两年，人人都在讲文字推理，讲大语言模型的爆点。

但是视觉理解，才一直是我们认知世界的第一道关卡。

当你来到这个世界睁开眼睛的第一刻，没有学会语言的时候，靠的就是你的眼睛。

我们先
看到光影、颜色，才逐渐分辨出父母的面孔，屋子的空间，
那时没有词汇、没有句子，只有模糊的光影与轮廓。

当我们对这个世界，通过视觉，一步步认识父母的脸，认识身边的玩具，认识窗外的树影，有了基本的认知后，然后才有了咿呀学语的过程。

它是我们触及世界的第一道门，不仅仅是看见了什么，更是用看去建立理解，进而触发思考与关联。

语言是有门槛的，你要先懂词语的意思。可视觉先于语言，是不需要翻译的输入。

有太多普通人，不知道如何描绘自己的需求，无法组织语言清楚的表达一件事，但是把图片扔给AI，问一下，这是任何人都会的。

上至80岁老人、下至10岁孩童，都可以。

所以，对于视觉理解模型，我才如此看重。

而这次新发布的豆包视觉理解模型，除了在火山开放了API，也已经在豆包上上线可以直接体验了。

我也第一时间，在发布会现场拉着我的朋友
@赛博禅心
和
@Max
，在会场厕所门口，从早上跑到中午，饭都没吃，跑了大概100个case，跟GPT4o对比做了个详细评测。

虽然感觉有点对不起他两，但是最后的结果，还是很让人有点惊喜的。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURodo4IVV1DEibNIcHO5l274teqEIznicoLeiaGH9o7do4Wic6ueIianhtvRYz6micgVMeQpsv0LUT0S9QRg/640?wx_fmt=jpeg)

我们测的第一波例子，也是很多视觉大模型最痛苦的，就是数数。

我这有一个万恶之源的图，在好多论文里面都出现过，就是让大模型来数这张图里面有几只狗。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbWWWykCw0VnaPJy8sicicTWRdx0BPGu2kkcfMpIvYMOzs2pfGPiagq3XQA/640?wx_fmt=png&from=appmsg)

正常人类直接数中间的狗头，都非常清晰的能数清楚，一共是12只狗。

但是对于AI来说，那就炸了。数数这事，是最难的。

GPT4o非常自信的给了一个11只的答案。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbncqTou5fZWqFkicSrg4et1xMNE5MI7x0m56KaiclCwLgtjwhEYMI9BPA/640?wx_fmt=png&from=appmsg)

Claude3.5也一个样，自信的爆出了11只的答案，Claude和GPT这两冤家，差点弄的我以为自己数错了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbjBdKZy6d4Uq9VpxUGBau05T4eeHZIIB4PLiacVjvZ3oY0tqvGjnnhBw/640?wx_fmt=png&from=appmsg)

直到把这张图发给豆包。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbVl0wXS7tCLOJKdHR1VquUW50DQMSUib0WRaQGibsLYnpeCictBanVibOeA/640?wx_fmt=png&from=appmsg)

我还怕豆包是不是幻觉了，连续roll了5次，每次都是坚定不移的12只。

而且相比于GPT，还准确的识别了这是金毛巡回犬的幼犬，在答案的准确性和丰富度上，都比GPT4o要强。

于是，我又让它俩，做了另一件更难的事。

红框里有几个手办？分别是什么角色？

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbDPsibt1RzQ3xPmfcyHToBu2xw3ct9IP4icJUNy5VGnOvfZFyGIHRC08Q/640?wx_fmt=jpeg&from=appmsg)

不仅需要精准的识别出数量，还要知道每个角色是啥，这个能答上来，那才是真的懂了。

结果GPT上来直接抽风，上来就是忽悠我4个。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzb8libE8L1Sm50dfZrPW1hI2kmBDWFVPBz7Xlw1kHNdzvBXBRssW4iaaWg/640?wx_fmt=png&from=appmsg)

然后那些角色也在那给我瞎掰，不是，哪有孙悟饭啊？哪有金发角色啊？你家孙悟饭蓝头发啊？

真的，槽点太多，我都不知道从哪吐槽起了。

再回头看豆包。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURq6j6ZInxDHEIVg9nxvj7goaM8afibJ3bRWKPqbPWXlyGHsiciabdwwjR2lPmlgPBhhktlw0wq6QeJcA/640?wx_fmt=png&from=appmsg)

数量6个对了，4个《火影》系列的手办，从
左到右其实是波风水门、漩涡鸣人、迪达拉、蝎
，豆包对了前面两个水门和鸣人，再加漫威的一个雷神和绿巨人。

正确率66%，虽然没能完美识别，但也算是一个巨大的进步了。

这一波，说一句把GPT4o摁在地上打不过分吧。。。

测完数数后，我们又测了一波看图识景点。

直接掏出了黑悟空里面的十大景点，测了一波。

大部分GPT4o和豆包都差不太多，
几乎打了个平手，像大足石刻、悬空寺、开元寺这种都识别出来了，而像小西天、
水陆庵野都一起翻车了
。

本来我觉得这两会在这个点上打个平手，结果，最后一题，GPT4o翻了车。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbVea6f22f5Ujbyrj1zTAY9YU3LRTm3y7zOyKTlNyu7H7Qfvp36BWnicQ/640?wx_fmt=png&from=appmsg)

这个塔林，是
山东济南灵岩寺塔林。
自唐以降，墓塔成林。

早为钟，黄昏为鼓，白为方，才有了所谓“晨钟暮鼓白天方”。

而豆包，在这最后一题上，守住了自己的荣耀，回答了上来，从而险胜GPT4o一筹。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr4YEicZzricRqplXHKGRUuzbInIhBZ0a8YzrP8kYEM63nGm1LfgY3HCzj6RSvmofbVW4CMZfz5wq3g/640?wx_fmt=png&from=appmsg)

在一些世界常识中，GPT4o也落败了。

比如这根经典的滚珠丝杆，做了个视觉误导，问哪根最长。

豆包没啥问题，准确的回答了左边第二根最长。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUymjOzfwHO0zOvGJrG0shMSHz5WRb5wib7Bp28iarWd62Qn3ks8ibN3SicUYQ/640?wx_fmt=png&from=appmsg)

但是GPT4o，却又翻车了，我roll了5次，每一次都信誓旦旦的告诉我，就是最左边最长，我都甚至怀疑是不是我自己的眼睛瞎了。。。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUym41wOlPKmuOIdvJEcn4keibflibhjGpdTiazeAbjLFI5eBQp9DjJyWAAKA/640?wx_fmt=png&from=appmsg)

我们也做了一个非常详细的统计表格。把豆包和GPT4o的评测，每个跑三次放在了一起对比。

也能看出来，在大多数的任务上，豆包的这个视觉理解大模型都比GPT4o识别的更精准、更详细，对中国文化的一些内容，懂的也更多。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUymmYmUiaxPEyA2QDqgKKKS3YoTiaOrKoU43hsdUjQrg8icj1Y2ZlQgPFG0w/640?wx_fmt=png&from=appmsg)

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUym76T8W586kDqjEicmib2zybZFEnuOyRU3UW6j74XkXW6IAm95WLRGLLOw/640?wx_fmt=png&from=appmsg)

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUymVibMntcibMQmm2t7voBRvx0gL7jNZwwL7pibGwukuvd2d4OTY0whzQZfw/640?wx_fmt=png&from=appmsg)

而且还有个很有趣的点，就是GPT4o因为那坑爹到家的安全限制，所以他没法看到任何人脸。

但是，豆包可以。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURpfuwr0ZcByTMR3LsREVUymR5aPhd8p7Ppn7xGdYhnx8npibOIquBE1NVEDibb8miadgSVt9eN5OxhBQ/640?wx_fmt=png&from=appmsg)

当然，也并不是说豆包在视觉理解上，它就强到爆炸了。

不行的点，当然也有。

比如我们发现，在一些数学公式的计算上，错误率还是会有一些的。

比如这道题。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURq6j6ZInxDHEIVg9nxvj7go8IYHJwQFc03kcdAa7piabWbLC7dVfcZgjVaoicupKWntCqloyBHgb89g/640?wx_fmt=png&from=appmsg)

答案其实是A。

但是扔给豆包的时候，会发现，回答还是会有一些错误。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURq6j6ZInxDHEIVg9nxvj7goSrRE4CVVZmzaFFpapHsf0gpH6SStIRbztJ8nnpPGgCYIQpY7aicyNNA/640?wx_fmt=png&from=appmsg)

在一些复杂的计算上，还是会有一些差距，毕竟做题，真的一直以来都是大模型的短板。

但整体来看，这波升级就是解决了很多基础的常识性问题，让大模型，有了更强的眼睛，也有了更好的脑子。

还是非常有用的。

文章的最后，我突然想说一个关于我朋友和他想要的AI的故事。

这哥们是一个大概40岁出头的中年人，压力很大，背着房贷，四脚吞金兽还在地上跑。人在一线城市，平时要上班养家糊口，又在业余时间搞了点自己的小买卖，想减轻一点家里的压力。

他以前和我说过，他最大的痛苦就是没有时间学更专业的技能，他那个网店是卖点数码的小玩意，但是吧自己又不会拍好看的商品图，不懂设计，也没有钱请专业摄影师和设计师。

我当时给他推荐了一些电商的AI生图工具，能自动给产品做美化背景，能改色调、能帮他处理一些杂事。

但有个问题，这哥们没啥想象力，审美上也有点差异，所以对于AI绘图的那些Prompt描述能力不行，总是词不达意，AI给出的图经常也有点离谱。

后来有天，他跟我说过，他真正想要的那种AI产品，是他不用管那些乱七八糟的，是想让自己的数码小玩意融入一个夏日海滩的场景的时候，他只需要拍张桌上堆满物品的乱七八糟的图，然后把那个产品圈出来，对AI说：

“给我用这件单品，搞个夏日风海报，然后把我桌面上那些杂乱的东西都变成整洁的道具摆放。”

AI看懂后，直接创作出一张清爽的营销图，就完事了。

这个哥们跟我聊天说这样的需求时，他眼睛里放光。问我有没有这样的东西。

我说，现在还真没有。

然后看着他可惜的眼神，嘴角轻轻的叹了口气。

但是我相信，随着视觉理解模型的进步，随着一句话改图的进步，随着这两者，发光发热继续融合。

一定会有那么一天，能让那哥们，有眼睛里发光的那天。

而且可能，就在不远的将来。

让每个人，都能享受科技的乐趣，这就是技术，真正该发挥的作用。

不是替代，而是帮助。

帮助一个普通人在沉重生活里找到一丝自我创造的乐趣。

帮助那些有想法但缺手段的人，让他们用更少的时间把脑中蓝图变为现实。

我觉得，这可能才是，最酷的事吧。

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
