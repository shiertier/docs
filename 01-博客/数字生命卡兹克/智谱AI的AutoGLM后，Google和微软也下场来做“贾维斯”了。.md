---
title: "智谱AI的AutoGLM后，Google和微软也下场来做“贾维斯”了。"
发布日期: 2024-10-30
作者: "数字生命卡兹克"
来源: "微信公众号"
原文链接: "https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647666253&idx=1&sn=34327ec7163ccfd16ce7d42063b746fb&chksm=f139ca5c99b00ee1868b3693d084f1e9baf0aefce90213b1b989009f295c0e64ecf8117ea7eb"
---

## 摘要

**1) 一句话总结**
随着多模态基座模型和强化学习技术的成熟，微软、谷歌、Anthropic与智谱AI等企业近期密集布局自主人工智能（Agent），标志着AI正从纯文本交互向直接接管设备界面的全自动化操作迈进。

**2) 关键要点**
*   **密集发布潮**：短短一周内，Anthropic（Claude Computer Use）、智谱AI（AutoGLM）、微软（OmniParser）相继发布了用于设备或网页自动化的模型，谷歌也被曝将推出接管计算机的AI产品。
*   **市场反馈积极**：自主人工智能的出现点燃了市场热情，智谱AI的AutoGLM发布后，在二级市场直接带动了相关概念股的爆发。
*   **核心工作流**：自主AI完成任务的标准流程为：理解用户需求 -> 系统规划 -> 调用工具执行任务 -> 目标完成。
*   **早期方案的局限**：一年多前爆火的AutoGPT（GitHub 16万星）因纯基于大语言模型，无法直接处理屏幕视觉信息（图转文会丢失信息），导致可控性差、任务易中断。
*   **突破因素一：多模态模型成熟**：过去一年半进展缓慢的首要原因是多模态模型训练复杂且耗费资源。近期各家多模态基座能力基本达标，使AI能够直接“看懂”屏幕UI元素。
*   **突破因素二：强化学习与动态数据**：互联网缺乏教导AI“如何操作App”的动态行为数据。今年3、4月起，业内形成共识，通过强化学习补充数据，构建类似o1模型的多模态巨型思维链（CoT）。
*   **执行机制**：模型在执行时，会一边观察当前屏幕界面，一边利用思维链进行步步推理（如判断是否需要打开新界面、识别按钮/表单、点击发送并验证目标是否完成）。

**3) 风险与不足**
*   **“滑动”操作预测困难**：滑动屏幕对AI而言是目前最麻烦的操作。人类基于数十年的UI经验能预测下一屏内容，而AI缺乏这种UI常识和交互可预测性。
*   **产品成熟度欠佳**：目前已发布的产品（如Claude的Computer Use和智谱的AutoGLM）仍存在一堆问题，远未达到完美地步，后续仍需灌入大量行为性数据以形成泛化能力。

## 正文

昨天，微软忽然发布了一个新的模型，能够用于网页自动化操作。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSHwSWyGs1XoX55icRu8bwtISYbVGJiaZYE9GQbqAibXqdN2aUj7z4vpA6eg/640?wx_fmt=png&from=appmsg)

他们也正式开卷跟智谱AutoGLM一样自主人工智能了。

而前天，我也在The Information网站上看到一个消息：

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSHDpj3xqndBoBNUBNZUr6YicquibsQ19ia79pfAk1In9Q6UfztBw6wGEGibw/640?wx_fmt=png&from=appmsg)

标题Google Preps AI That Takes Over Computers，翻译过来就是：

谷歌准备推出接管计算机的人工智能。

上周三Claude、周五智谱AI、周末Google，然后微软。

短短一周内，已经有四个公司爆出来要发布类似的产品了，其中三个已经悄悄发布产品：Anthropic的Claude，智谱的AutoGLM和微软的OmniParser。这些产品的能力有目共睹。

Goolge虽然也只是个爆料，但是大概率今年就能出来，非常心急，想把坑先占上。

而且，我知道的消息是，OpenAI内部肯定也在做，就看什么时候掏出来了。

二级市场对于这种自主人工智能，反馈也非常的正。上周五智谱的AutoGLM出来之后，在金融圈直接爆了，连智谱AI概念股都出来了。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSHlZUVr8ZTSoccczqcVq8JoeMsvjhajAObMELenicsVyWibnRkhkskUrRw/640?wx_fmt=png&from=appmsg)

自主人工智能，好像瞬间点燃了AI圈的热情。

又开启了新一轮的用户心智的抢占。

毕竟刚刚开卷，哪家最早发，哪家确实就是会有优势。

不过自主人工智能的热度确实有点超乎了我的想象。

不过也能理解，类似于这种你发个指令他就会全自动化去处理的AI，
才符合我们对人工智能的真正的期待，才有一点，那种AI变成现实的感觉。

现在的AI，坦率的讲，虽然有一些智能，但是远远离不开人工，完全没有达到解放双手的目的，绝大多数时候都是“人工”+“智能”。

而且对于各家AI公司来说，底层模型的能力已经卷到一定的瓶颈了，看现在大模型的一些榜单，大家也一点不关心了。

需要一些更科幻的，更新鲜的刺激。

所以这个时候，自主人工智能过来接棒，就很香。

而自主人工智能完成任务的一个大致流程是：

理解用户的需求-》系统规划-》调用工具执行任务-》目标完成

这个流程看起来其实不复杂。

一年多前，就有人在做了，最经典的那个项目，github上狂揽十六万星的噬星狂魔AutoGPT。

![](https://mmbiz.qpic.cn/mmbiz_png/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSHlOPL5URRG9wdoWvz8hnmFBPKUUbL5PygibaiaPZU9bIgDNe6NuHv4YEQ/640?wx_fmt=png&from=appmsg)

但是AutoGPT到后面开始沉寂，其实有个很大的问题，就是完全基于大语言模型做的。

这个就有很多的局限性。

比如，纯粹的语言模型只能处理文本，而现在很多任务比如点外卖，打车都需要读取屏幕信息。大语言模型本身不能直接处理，往往需要多加一步将图片转换为文本输入。

而图片转换为文字后，对于大语言模型又会丢失很多信息。

好比你被蒙着双眼，只是语言告诉你屋里有些什么，无论语言描述多么细节，你想象力多么丰富，脑海里都无法还原得与真实一模一样。

模型的可控性比较差，模型就容易懵逼，导致任务中断，或干脆给你随机发挥，听天由命。

所以AutoGPT能做到的事情还是比较少，效果也没有那么好，慢慢就淡出大家的视野了。

直到最近这波新的自主人工智能浪潮。

但是我也挺好奇一个问题，就是这将近一年半的时间，自主人工智能为啥都什么消息，直到最近，才开始密集发声？

是各家都在卷其他赛道，无暇顾及，还是都在做，只不过遇到了瓶颈，最近才有所突破？

我就去密集咨询几家国内AI大厂的朋友。

其实大家口径也都出奇的一致。

就是大家都在稳步推进，只是最近刚好到了一个可以拿出来用的时间点，而且大家的进展其实也没有差异太多。

而这一年半，自主人工智能没咋出现在公众视野，看起来进展很缓慢的样子，其实有两个最主要的原因。

多模态模型不够成熟。

2. 缺太多行为数据了。

第一个点其实很好理解。

就是你不能让模型蒙着眼睛去规划任务，他都不知道屏幕上面的元素长啥样，纯粹靠文字来描述，这个效果肯定很差。

所以推进这块，必须要有很强的多模态模型的基座能力作为基础。

而多模态的模型，训起来其实就比纯粹的大语言模型复杂多了。数据量、资源的消耗都是指数级增长。本身就是慢，连Claude都是今年3月才上线多模态能力的。

所以在模型基座上，就是会很拖沓，这个是客观的事实，不过最近几个月，大家的多模态模型已经基本都能用了，所以基座模型层面，其实就是刚好到了一个节点。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSH00am77DTEg2SaBc7lQujiaXjGO6lI3v3kzKfpFanIxk5bpoXgx0WcSA/640?wx_fmt=jpeg&from=appmsg)

去调研的智谱的大佬，也给出了我们同样的答案。

第二个原因，其实就是数据上。

虽然互联网包含大量的人类知识，但主要由静态信息（图片、文字）组成，这些静态信息无法反应一些动态的过程。

比如，模型可以学习理解外卖界面上的脆皮炸鸡是多少钱。因为网上有大量的静态数据教会它钱是什么、能干什么。模型可以理解“钱是能买到炸鸡的”。

但缺乏动态数据教它怎么“找到美团App，点开App，如何搜索脆皮炸鸡，然后点击下单”的这样一个动态过程。

这个其实就跟o1的那套推理的路很像，模型不仅要知道结果，还要知道其中的过程。

整个自主人工智能的操作路径，其实就是一个多模态的巨型思维链。

o1和claude给大家打了个样，证明了强化学习这条路是对的，那强化基座模型的推理能力是一个应用方向，而做这种真正的Agent，又是一个强化学习应用的典型场景。

一个偏基模，一个偏应用。

而这套通过强化学习补充数据的范式，其实也就是今年3、4月以后，才逐渐形成的业内共识。

所以，着就能解释，为什么过去一年半的时候，这种自主人工智能一直没啥进展，直到最近才密集发声，其实就是多模态基座和数据的原因。

最后，再简单说说
AutoGLM这种能力，大概是怎么实现的，具体细节他们也都没透露，我只能根据我的调研结果，进行一些猜测，不过AutoGLM团队最近应该会发个技术报告，到时候可以关注一下。

比如一个最简的例子，
说：
微信发送“今天疯狂星期四V我50”的消息给鲜虾包。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OjgKEXmLURr9VtZia3TBDxCkNJwSK6WSHgZ7QLlJmvsYEticicePCYz7PfiaZDfVM9XPrBLCLiaSzJD8qhqibTszCHCA/640?wx_fmt=jpeg&from=appmsg)

那么大模型拿到任务后，
会一边看当前屏幕的界面，一边利用思维链一步一步推理：

任务可以在当前的UI界面下继续进行吗？是的话进行第2步，否的话就会思考“应该打开什么界面”然后进行下一步动作。
分析当前UI和最终任务的关系，分解成一步一步的动作”
现在首先需要填充输入框“今天疯狂星期四V我50
”
然后点击“发送”
然后思考问题是否最终被解决？如果是的话结束，如果否的话回到第1步继续循环。

这一切，都建立在模型能看到屏幕，能理解屏幕上那些乱七八糟的元素的基础上。他不仅需要复杂的规划能力，还需要直到这个是按钮，这个是单选控件，这个是表单，这个是开关等等。

而滑动这个操作，反而是最麻烦的，人看起来很简单，是因为人对于UI界面，已经有数十年的经验了，滑动本身就是预测的过程，我们根据我的经验，大概能猜到这一屏如果没有我要的信息，那他可能是在下一屏。

所以我们会进行一个滑动操作，但是对于AI来说，这个预测，反而是最难的。

所以后面，需要继续灌数据，灌大量的行为性数据，形成泛化能力。

让AI，有跟人类一样的，UI常识，和对交互的可预测性。

虽然目前不是那么完美。

Claude的Computer Use和智谱的AutoGLM，都有自己的一堆问题，也远远没到一个算是完美产品的地步。

但这毕竟也只是刚刚开始。

当一切路径明确。

两个月时间。

可能，一切就变天了。

## 关联主题

- [[00-元语/数字生命卡兹克]]
- [[00-元语/AI]]
- [[00-元语/llm]]
