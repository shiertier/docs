---
title: "迈向智能体系统扩展的科学：智能体系统何时且为何有效"
---

## 摘要

**1) 一句话总结**
基于对180种配置的大规模定量评估，本文打破了“智能体越多越好”的经验法则，提出了应根据任务的具体属性（如可并行性、顺序依赖和工具密度）来选择最优智能体架构的扩展原则。

**2) 核心要点**
*   **智能体任务的界定：** 真正的智能体任务需具备三个属性：与外部环境持续多步交互、在部分可观测条件下迭代收集信息、基于反馈自适应调整策略。
*   **评估范围：** 研究在四大基准测试（财务推理、网页导航、规划、工具使用）中，使用三大模型系列（GPT、Gemini、Claude），评估了五种架构（单智能体、独立型、集中型、去中心化型、混合型）。
*   **对齐原则（性能提升）：** 在可并行化的任务（如财务推理）中，集中型多智能体架构的性能比单智能体系统（SAS）提升了 **80.9%**。
*   **顺序惩罚（性能下降）：** 在需要严格顺序推理的任务（如规划）中，多智能体变体因通信开销导致推理碎片化，性能下降了 **39%至70%**。
*   **架构与安全性（错误控制）：** 集中型系统通过协调者充当“验证瓶颈”，将错误放大率控制在 **4.4倍**；相比之下，独立型系统缺乏检查机制，错误放大率高达 **17.2倍**。
*   **预测模型：** 研究开发了一个预测模型（$R^2 = 0.513$），能够利用工具数量和可分解性等任务属性，以 **87%** 的准确率预测并识别出未知任务的最优协调策略。

**3) 风险/不足**
*   **错误级联风险：** 在多步交互中，单一错误极易在整个工作流中引发连锁反应；特别是独立型多智能体系统，会使错误不受控制地级联放大（放大17.2倍）。
*   **认知预算消耗：** 在偏向顺序执行的工作流中盲目增加智能体，会产生高昂的通信开销，导致系统没有足够的“认知预算”完成实际任务。
*   **工具使用瓶颈：** 当任务需要调用大量工具（如超过16种）时，协调多个智能体所产生的开销会不成比例地急剧增加。

## 正文

人工智能（AI）智能体——具备推理、规划和行动能力的系统——正逐渐成为现实世界 AI 应用的常见范式。从编程助手到个人健康教练，整个行业正在从单次问答向持续的多步交互转变。

在传统的机器学习模型中，研究人员长期使用既定指标来优化准确率，但智能体引入了新的复杂性。与孤立的预测不同，智能体必须在持续的多步交互中进行导航，在这一过程中，一个单一的错误可能会在整个工作流中引发连锁反应。这种转变促使我们超越标准的准确率指标，去思考：我们究竟该如何设计这些系统以实现最佳性能？

从业者通常依赖经验法则，例如假设“智能体越多越好”，认为增加专业化的智能体会持续改善结果。然而，通过对 180 种智能体配置进行大规模的受控评估，我们得出了首个针对 AI 智能体系统的定量扩展原则，并对这一假设提出了挑战。研究表明，“增加智能体”的方法往往会触及天花板，如果与任务的具体属性不匹配，甚至会导致性能下降。

### 什么是真正的“智能体”任务？

为了理解智能体如何扩展，我们首先界定了什么使一项任务具有“智能体特性”。传统的静态基准测试衡量的是模型的知识，但无法捕捉实际部署中的复杂性。我们认为，真正的智能体任务需要具备以下三个特定属性：

1.  与外部环境进行**持续的多步交互**。
2.  在部分可观测的条件下进行**迭代式的信息收集**。
3.  基于环境反馈进行**自适应的策略调整**。

我们在四个不同的基准测试（涵盖财务推理、网页导航、规划和工具使用）中，评估了五种经典的架构：一种单智能体系统（SAS）和四种多智能体变体。这些架构定义如下：

*   **单智能体（SAS）：** 一个孤立的智能体，按顺序执行所有推理和行动步骤，并拥有统一的记忆流。
*   **独立型（Independent）：** 多个智能体并行处理子任务，彼此不进行通信，仅在最后汇总结果。
*   **集中型（Centralized）：** 一种“中心辐射型”模型，由一个中央协调者将任务分配给工作智能体，并综合它们的输出。
*   **去中心化型（Decentralized）：** 一种点对点网络，智能体之间直接通信以共享信息并达成共识。
*   **混合型（Hybrid）：** 结合了层级监督和点对点协调，在中央控制与灵活执行之间取得平衡。

### 打破“智能体越多越好”的神话

为了量化模型能力对智能体性能的影响，我们在三大主流模型系列（OpenAI GPT、Google Gemini 和 Anthropic Claude）上评估了这些架构。结果揭示了模型能力与协调策略之间复杂的关系。

虽然性能通常会随着模型能力的提升而提高，但多智能体系统并非万能药——根据具体配置的不同，它们既可能显著提升性能，也可能出人意料地导致性能下降。增加智能体可以在可并行化的任务中带来巨大的收益，但在更偏向顺序执行的工作流中，往往会导致收益递减，甚至性能下滑。

*   **对齐原则（The alignment principle）：** 在可并行化的任务（如财务推理，不同的智能体可以同时分析收入趋势、成本结构和市场对比）中，集中型协调架构的性能比单智能体提高了 **80.9%**。将复杂问题分解为子任务的能力使智能体能够更高效地工作。
*   **顺序惩罚（The sequential penalty）：** 相反，在需要严格顺序推理的任务（如规划任务）中，我们测试的每一种多智能体变体都导致性能下降了 **39%至70%**。在这些场景下，通信的开销使得推理过程碎片化，导致没有足够的“认知预算”来完成实际任务。
*   **工具使用瓶颈（The tool-use bottleneck）：** 我们发现了一种“工具-协调权衡”。当任务需要使用更多工具（例如，一个可以访问 16 种以上工具的编程智能体）时，协调多个智能体所产生的“税收（开销）”会不成比例地增加。

### 架构作为一种安全特性

对于现实世界的部署而言，最重要的一点或许是我们发现了架构与可靠性之间的关系。我们测量了**错误放大率**，即一个智能体的错误传播到最终结果的速率。

我们发现，独立型多智能体系统（智能体并行工作且互不交流）会将错误放大 **17.2倍**。由于缺乏互相检查工作的机制，错误会不受控制地级联放大。而集中型系统（带有协调者）则将这种放大率控制在了 **4.4倍**。协调者有效地充当了“验证瓶颈”，在错误传播之前将其拦截。

### 智能体设计的预测模型

除了回顾性分析，我们还开发了一个预测模型（$R^2 = 0.513$）。该模型利用工具数量和可分解性等可测量的任务属性，来预测哪种架构表现最佳。对于未见过的任务配置，该模型能够以 **87%** 的准确率正确识别出最优的协调策略。

这表明我们正在迈向一门新的智能体扩展科学。开发者不再需要盲目猜测是该使用智能体集群还是单一的强大模型，而是可以通过观察任务的属性（特别是其顺序依赖性和工具密度），来做出有原则的工程决策。

### 结论

随着基础模型的不断进步，我们的研究表明，更聪明的模型并不会消除对多智能体系统的需求，反而会加速这种需求——但前提是必须采用正确的架构。通过从经验法则转向定量原则，我们可以构建出下一代 AI 智能体，它们不仅在数量上更多，而且将更加智能、安全和高效。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Agent]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/evals]]
