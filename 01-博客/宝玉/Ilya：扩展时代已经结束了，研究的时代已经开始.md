# Ilya：扩展时代已经结束了，研究的时代已经开始

## 文档信息
- 来源：https://baoyu.io/blog/ilya-from-scaling-to-research
- 发布日期：2025-11-26
- 作者：宝玉

## 摘要

### 1) 一句话总结
Ilya Sutskever 认为 AI 行业已从单纯堆砌算力和数据的“扩展时代”回归“研究时代”，其新公司 SSI 将致力于解决核心的泛化问题，目标是在 5 到 20 年内打造出能像人类一样快速学习的 AGI。

### 2) 关键要点
*   **扩展时代（Scaling）面临瓶颈**：自 2020 年起依赖堆砌数据和算力获取确定性回报的时代正在结束，互联网预训练数据即将耗尽，且单纯在强化学习上增加 100 倍算力未必能带来质变。
*   **回归研究时代**：行业重新回到需要探索新想法的阶段。SSI 认为前沿研究的真正瓶颈是“想法”而非最大规模的算力（如 AlexNet、Transformer 和 o1 的初期研发均未依赖极致算力）。
*   **重新定义 AGI**：AGI 不应是一个出厂即具备所有知识的“成品”，而应是一个像人类一样能够快速学习和试错任何技能的“学习者”。
*   **泛化能力是核心**：当前模型在基准测试中表现优异，但真实世界的泛化能力极差（如改 bug 时陷入死循环）。人类情绪起到了“内置价值函数”的作用，能提供早期反馈从而实现高效泛化。
*   **SSI 的战略调整**：SSI 放弃了原先“不发布中间产品、直奔超级智能”的计划，转向“渐进式部署”，以促使公众、政府和竞争对手直观感受到 AI 能力，从而推动安全合作与监管。
*   **SSI 的资源与目标**：SSI 已融资 30 亿美元，资金将高度集中于前沿研究而非产品开发或推理服务，目标是在 5 到 20 年内实现人类级别的学习能力。
*   **安全对齐的新思路**：Ilya 提出让 AI 关心“所有有感知能力的生命”（包含 AI 自身），认为这比单纯让 AI 关心人类更自然；同时提及“人机融合”（如脑机接口）是另一种消解对齐问题的激进方案。

### 3) 风险与隐患
*   **人类主导的“奖励作弊（Reward Hacking）”**：在强化学习阶段，研究员为了让基准测试（Benchmark）得分好看而过度优化训练目标，导致模型“应试能力”与“真实世界能力”之间的鸿沟被放大。
*   **对齐方案的少数派风险**：如果将 AI 对齐目标设定为关心“所有有感知能力的生命”，未来人类在数量上可能成为极少数，这无法绝对保证人类的利益。
*   **认知与想象力脱节**：由于不存在的 AI 很难被想象，人们（甚至包括 AI 研究员）常因当前模型的低级错误而低估未来 AI 的能力与风险，导致安全防范和监管滞后。

## 正文
**Ilya Sutskever** 大概得有一年多没参加播客访谈了，自从 OpenAI 宫斗离职创办 **SSI（Safe Superintelligence）** 后之后就很少露面了。

最近，他在 **Dwarkesh Patel** 的播客中进行了一场长达一个半小时的深度对谈。[https://www.youtube.com/watch?v=aR20FWCCjAs](https://www.youtube.com/watch?v=aR20FWCCjAs)

Ilya 毕竟是 Ilya，他不像 Sam Altman 整天满嘴跑火车，只是为了卖货，访谈里面有很多干货，我猜一些观点甚至会影响以后 AI 在研发和投资上的走向。

一个多小时的内容很难说几句话就总结完，还是按照话题挨个整理一下。

* * *

【1】为什么 AI 能在考试中碾压人类，却修不好一个简单的 bug？
----------------------------------

Ilya 在访谈里抛出一个很多人都遇到过也困惑过的现象：现在的模型在各种评分上表现惊艳，但用起来却远远跟不上能力曲线。更诡异的是，你让它改个 bug，它改完引入新 bug；你指出新 bug，它道歉后又改回旧 bug。两个 bug 来回切换，像在打乒乓球。

Ilya 使用一个类比来解释这个问题的：**想象两个学生**。

*   **第一个**立志成为顶级竞赛程序员，刷了一万小时题，背熟所有算法模板，练到条件反射般精准。

*   **第二个**觉得竞赛挺酷，随便练了一百小时，也拿了不错的成绩。

**哪个人未来职业发展更好？大概率是第二个。**

Ilya 说，现在的模型比第一个学生还极端。训练时把所有竞赛题都刷了，还做了数据增强生成更多变体，一遍遍强化。结果就是：所有算法技巧都刻在指尖，但这种准备程度本身就限制了泛化能力。

> 这个类比点破了一件事：**能力和泛化是两回事**。刷题刷到极致，可能恰恰堵死了触类旁通的路。

* * *

【2】真正的 reward hacking（奖励作弊）是人类研究员
---------------------------------

问题出在哪？Ilya 认为是训练数据的选择逻辑变了。

*   **预训练时代很简单**：数据越多越好，什么都往里塞，不用挑。

*   **但 RL（强化学习）时代不一样了**：你得选择做什么强化训练、用什么环境、优化什么目标。

于是一个微妙的循环出现了：研究员想让发布时的 benchmark 得分数字好看，就设计能提升这些指标的 RL 训练。模型变得越来越会考试，但考试能力和真实世界能力之间的鸿沟被放大了。

> **讽刺的是，真正在奖励作弊的不是模型，是设计训练的人在不知不觉中过度关注了考试成绩。**

* * *

【3】为什么人类学东西这么快
--------------

说到这里就要问一个更深的问题：为什么人类学东西这么快，而且这么稳？

Ilya 提到一个特别有意思的医学案例：

> 有个人因为脑损伤，失去了所有情绪——不会难过、不会愤怒、不会兴奋。看起来他还是能说话，能做智力题，测试分数也正常。但他的生活彻底崩溃了：花几个小时决定穿哪双袜子，财务决策一塌糊涂。

这说明什么？**情绪不只是情绪，它在某种程度上充当了内置的价值函数。** 它告诉你什么事值得做，什么选择是好是坏，不需要等到最后结果出来才知道。

价值函数是个技术术语，但概念不难理解。传统的强化学习是这样的：模型做一长串动作，最后得到一个分数，然后用这个分数去调整之前所有步骤。问题是，如果任务需要做很久才有结果，学习效率就很低。

**价值函数的作用是“提前剧透”**。比如下棋时你丢了一个子，不用下完整局就知道这步不好。编程时如果探索了一个方向走了一千步发现不对，价值函数能让你在一开始选择这个方向时就得到负反馈。

> 这就像 GPS 导航，不是等你开到终点才告诉你这条路不对，而是在你刚拐错弯时就开始重新规划。

人类大概就是有这么一套系统。而且这套系统的妙处在于：它相对简单，却在非常广泛的情况下都管用。我们的情绪主要是从哺乳动物祖先那里继承来的，针对的是几百万年前的环境，但放到现代社会居然还能用得不错。当然也有失灵的时候，比如面对满街的美食，我们的饥饿感就管不住了。

* * *

【4】堆算力堆数据的扩展时代已经结束了，研究的时代已经开始
-----------------------------

Ilya 认为规模化时代结束了？

他给了个很有意思的视角：

*   在 **2012 到 2020 年**，大家在做研究，试这试那，看什么有意思。

*   然后 **2020 年左右**，scaling law(规模化定律) 被发现了，GPT-3 横空出世。突然之间所有人意识到：原来只要扩展规模，堆数据、堆算力、堆模型参数大小，一路扩充，就能稳定得到更好的结果。

规模化的好处是低风险。而研究是有风险的，你得雇一堆聪明人去探索，不保证有成果。但规模化？只要投入更多资源，就一定能看到回报。公司喜欢这种确定性。但副作用是它吸走了房间里所有的氧气，创新空间被压缩，最后变成“公司比想法多”的局面。

**但现在呢？** 预训练的数据就那么多，互联网就这么大，总会用完。Gemini 据说找到了从预训练榨取更多的方法，但这条路终归有尽头。然后大家转向了强化学习，开始在那上面堆算力。

可问题是：现在算力已经这么大了，再 100 倍真的会质变吗？Ilya 不这么认为。他觉得我们又回到了需要思考到底该做什么的阶段，而不是继续闷头堆资源。

> 这就像爬山。一开始你发现有条路，往上走就是了，越走越高。但总有一天，你会发现这条路到头了，再往前走也高不了多少。这时候要么换条路，要么换种爬法，总之不能继续原来的策略。

现在的 AI 行业就处在这个节点上。所以 Ilya 说：**我们回到了研究时代，只不过这次手里有大得多的计算机。**

* * *

【5】泛化能力才是核心问题
-------------

在 Ilya 看来，当前最根本的问题是：**这些模型泛化能力太差了。**

什么叫泛化能力差？就是学一样东西需要的数据量太多，而且学会的东西换个场景就不灵了。

**人类不是这样的：**

*   一个青少年学开车，10 个小时基本就能上路了。

*   人类五岁小孩的视觉能力就足以支持自动驾驶了，虽然他不会开车，但识别路况、判断距离这些能力已经很强了，而且这些能力是在父母家里那种数据多样性很低的环境里学会的。

*   更关键的是，人类学编程、学数学这些东西也很快。这些可不是进化给我们的能力，因为我们的祖先根本不需要写代码。

这说明人类不光是在某些特定任务上有进化优势，而是在学习这件事本身上就有某种更本质的能力。

模型呢？虽然在某些具体任务上超过了普通人，但要说学习能力，还差得远。

Ilya 对这个问题有想法，但他说现在不能详细聊，因为在这个竞争激烈的领域，不是所有机器学习想法都能公开讨论的。不过他给了个方向性的提示：**这很可能跟如何做到像人类那样高效、稳定的泛化有关。**

还有一个可能的阻碍：也许人类的神经元实际上比我们想的做更多计算？如果是这样，事情就更复杂了。但不管怎样，人类的存在本身就证明了这种高效学习是可能的。

* * *

【6】重新定义 AGI：从成品到学习者
-------------------

这里 Ilya 做了一个概念上的重要修正。

**AGI 这个概念怎么来的？** 是作为“狭隘 AI”的反面而诞生的。以前的 AI 只会下棋，只会玩游戏，非常狭隘。所以大家说：我们要造通用的 AI，什么都能做的 AI。

预训练强化了这个印象，因为预训练确实让模型在各种任务上都变强。于是“通用 AI”和“预训练”在概念上绑定了。

**但这里有个问题：按照这个定义，人类自己都不算 AGI。**

人类有一套基础能力，但缺乏大量具体知识。我们靠的是持续学习。一个聪明的十五岁孩子什么都不会，但学什么都快。“去当程序员”、“去当医生”、“去学习”，部署本身就包含一个学习和试错的过程。

> 所以 Ilya 心目中的超级智能，**不是一个出厂就什么都会的成品，而是一个能像人一样快速学习任何技能的学习者。**

* * *

【7】能自动学习的 AI 会有多快到来？有多危险？
-------------------------

那么问题来了：如果有这样一个系统，能像人类一样快速学习，而且可以大量复制部署，会发生什么？

人类入职六个月才能产出价值，但这种 AI 可能几周就行。而且不同副本学到的东西还能合并，这是人类做不到的。这难道不会导致某种爆炸式增长？

Ilya 认为确实会有快速的经济增长，但到底有多快很难说。一方面有高效的劳动者，另一方面现实世界很大，很多东西有自己的节奏，不是你想快就能快的。

但他确实改变了一些想法。以前 SSI 的计划是“直奔超级智能”——不发布中间产品，一鼓作气搞定终极目标。现在 Ilya 觉得，**渐进式发布可能更重要**。

* * *

【9】为什么要渐进部署？因为想象不出来就得看见
-----------------------

访谈中 Ilya 反复强调一点：**AI 的问题在于它还不存在，而不存在的东西很难想象。**

你可以读一篇文章说“AI 会变得多厉害”，但读完你觉得“哦，有意思”，然后回到现实。如果你亲眼看到 AI 在做那件事，感受完全不同。

> 他打了个比方：像二十岁时讨论“年老体弱是什么感觉”。你可以聊，可以想象，但真正的理解只能来自经历。

这导致一个实际问题：所有关于 AI 风险的讨论，都基于对未来 AI 的想象。而想象往往跟不上现实。就连天天做 AI 的人，也会因为当前模型的各种低级错误而低估未来模型的能力。

Ilya 的预测是：随着 AI 变得更强，人们的行为会发生根本改变。竞争对手会开始合作搞安全，政府和公众会开始认真对待监管。这些事情现在开始有苗头了，但还远远不够。**而真正的催化剂，是让人们看到更强的 AI。**

这也是他对 SSI“直奔超级智能”策略有所松动的原因。原本的想法是不参与市场竞争，专心做研究，等东西准备好了再拿出来。现在他觉得，让 AI 被看见这件事本身是有价值的。当然，无论哪种路径，最终部署都必须是渐进的。

* * *

【9】SSI 在做什么？不同的技术路线
-------------------

SSI 融了三十亿美元。这个数字单看很大，但跟其他公司动辄几百亿的投入比起来似乎不够。

Ilya 算了一笔账。那些大数字里，很大一部分是用于推理服务的。另外，做产品需要大量工程师、销售、产品功能开发，研究资源被稀释。真正用于前沿研究的资源，差距没看起来那么大。

**更重要的是，如果你在做不一样的事，不一定需要最大规模的计算来验证想法。**

*   AlexNet 用两块 GPU 训的。

*   Transformer 论文最多用了 64 块 2017 年的 GPU，换算成今天也就两块卡。

*   第一个推理模型 o1 的推理能力也不是靠堆算力堆出来的。

研究需要一定算力，但不需要最大算力。**真正的瓶颈是想法。**

那 SSI 的技术路线是什么？Ilya 没有完全透露，但核心方向是解决泛化问题。他认为现在的方法会走一段然后撞墙，继续进步但无法突破到真正的人类级学习能力。而 SSI 在探索不同的路径。

**时间表呢？五到二十年，达到人类级别的学习能力。**

* * *

【10】安全对齐是什么？应该对齐什么？
-------------------

说到超级智能，绕不开安全对齐问题。Ilya 的想法是：**让 AI 关心有感知能力的生命。**

为什么是这个目标而不是“关心人类”？他给了个有意思的理由：AI 本身也会有感知能力。如果你想让 AI 关心人类，可能反而更难，因为它需要做某种特殊化处理。而如果让它关心所有有感知的存在，某种程度上更自然，类似人类对动物的共情，来自于我们用同样的神经回路去理解别人和理解自己。

当然这个方案也有问题。如果大部分有感知能力的存在都是 AI，那人类在数量上会是极少数。这真的能保证人类的利益吗？

Ilya 承认这不一定是最好的方案，但他认为至少应该把它列入候选清单，让各家公司到时候可以选择。

还有一个思路他提了但不太喜欢：**人机融合**。通过类似 Neuralink 脑机接口的技术，让人类部分成为 AI。这样 AI 的理解就是人的理解，AI 的处境就是人的处境，对齐问题某种程度上就消解了。但这显然是个很激进的方案。

* * *

【11】如何硬编码高级欲望？
--------------

访谈最后有一段很有趣的讨论。

人类有很多社会性的欲望：想被人尊重、在乎社会地位、关心别人怎么看自己。这些不是低级信号，不像闻到食物香味那样有直接的化学感应器。大脑需要整合大量信息才能“理解”社交场合发生了什么。

**但进化却成功地把“关心这件事”硬编码进了基因。怎么做到的？**

如果说“把多巴胺连到嗅觉感受器”还能想象，那“把奖励信号连到某种需要整个大脑协同计算才能得出的高级判断”就很难想象了。

Ilya 说他有一些猜想，但都不令人满意。这是个谜。但这个谜的存在本身就很有启发性，它说明进化找到了某种方法，可靠地给复杂认知系统植入高级目标。

* * *

【12】什么是研究品味？
------------

访谈最后，Dwarkesh 问了 Ilya 个很本质的问题：作为联合创造了 AlexNet、GPT-3 等一系列里程碑工作的人，你怎么判断什么想法值得做？

Ilya 的回答很诗意：**我寻找的是美感。**

不是随便的美感，而是多方面的美：

*   简洁性

*   优雅性

*   正确的大脑启发

人工神经元是个好想法，因为大脑确实有很多神经元，虽然大脑很复杂但神经元这个抽象感觉抓住了本质。分布式表示是个好想法，因为大脑确实是从经验中学习。

当一个想法在多个维度上都显得“对”，都有某种内在的和谐，你就可以建立自上而下的信念。这种信念很重要，因为它支撑你在实验结果不好时继续坚持。

有时候实验失败不是因为方向错了，而是因为有 bug。怎么判断该继续调试还是放弃方向？靠的就是这种自上而下的美学直觉：这个东西应该是这样的，所以一定能 work，继续找问题。

> **这可能就是顶尖研究者和普通研究者的区别。** 普通研究者容易被数据牵着走，实验不 work 就换方向。而顶尖研究者有某种品味，知道什么是深层次正确的，能够在实验结果和内在直觉之间找到平衡。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/alignment]]
- [[00-元语/interview]]
- [[00-元语/benchmark]]
- [[00-元语/risk]]
- [[00-元语/roadmap]]
