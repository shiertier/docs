# 人工智能的最高奖项——图灵奖，近日颁给了强化学习领域的两位先驱：安德鲁·巴托和理查德·萨顿

## 文档信息
- 来源：https://baoyu.io/blog/turing-award-reinforcement-learning-pioneers
- 发布日期：2025-03-05
- 作者：宝玉

## 摘要

**1) 一句话总结**
2024年度图灵奖授予了安德鲁·巴托（Andrew Barto）和理查德·萨顿（Richard Sutton），以表彰他们在强化学习（RL）领域的开创性贡献，其理论与算法已成为AlphaGo和ChatGPT等现代人工智能系统的核心基石。

**2) 关键要点**
* **奖项授予**：ACM于2025年3月5日宣布，巴托与萨顿因奠定强化学习的核心理论框架而获得计算机界最高荣誉——2024年度图灵奖。
* **研究起点**：两人于1977年在麻省大学阿默斯特分校开启合作，打破当时符号AI的传统，确立了机器通过“最大化环境奖励信号”进行自主试错学习的研究方向。
* **倒立摆突破（1983年）**：两人及同事成功研发出由神经网络驱动的自学习控制系统，使机器在无预编程规则的情况下，通过试错自主学会了平衡倒立摆。
* **演员-评论家架构（1984年）**：萨顿在博士论文中提出Actor-Critic架构，将智能体拆分为负责选择动作的“演员”和负责评价的“评论家”，有效解决了时间信用分配问题。
* **时间差分学习（1988年）**：萨顿提出时间差分（TD）学习方法，允许智能体基于“下一刻的预期”边学边预测。该算法在90年代初被用于TD-Gammon程序，首次在西洋双陆棋中达到人类顶尖水平。
* **策略梯度方法**：两人及团队发展了直接以最大化预期累积回报为目标的策略梯度方法，解决了动作空间连续场景下的策略平滑调整问题，成为深度强化学习的重要基石。
* **出版经典教材（1998年）**：两人合著出版《强化学习：导论》，统一了马尔可夫决策过程、值函数、Q学习等概念与术语，确立了强化学习的统一研究范式。
* **现代AI应用**：其理论直接催生了DeepMind AlphaGo（2016年击败人类围棋冠军的自我博弈技术），并构成了OpenAI ChatGPT等大模型所依赖的“人类反馈强化学习（RLHF）”技术的核心。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/decision-making]]
- [[00-元语/ChatGPT]]
- [[00-元语/llm]]

## 正文
2025年3月5日，全球最大的计算机协会ACM宣布，人工智能领域的最高荣誉——图灵奖——颁给了强化学习领域的两位先驱：安德鲁·巴托（Andrew Barto）和理查德·萨顿（Richard Sutton）。他们提出的强化学习理论，如今已成为ChatGPT等热门AI系统背后的核心技术。

![Image 1](https://baoyu.io/uploads/2025-03-05/Association_For_Computing_Machinery_barto_sutton.jpg.jpeg)

故事的起点是1977年，当时巴托在美国麻省大学阿默斯特分校做研究。他提出了一个有趣的想法：

> 大脑里的神经细胞就像一个个追求享乐、躲避痛苦的小生命。也就是说，人类智慧其实源自无数个细胞为了最大化快乐、减少痛苦而不断摸索。

正是这一念头——一个“享乐式”学习系统渴求最大化环境中特殊信号（奖励）的概念——点燃了他们开拓强化学习（Reinforcement Learning, RL）新领域的激情。他们或许没有料到，这场始于20世纪70年代末的思想碰撞，将改变人工智能发展的轨迹。

不同寻常的求学之路
---------

巴托与萨顿的合作之旅，源自各自迥异却同样非凡的学术背景。年长几岁的巴托出生于20世纪40年代末，最初在密歇根大学攻读造船工程。然而，大学时期一次偶然的阅读经历改变了他的方向：Michael Arbib 以及 McCulloch 和 Pitts 关于“大脑建模”的著作令他着迷。巴托仿佛看到了计算机模拟大脑活动的无限可能性，于是毅然转向数学和计算机科学，1975年获得计算机科学博士学位，研究课题涉及**细胞自动机**——一种启发自生物学的计算模型。这一跨领域的背景培养了他宽广的视野：既懂工程，又通大脑与计算模型，为日后探索智能学习系统打下基础。

相比之下，萨顿则从心理学踏入了计算机科学的大门。他1961年出生于美国俄亥俄，成长在伊利诺伊州芝加哥郊外。斯坦福大学的心理学本科教育让年轻的萨顿对行为学习规律产生了浓厚兴趣——动物如何通过奖励和惩罚来学习，是他关注的核心问题。1970年代，他受到科学家 Harry Klopf 前沿思想的影响：**仅有人工指导的监督学习远不足以产生真正聪明的行为，机器需要通过“奖赏驱动”的试错学习来获得智能**。Klopf 提出的“适应系统的异静态理论”（俗称“享乐型神经元”理论）认为生物神经元会因追求正向刺激而改变行为——这个观点深刻启发了萨顿，使他认识到“奖励”在学习中的独特价值。这一认识直接引领他走向强化学习研究。1978年，萨顿带着心理学背景来到马萨诸塞大学阿姆赫斯特分校攻读计算机硕士，1984年在巴托的指导下取得博士学位。他的博士论文开创性地提出了**演员-评论家（actor-critic）体系架构**，解决了强化学习中的时间信用分配问题，即智能体如何将奖励归因于先前的某些行为。

相遇于阿姆赫斯特：合作的起点
--------------

正是在麻省大学阿姆赫斯特分校，这两条学术轨迹实现了交汇。1977年，巴托作为博士后研究员加入该校计算机系，并留校任教，创建了自适应网络实验室（后发展为自主学习实验室）。不久之后，年轻的研究生萨顿慕名而来，拜师于巴托门下。导师与弟子在学术理念上的默契，为他们的合作奠定了基础。两人都痴迷于 **“类脑”的学习算法**——让计算机像神经元网络那样，通过环境反馈来调整自身。

1979年底，巴托和萨顿共同参与了一个由 A.Harry Klopf 提出的大胆研究项目，探索所谓 **“异静态自适应系统”的理论**。这个项目旨在复兴一个几乎被遗忘的假设：用**类神经元自适应单元**构成的网络，或许能成为人工智能的一条可行路径。在那个专家系统和符号AI占主导的时代，大多数研究者更热衷于预编程知识和规则，对这种让机器**自己学会行动**的想法兴趣寥寥。然而，巴托和萨顿在批判性地消化 Klopf 丰富思想的过程中，敏锐地发现了一个被忽视已久的关键点：_智能体可以通过最大化来自环境的奖励信号来驱动学习_。换言之，机器不应只是被动接受人类教导，它应当有某种内部“动机”去自主尝试、犯错、纠正，从而不断改进。这听起来像心理学中的行为主义实验——仿佛计算机也能像老鼠在迷宫中那样，为了糖块四处摸索。正是对这一朴素想法的坚定信念，使他们走上了与众不同的研究道路。

通过查阅早期文献，他们了解到强化学习的思想其实早在人工智能黎明期就有所萌芽：20世纪50年代，Arthur Samuel 的跳棋程序已经展示了机器自我对弈、自我改进的雏形；60-70年代，Klopf 等人在动物学习和**享乐型人工智能**上的探索也提供了灵感。但是，这些零星的尝试当时并未形成体系，大多研究者很快转向了模式识别、监督学习或直接放弃了让机器自学的念头。巴托和萨顿意识到，这正意味着一个机会：**去系统地发展那个被搁置的基本问题——机器如何通过试错获得奖励**。他们预感到，抓住这个核心概念并深入研究，或许能开启人工智能全新的分支。

奠基80年代：强化学习理论的崛起
----------------

踏上这条少有人走的道路后，巴托和萨顿很快用成果证明了他们选择的价值。1983年，他们与同事共同发表了一项突破性成果：一个由**神经网络**驱动的自学习控制系统学会了平衡一根倒立的细杆（即经典的“倒立摆”问题）。在这项实验中，计算机控制着小车来回移动，以保持竖直杆不倒下。起初系统一无所知，凭借不断的尝试和失败获得反馈信号：杆倒下时给予“惩罚”（成本信号），未倒则持续“奖励”。令人惊讶的是，经过多轮训练，这个仿生的**强化学习控制器**逐渐掌握了平衡技巧。要知道，在此之前，控制论领域通常依赖人类专家推导方程来解此类问题，而巴托与萨顿的方法却让机器**自己学会了如何控制**。这一成果在学界引起轰动——人们亲眼目睹了“试错学习”不再只是心理学概念，而是可以让机器解决实际难题。对于两位研究者来说，这无疑是巨大的鼓舞：它印证了他们对于强化学习潜力的信念。

随着实践突破的同时，他们也在积极构建强化学习的理论框架。1984年，萨顿完成博士论文，正式提出了**演员-评论家（Actor-Critic）架构**。这是一种受心理学启发的强化学习算法设计，将学习智能体拆分为两个彼此协作的部分：“演员”负责选择动作，“评论家”则根据动作后的反馈更新对行为的评价。当时许多研究者还在苦恼如何有效地在长期回报和即时行为间分配信用，而演员-评论家模型提供了优雅的解决思路。它借鉴了大脑中分工明确的机制（类似于多巴胺系统的奖赏评估），为算法设计注入了新的灵感。可以说，演员-评论家框架成为此后众多强化学习算法的蓝本之一。

**时间差分学习（Temporal-Difference Learning, TD）** 的发明，则是萨顿在 1980 年代中期做出的另一项里程碑式贡献。1988年，他发表论文详细阐述了TD学习方法。这种方法巧妙地融合了动态规划和蒙特卡洛方法的优点，允许智能体 _边学边预测_：通过当前对未来的估计来调整现有估计，而无需等到最终结果揭晓。通俗地说，机器在每一步都根据 **“下一刻的预期”** 来微调对“当前状况价值”的评估。这一想法在当时极具前瞻性——它赋予算法一种更灵敏的“嗅觉”，使其能够更快、更平滑地学习长期回报。当代著名计算机科学家盖瑞·泰萨罗（Gerald Tesauro）深受该思想启发，在90年代初开发了**TD-Gammon**，一个学下西洋双陆棋的程序。TD-Gammon 利用时间差分算法自我对局，不断提升棋艺，最终达到人类顶尖选手水平。这是强化学习在游戏领域的首次重大胜利，也是在现实任务中验证萨顿理论的一大佐证。

与此同时，巴托和萨顿还致力于扩展强化学习的适用范围。他们认识到，许多复杂环境下，直接让智能体学状态价值还不够，还需要直接优化行为的选择机制。因此，他们与同事们发展出**策略梯度方法（Policy Gradient Methods）**，这套方法直接以最大化预期累积回报为目标，沿着性能提升的方向调整策略参数。相比于价值函数法的间接性，策略梯度提供了一条**直接优化策略**的途径，尤其适用于动作空间连续、策略需平滑调整的场景。例如，在机器人控制中，策略梯度算法可以让机器人渐进式地改良动作模式，如同人在练习中不断调整姿势一般。这些方法后来成为深度强化学习的重要基石，被广泛应用于各类复杂决策任务。可以说，在20世纪80年代到90年代之交，巴托和萨顿带领的研究脉络奠定了强化学习的核心理论框架：从值函数逼近到策略优化，一整套概念和算法日趋成型。

“介绍”强化学习：理论圣经的诞生
----------------

学术理论的成熟离不开知识的系统传播。1990年代中期，强化学习领域已涌现出众多成果，但缺乏一本综合性的著作来梳理概念、统一术语。巴托和萨顿决定携手撰写这样一本书，为新人和业界搭建一座理解强化学习的桥梁。经过数年的打磨，**《强化学习：导论》** 于1998年由MIT出版社正式出版。这本书就像圣经一般迅速在AI圈传播开来。书中以平实易懂的语言和丰富的实例阐明了强化学习的各个关键组成：马克夫决策过程（MDP）、值函数、策略、探索-利用权衡、时间差分、Q学习、策略梯度等等。更重要的是，它将先前分散于各子领域（控制论、认知科学、计算机科学等）的想法融会贯通，确立了统一的强化学习研究范式。许多后来成为该领域中坚的人才，都是以此书为入门起点踏入强化学习世界的。可以说，这本由巴托和萨顿合著的经典教科书，不仅记录了他们多年的思考精华，更塑造了整个学科的发展走向。

该书出版后不久，巴托和萨顿在学界已被公认为现代强化学习研究的奠基者。他们所在的麻省大学自适应学习实验室也培养出一批杰出的学生和同事。其中就包括日后大名鼎鼎的研究者——例如 **David Silver**（萨顿在阿尔伯塔大学时期的学生之一），他后来成为 DeepMind 公司强化学习团队的领军人物。师徒相继，学术火炬在传递：Silver 等人站在巴托和萨顿打下的理论基础上，将强化学习应用于更宏大的舞台。

从实验室到世界舞台：强化学习改变AI
------------------

进入21世纪，计算能力的飞跃和神经网络的复兴为强化学习插上了腾飞的翅膀。巴托和萨顿在上世纪提出的思想，迎来了在现实中大放异彩的机会。2010年代中期，强化学习与**深度学习**的结合催生了“深度强化学习”革命。一时间，机器不仅能学会玩简单的游戏，还能在极其复杂的环境中击败人类高手。最引人注目的例子莫过于 **AlphaGo**。

2016年，谷歌旗下的 DeepMind 公司开发的 AlphaGo 程序，凭借深度强化学习和蒙特卡洛树搜索技术，连续战胜欧洲冠军樊麾与围棋世界冠军李世石，举世哗然。AlphaGo 的神经网络先通过人类棋谱进行初步训练，但真正让它登峰造极的，是让它与自己反复对弈，在**自我博弈中强化学习**棋艺的过程。通过成千上万次自我对战，AlphaGo 不断调整下棋策略，优化每一步落子的“价值”和“策略”判断，最终达到超越人类九段高手的水准。这正是萨顿和巴托数十年前所描述原理的现实写照：机器通过试错和奖励信号，学会了复杂的策略。AlphaGo 的胜利被誉为人工智能史上的里程碑事件——许多专家原以为围棋至少要再过十年才可能被AI攻克。当李世石无奈地在棋盘前投子认输时，站在他身后的，正是强化学习的威力。此刻，遥想1983年倒立摆前废寝忘食调试程序的那两位麻省大学研究者，他们或许会会心一笑：小小试验田里的种子，终于在世界舞台绽放出炫目的花朵。

强化学习的不凡之处不仅在于打败围棋冠军，也在于改变人工智能应用的范式。在AlphaGo之后，强化学习被迅速推广到机器人控制、自动驾驶、金融决策等领域，让机器在复杂环境中自主决策成为可能。而近几年炙手可热的大型语言模型也从中受益。OpenAI 的 **ChatGPT** 等对话式AI在训练中引入了 _人类反馈强化学习_（Reinforcement Learning from Human Feedback, RLHF）技巧，以大幅提升模型回答的有用性和安全性。在 ChatGPT 的训练过程中，研究者先用人类示范进行监督学习打好基础，然后引入强化学习机制：由人工反馈来评判模型回答的好坏，作为奖励信号来进一步微调模型。这一过程让模型“学会”了更好地顺应人类意图，说出让人满意的回答。可以说，强化学习不再只是游戏中的制胜法宝，更成为人工智能领域广泛适用的训练范式之一——从让机器臂学会拿捏物体，到让聊天机器人变得更懂人心，背后都有萨顿和巴托当年奠定的原理在发挥作用。

协作的力量与永恒的影响
-----------

回顾巴托和萨顿数十年的学术旅程，他们的故事本身就像一部经典叙事：两位志趣相投的探索者，在无人涉足的学术荒野中并肩前行，经历质疑亦收获成功，最终开辟出一片新天地。他们的合作关系始于师生，却远不止于此——更是伙伴、共创者、知音。在20世纪80年代默默无闻地打磨理论时，在90年代编写教科书传播新知时，以及在此后不同的机构继续深耕研究时，他们始终保持思想上的共振。事实证明，正是这种长期的合作与互补，使他们得以将强化学习从点滴想法发展为体系完整的学科。

学术界和业界也在用最高的荣誉向他们致敬。2017年，IJCAI授予巴托卓越研究奖，称赞他在强化学习理论和应用上的开创性贡献。更大的惊喜在几年后到来：**2024年度的图灵奖**——计算机领域的诺贝尔奖——由巴托和萨顿共同获得。这个消息令人工智能圈为之振奋。颁奖词高度评价了他们在强化学习领域的奠基性工作，以及这些工作对当今AI技术（包括游戏AI和大型预训练模型等)的深远影响。据《纽约时报》报道，评委会特别提到，如今连 ChatGPT 这样的先进AI系统，都能看到两位先驱早年研究的影子。对于巴托和萨顿来说，从1979年实验室里的那次夜谈到登上图灵奖的领奖台，这段旅程恍若跨越一个时代。而对于整个科技界，他们所开创的强化学习范式，已经并将继续深刻地影响着人工智能的发展方向。

巴托和萨顿的故事没有画上真正的句点。即使如今两位学者已是桃李满天下，他们依然活跃在学术前沿：萨顿在加拿大艾伯塔大学培养新一代AI人才，并与 DeepMind 合作推进研究；巴托虽已从麻省大学退休，却依旧以荣誉教授之名参与学术活动，为后辈指点迷津。他们的思想通过学生、合作者，透过论文和著作，像接力赛般延续下去。

正如强化学习中“小步递推”终获远大发展那样，巴托和萨顿的学术旅程告诉我们：**前沿科学的开拓常源于最朴素的疑问，不忘初心，方得始终。** 从密歇根的数学课堂到阿姆赫斯特的实验室，从平衡倒立摆的小尝试到挑战围棋大师的惊天一局，他们一路开创、引领，将一颗小小的想法种子培育成参天大树。这棵大树正为整个人工智能领域源源不断地输送养分，而这对传奇搭档的故事，也将激励后人继续探索智能之路，书写新的篇章。

附录
--

2024年度计算机协会（ACM）图灵奖获得者介绍：

### 安德鲁·巴托（Andrew Barto）

![Image 2](https://baoyu.io/uploads/2025-03-05/1741214577676.png)

安德鲁·巴托是美国麻省大学阿默斯特分校信息与计算机科学系的荣誉退休教授。他的学术生涯始于1977年，当时他在麻省大学阿默斯特分校从事博士后研究。此后，巴托陆续担任了副教授、教授，以及系主任等职位。

巴托的本科毕业于密歇根大学数学系，并获得优异成绩。之后，他又在同一所大学完成了计算机与通信科学领域的硕士与博士学位。

在学术成就方面，巴托曾获得多项荣誉，包括：

*   麻省大学神经科学终身成就奖（UMass Neurosciences Lifetime Achievement Award）

*   国际人工智能联合会议研究卓越奖（IJCAI Award for Research Excellence）

*   IEEE神经网络协会先锋奖（IEEE Neural Network Society Pioneer Award）

此外，他还是美国电气与电子工程师学会（IEEE）会士，以及美国科学促进会（AAAS）会士。

* * *

### 理查德·萨顿（Richard Sutton）

![Image 3](https://baoyu.io/uploads/2025-03-05/1741214602606.png)

理查德·萨顿现任加拿大阿尔伯塔大学计算机科学教授，同时担任总部位于美国得克萨斯州达拉斯市的人工通用智能公司Keen Technologies的研究科学家。他还是加拿大阿尔伯塔机器智能研究院（Amii）的首席科学顾问。萨顿曾于2017至2023年间担任DeepMind的杰出研究科学家。

早年，萨顿在美国新泽西州AT&T香农实验室（AT&T Shannon Laboratory）的人工智能部门担任首席技术研究员（1998至2002年）。他和巴托的长期合作始于1978年，当时萨顿在麻省大学阿默斯特分校攻读博士学位及进行博士后研究，巴托是他的导师。

萨顿拥有斯坦福大学心理学学士学位，以及麻省大学阿默斯特分校计算机与信息科学硕士和博士学位。

他的主要荣誉包括：

*   国际人工智能联合会议研究卓越奖（IJCAI Research Excellence Award）

*   加拿大人工智能协会终身成就奖（Lifetime Achievement Award from the Canadian AI Association）

*   麻省大学阿默斯特分校杰出研究成就奖（Outstanding Achievement in Research Award）

萨顿还是伦敦皇家学会会士（Royal Society of London），国际人工智能促进协会（AAAI）会士，以及加拿大皇家学会（Royal Society of Canada）会士。

**参考资料：**

1.   萨顿的生平与贡献 ([Biography S. Sutton - HandWiki](https://handwiki.org/wiki/Biography:Richard_S._Sutton#:~:text=Sutton%20received%20his%20B,3)) ([Biography S. Sutton - HandWiki](https://handwiki.org/wiki/Biography:Richard_S._Sutton#:~:text=He%20was%20influenced%20by%20Harry,4))

2.   巴托的学术背景 ([Andrew Barto - Wikipedia](https://en.wikipedia.org/wiki/Andrew_Barto#:~:text=Barto%20received%20his%20B,2))

3.   萨顿和巴托在1979年参与的强化学习项目介绍 ([ınez Terry Sejnowski David Silver Gerry Tesauro Georgios Theocharous and Phil | Course Hero](https://www.coursehero.com/file/p45i00dj/%C4%B1nez-Terry-Sejnowski-David-Silver-Gerry-Tesauro-Georgios-Theocharous-and-Phil/#:~:text=Preface%20to%20the%20First%20Edition,to%20be%20a%20promising%20approach))

4.   1983年巴托、萨顿等在倒立摆控制上的突破 ([Abstract for jervis_tr115](http://mi.eng.cam.ac.uk/reports/abstracts/robotics/jervis_tr115.html#:~:text=In%201983%2C%20Barto%2C%20Sutton%20and,pendulum%20learning%20in%20real%20time))

5.   萨顿在1980年代提出时间差分学习和演员-评论家架构 ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=TD,game%20of%20backgammon%20at%20the)) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=1.%20,1007%2FBF00115009))

6.   萨顿对策略梯度方法的贡献 ([Biography S. Sutton - HandWiki](https://handwiki.org/wiki/Biography:Richard_S._Sutton#:~:text=a%20professor%20of%20computing%20science,12%20and%20policy%20gradient%20methods))

7.   《强化学习：导论》教材及其影响 ([Andrew Barto - Wikipedia](https://en.wikipedia.org/wiki/Andrew_Barto#:~:text=During%20this%20time%20at%20UMass%2C,3))

8.   AlphaGo 使用强化学习实现自我提高，并取得里程碑式胜利 ([AlphaGo - Wikipedia](https://en.wikipedia.org/wiki/AlphaGo#:~:text=layers%2C%20trained%20by%20reinforcement%20learning.,64)) ([AlphaGo - Wikipedia](https://en.wikipedia.org/wiki/AlphaGo#:~:text=AlphaGo%27s%20March%202016%20victory%20was,69))

9.   ChatGPT 应用人类反馈强化学习进行模型优化 ([AssemblyAI](https://www.assemblyai.com/blog/how-chatgpt-actually-works/#:~:text=The%20creators%20have%20used%20a,harmful%2C%20untruthful%2C%20and%2For%20biased%20outputs)) 10.巴托和萨顿因强化学习贡献获得图灵奖等荣誉 [https://awards.acm.org/about/2024-turing](https://awards.acm.org/about/2024-turing)
