# Redis 之父 Salvatore Sanfilippo 的年终 AI 反思

## 文档信息
- 来源：https://baoyu.io/blog/redis-creator-salvatore-sanfilippos-year-end-ai-reflections
- 发布日期：2025-12-20
- 作者：宝玉

## 摘要

**1) 一句话总结**
Redis 创始人 Salvatore Sanfilippo 在 2025 年终反思中指出，大语言模型（LLM）的理解与推理能力已被证实，强化学习与思维链正打破算力瓶颈，且现有的 Transformer 架构仍有潜力通向 AGI。

**2) 核心要点**
*   **“随机鹦鹉”论已被推翻**：LLM 在各类专业考试中表现优异，且逆向工程证实模型内部已形成对概念的表征，而非单纯的词语概率拼凑。
*   **思维链（CoT）是重大突破**：CoT 允许模型在回答前采样内部表征（调动上下文），结合强化学习，能引导模型逐步收敛至正确答案。
*   **强化学习打破数据瓶颈**：在代码优化、数学证明等具备“可验证奖励”的任务中，模型无需人类标注即可自我评判和提升，产生了几乎无限的训练信号。
*   **程序员群体态度反转**：因 AI 节省的时间已远超修正错误的成本，怀疑派大幅减少；目前使用方式主要分为“对话式协作助手”与“自主编码智能体”两派。
*   **Transformer 架构潜力仍存**：LLM 本质上是在可微分空间内近似离散推理，可能无需根本性的新范式即可实现 AGI。
*   **模型本质未变**：思维链的加入并未改变 LLM 的底层逻辑，其架构仍是 Transformer，训练目标依然是预测下一个 token。
*   **ARC 测试结果逆转**：原本旨在证明 LLM 缺乏抽象推理能力的 ARC 测试已被攻克（如 OpenAI o3 在 ARC-AGI-1 准确率达 75.7%），反证了 LLM 的推理能力。

**3) 风险与不足**
*   **生存风险**：未来 20 年 AI 发展的根本挑战是避免（人类）灭绝，确保足够强大的 AI 不会引发严重问题。
*   **认知不足**：尽管模型能力在加速提升，但人类目前对这些 AI 系统的内部机制和理解还远远不够。

## 正文
Redis 之父 Salvatore Sanfilippo 最近发了一篇年终 AI 反思，一共 8 条观点。

![Image 1](https://baoyu.io/uploads/2025-12-20/Gemini_Generated_Image_v8tc1zv8tc1zv8tc.png)

先说个背景：Salvatore 不是 AI 圈的人，他是程序员圈的传奇。2009 年创造了 Redis，这个数据库如今是全球最流行的缓存系统之一。2020 年他从 Redis 退休，去做自己的事。2024 年底回归 Redis，同时成了 AI 工具的深度用户，Claude 是他的编码伙伴。

这种身份很有意思——他既是技术大牛，又是 AI 的普通使用者，视角比纯 AI 研究者更接地气。

一、随机鹦鹉的说法，终于没人信了

2021 年，Google 的研究员 Timnit Gebru 等人发了篇论文，给大语言模型起了个外号叫随机鹦鹉。意思是这些模型只是在概率性地拼凑文字，既不理解问题是什么意思，也不知道自己在说什么。

这个比喻很形象，传播很广。但 Salvatore 说，到 2025 年，几乎没人再这么说了。

为什么？因为证据太多了。LLM 在律师资格考试、医学考试、数学竞赛上的表现超过了绝大多数人类。更关键的是，研究者通过逆向工程这些模型，发现里面确实形成了对概念的内部表征，不是简单的词语拼贴。

Geoffrey Hinton 的说法最直接：要准确预测下一个词，你必须理解这个句子。理解不是预测的替代品，而是做好预测的必要条件。

当然，LLM 是不是真的理解，哲学上还可以争论。但实用层面，这个争论已经结束了。

二、思维链是个被低估的突破

思维链，就是让模型在回答之前先把思考过程写出来。看起来简单，背后的机制却很深。

Salvatore 认为它做了两件事：

第一，它让模型在回答前先采样自己的内部表征。说人话就是，先把和问题相关的概念、信息调动到上下文里，再基于这些信息回答。这有点像人考试前先在草稿纸上列提纲。

第二，结合强化学习，模型学会了如何一步步把思考推向正确答案。每一个 token 的输出都会改变模型的状态，强化学习帮它找到那条能收敛到好答案的路径。

这不是什么神秘的东西，但效果惊人。

三、算力扩张的瓶颈被打破了

以前 AI 圈有个共识：模型能力的提升取决于训练数据量，而人类产出的文本是有限的，所以扩张迟早会撞墙。

但现在有了可验证奖励的强化学习，情况变了。

什么是可验证奖励？就是有些任务，比如优化程序速度、证明数学定理，模型可以自己判断结果好不好。程序跑得更快就是更好，证明对了就是对了，不需要人来标注。

这意味着模型可以在这类任务上持续自我提升，产生几乎无限的训练信号。Salvatore 认为，这将是 AI 下一个大突破的方向。

还记得 AlphaGo 第 37 手吗？那步棋当时没人看懂，后来证明是神之一手。Salvatore 觉得，LLM 在某些领域也可能走出这样的路径。

四、程序员的态度转变了

一年前，程序员圈子还分成两派：一派觉得 AI 辅助编程是神器，一派觉得是玩具。现在，怀疑派大规模倒戈了。

原因很简单：投入产出比过了临界点。模型确实会犯错，但它节省的时间已经远超你修正错误的成本。

有趣的是，程序员使用 AI 的方式分成了两派：一派把 LLM 当"同事"，主要通过网页界面对话式地用。Salvatore 自己就是这派，用 Gemini、Claude 这些的网页版，像跟一个懂行的人聊天一样协作。

另一派把 LLM 当"独立自主的编码智能体"，让它自己去写代码、跑测试、修 bug，人类主要负责审核。

这两种用法背后是不同的哲学：你是把 AI 当助手，还是当执行者？

五、Transformer 可能就是那条路

一些知名 AI 科学家开始探索 Transformer 之外的架构，成立公司研究显式符号表征或世界模型。

Salvatore 对此持开放但谨慎的态度。他认为 LLM 本质上是在一个可微分的空间里近似离散推理，不是不可能在没有根本性新范式的情况下就达到 AGI。而且，AGI 可能通过多种完全不同的架构独立实现。

换句话说，条条大路通罗马。Transformer 可能不是唯一的路，但也不一定是死路。

六、思维链没有改变 LLM 的本质

有人改口了。以前说 LLM 是随机鹦鹉，现在承认 LLM 有能力了，但又说思维链从根本上改变了 LLM 的本质，所以以前的批评仍然对。

Salvatore 直接说：他们在撒谎。

架构没变，还是 Transformer。训练目标没变，还是预测下一个 token。CoT 也是一个 token 一个 token 生成的，跟生成别的内容没有本质区别。你不能因为模型变强了就说它"变成了另一个东西"，来给自己的错误判断找台阶下。

这话说得挺不客气，但逻辑上确实站得住。科学判断应该基于机制，不能因为结果变了就改定义。

还有一个案例很能说明问题：ARC 测试。

七、ARC 测试从反 LLM 变成了亲 LLM

ARC 是 François Chollet 在 2019 年设计的测试，专门用来衡量抽象推理能力。它的设计初衷就是抗记忆、抗暴力穷举，只能靠真正的推理来解。

当时很多人认为，LLM 永远过不了这个测试。因为它需要的是从极少样本中归纳规则、应用到新情况的能力，这恰恰是随机鹦鹉做不到的。

结果呢？2024 年底，OpenAI 的 o3 在 ARC-AGI-1 上达到了 75.7% 的准确率。2025 年，即使是更难的 ARC-AGI-2，顶尖模型也能达到 50% 以上。

这个逆转挺讽刺的。当初设计这个测试，就是为了证明 LLM 不行。结果它反而成了证明 LLM 可以的证据。

八、未来 20 年的根本挑战

最后一条只有一句话：未来 20 年 AI 的根本挑战是避免灭绝。

没有展开，就这么一句。但你知道他在说什么。当 AI 真的变得足够强大，"怎么确保它不会出大问题"就不再是科幻话题了。

Salvatore 不是 AI 的狂热信徒，也不是怀疑论者。他是一个既懂技术又在实际用 AI 的人。不是纯学术的视角，也不是纯商业的吹捧，而是一个资深工程师的冷静观察。

他的核心判断是：LLM 比很多人愿意承认的要强大得多，强化学习正在打开新的可能性，而我们对这些系统的理解还远远不够。

这大概就是 2025 年 AI 发展的真实状态：能力在加速，争议在减少，但不确定性仍然巨大。

Reflections on AI at the end of 2025 [https://antirez.com/news/157](https://antirez.com/news/157)

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/Agent]]
- [[00-元语/evals]]
- [[00-元语/软件工程]]
- [[00-元语/risk]]
