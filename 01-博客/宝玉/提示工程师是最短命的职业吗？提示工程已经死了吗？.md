# 提示工程师是最短命的职业吗？提示工程已经死了吗？

## 文档信息
- 来源：https://baoyu.io/blog/prompt-engineering/is-prompt-engineering-dead
- 发布日期：2024-09-20
- 作者：宝玉

## 摘要

**1) 一句话总结**
尽管新一代AI模型（如o1）的强大推理能力使部分提示词“技巧”失效，但提示工程的核心——清晰表达“指令”与精确控制AI完成复杂任务——依然面临诸多挑战，因此提示工程并未消亡，仍将在长期内发挥重要价值。

**2) 关键要点**
*   **背景起因**：OpenAI o1 preview 模型能自动生成高质量思维链（Chain of Thought），使得简单提示词也能获得良好效果，引发了“提示工程已死”的讨论。
*   **提示工程的两大核心**：提示工程主要由“技巧（与模型相关的优化手段）”和“指令（意图表达与任务控制）”两部分组成。
*   **“技巧”会随模型升级而淘汰**：如设定角色（GPT-3时代有效，GPT-4后不明显）、情感勒索（重新对齐后效果打折）以及手动要求“一步步思考”等技巧，确实会随着模型能力的增强而失效。
*   **“指令”是提示工程真正的核心**：让AI真正理解人类意图并精确执行任务，这部分工作并未随模型升级而消失。
*   **意图与上下文管理**：提示工程师需要解决如何清晰表达意图，以及如何向AI提供适量且准确的上下文环境。
*   **复杂任务拆解**：合格的提示工程师需要将复杂任务拆分为简单任务，甚至组建多AI智能体协同的工作流。
*   **精确控制手段**：为应对自然语言的模糊性，提示工程常引入伪代码等方式来精确控制AI执行任务，并设计出错后的纠正与容错方案。
*   **领域专家不可替代**：在特定专业领域（如高质量翻译的“翻译-反思-综合”工作流），目前仍需领域专家介入才能设计出最佳提示词，AI尚无法完全自动生成。

**3) 风险/缺口**
*   **模型随机性与语言模糊性风险**：自然语言存在模糊性，且生成模型本质是概率预测模型，导致每次输出结果可能不同，给精确控制AI带来巨大挑战。
*   **安全与合规风险**：用户可能绕过限制生成不良内容，从而导致成本损失或引发政治风险。
*   **专业领域自动化缺口**：AI目前在特定专业领域尚无法自动生成超越人类专家的开创性解决方案或最佳提示词。

## 关联主题
- [[00-元语/AI]]
- [[00-元语/prompt]]
- [[00-元语/llm]]
- [[00-元语/Agent]]
- [[00-元语/workflow]]
- [[00-元语/career]]
- [[00-元语/security]]
- [[00-元语/compliance]]

## 正文
随着 OpenAI 新的推理模型 o1 preview 的发布，它能自动生成高质量思维链，很简单的提示词也可以得到很不错的效果，所以很多人觉得提示工程已经死了，提示工程师是世上最短命的职业之一。真的如此吗？

我们常说的提示工程，有两大核心部分：技巧和指令。

技巧就是那些与模型本身密切相关，各种让特定模型表现更好的技巧，比如说：

*   为模型设定角色 “你是一个有帮助的助手”，“你是一个资深前端工程师”等等。在 GPT-3 时代这个技巧很好用，GPT-4 后效果已经不太明显了。这可能是因为早期训练语料不够多模型不够强的时候，设定角色可以让角色相关的语料在生成时有更高的概率。随着模型能力的增强，已经不太需要这样的技巧。

*   情感勒索 “你只要按照我的做，我就给你$200 小费”，“我没有手指，没办法复制粘贴，请你完整生成”，“你不这样做就要个无辜的小孩要因此丧生”…… AI 在对齐阶段，被训练成为要人类有帮助的助手，结果反而被利用。但是这类常用的情感勒索方式，在新一轮模型训练的时候，会作为案例再次对齐，效果就会大打折扣。另外很多时候这样做也是因为模型不能很好遵循指令，模型能力增强后就不需要了。

*   思维链，让模型一步步思考 这算是提示工程的核心技巧了，将任务分成几步来做可以明显提升生成的效果，最著名的是“Let's think step by step”，对于给大语言模型的任务，生成若干推理步骤，得到更好的生成效果。到了 o1 更是将这种思维链发挥到了极致，你输入一个复杂的数学题，甚至不需要要求一步步思考，它都能生成高质量的思维链，解除复杂的数学题目。以至于很多人感慨提示工程已死。

当然还有很多像 few-shot、ReAct 这样的技巧，就不一一列举。如果是技巧部分，那真的每次新一代模型出来，就要喊一次提示工程工程已死了，因为技巧那部分随着模型升级一直在变的。

指令就是那些技巧之外的，你需要让 AI 能真正理解你意图，并且精确控制 AI 按照你的意图去完成任务的部分。其实这部分才是提示工程的核心部分，而且并不容易做好，因为它有很多难点：

*   如何清楚的表达自己的意图

表达清楚自己的意图其实很难，如果不能表达清楚，不管是 AI 还是人类，都很难懂你或者帮到你。比如说“五彩斑斓的黑”是什么颜色？

*   如何让 AI 明白所有相关的上下文

人和人沟通的时候，一个常见的错误就是一方假定对方明白自己知道的所有上下文，然后造成很多误解。跟 AI 也一样，但是如何让 AI 明白我们所处的上下文环境也是很有必要并且很难的事情：要如何交代清楚上下文，要交代多少上下文？

*   如何将复杂的任务拆分成简单的任务

我刚大学毕业那会，HR 会给员工推荐一本书，叫《把信送给加西亚》，本来挺好的故事，但是被老板们用来教育员工：员工收到老板的指令，就应该像书中的安德鲁·罗文那样，没有任何推诿，不讲任何条件，历尽艰险，徒步走过危机四伏的国家，以其绝对的忠诚、责任感和创造奇迹的主动性完成“不可能的任务”，把信交给了加西亚。后来自己去管人了才知道，好的管理者要善于帮助员工将复杂的任务拆分成简单的任务，并且在过程中提供帮助和引导，而不是给一个指令就等着结果。

让 AI 做事也是类似的，由于上下文的不完整，或者任务的复杂性，合格的提示工程师需要将复杂的任务拆分成几个简单的任务让 AI 去完成，甚至于需要组建一个完整的工作流，让多个 AI 智能体协同完成复杂的任务。

*   如何精确的控制 AI 做事

提示词是用自然语言来书写的，但自然语言有一个特点就是其模糊性，同样一句话可以有不同的解读；另一方面由于现在的生成模型是概率预测模型，所以每次结果可能会不一样，这就给精确控制 AI 做事带来了很大挑战。以至于现在提示工程都有一个途径就是使用伪代码来精确控制 AI 执行任务，并且效果很好，因为代码本质就是一种精确操纵机器的语言。即使现在 o1 这样强大的推理模型出现，模型的随机性还是没能解决，还是需要提示工程师去反复尝试才能找到一个相对稳定的方案，以及在出错后的纠正和容错方案。

*   如何防止用户绕过限制做一些不好的事情

作为一个普通用户，能让 AI 帮我们完成任务就够了，但对于专业的提示工程来说，还需要防止用户做一些不好的事情，生成不好的内容，这可能造成很多成本上的损失，可能有政治风险。

*   如何针对特定任务提出开创性的创造解决方案

现在 o1 能帮助解决数学问题，这很强，但我们需要 AI 解决的不仅仅是数学问题，还有很多日常任务或者特定领域的任务，也许未来 AI 能在各个领域写出超过普通人的思维链，但这些任务可能需要真正对这个领域有深入理解和洞察的人才能写出最佳提示词。比如你让 o1 翻译一段文本，它也只能正常翻译，但公认的翻译效果更好的提示词，是要 AI 先直接按照字面意思翻译，再针对翻译的结果去检查、解释、反思，最后再综合一起生成翻译结果，而这样的提示词目前 AI 还不能自动生成，当然也许很快在翻译领域就可以做到了，不过对于一些专业领域，短时间内恐怕还是需要和领域的专家一起，才能生成最佳的提示词。

这有点像工业自动化领域，最初的自动化，就是用机器把操作最好的工人的工作的动作模仿下来实现自动化，然后再超越最优秀的工人。也许 AI 将来也能超过各个领域的专家，但那时候就真的是 AGI 时代了。

[](https://baoyu.io/blog/prompt-engineering/is-prompt-engineering-dead#%E6%9C%80%E5%90%8E)最后
--------------------------------------------------------------------------------------------

AI 时代，总是在搞大新闻，一会是 AI 要替代程序员了，一会是提示词工程师是最有潜力的职业，一会是提示词工程师是最短命的职业。然而真正去透过现象看本质，里面有太多的以偏概全，太多噱头。就提示工程这事来说，会像编程一样，还会在很长一段时间存在并发挥巨大的价值。

真正的提示工程，本质还是怎么让 AI 懂你，怎么让 AI 听话。在让别人懂我们和让别人听话这事上，我们已经奋斗了几千年了，至今还在努力中，也许 AI 会容易一点吧。
