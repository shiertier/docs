# 一些“小模型”的使用案例

## 文档信息
- 来源：https://baoyu.io/blog/small-model-use-cases
- 发布日期：2025-01-24
- 作者：宝玉

## 摘要

**1) 一句话总结**
本文汇总了参数量在0.5B至3B的“小模型”在文本分类、办公辅助、自动化及娱乐等领域的实际应用案例，突出了其在隐私保护、低成本和本地离线部署方面的核心优势。

**2) 关键要点**
* **模型界定**：常见的“小模型”通常指参数量约0.5B～3B的语言模型（如Gemma 2 2B、Llama 3.x 3B等），或更小的场景专用模型。
* **文本分类与提取**：常用于批量处理结构化/半结构化数据，例如在Excel中通过公式批量筛选数千篇医学文献、识别高优先级求助短信，以及结合OCR将图片文本解析为JSON。
* **办公与生产力辅助**：可用于自动生成Git提交信息候选、批量重命名文件、生成单行代码脚本（如Bash/Python），微软甚至曾研究用仅60M参数的模型（FLAME）进行Excel公式修复。
* **数据脱敏与隐私保护**：在处理机密信息（如代码变量、密码）时，可先用本地小模型生成占位符，再发送给外部大模型，避免敏感数据外泄。
* **网页集成与自动化**：支持轻量级自动化任务，如识别并过滤网站Cookie同意横幅、自动跳过视频赞助片段，或通过WebGPU/WebAssembly直接在浏览器内离线运行。
* **游戏与娱乐应用**：在Godot等游戏引擎中集成2B～7B模型为NPC生成动态对话以提升沉浸感，或用于实时生成故事、辅助迭代个人音乐歌单。
* **核心驱动力**：本地运行小模型的主要优势在于数据不出本地（隐私安全）、不依赖云GPU（成本与速度可控），且在单一任务上微调后性能可媲美大模型。

**3) 风险与不足**
* **反作用风险**：使用小模型自动回复垃圾短信可能会向发送方确认该手机号处于活跃状态，或被运营商判定为有交互意愿，从而引来更多垃圾信息。
* **逻辑与推理薄弱**：小模型在深层次推理、复杂逻辑和数学精确度（如严格逻辑、日期时间推算）上表现较弱，容易出错。
* **上下文窗口受限**：部分小模型上下文长度有限，难以直接处理长文档，必须依赖文本分块或RAG（检索增强生成）技术。
* **多语言能力差异**：许多小模型主要基于英文数据训练，对中文及其他语言的处理性能有限，通常需要自行微调或寻找特定的多语版本。
* **生态共享难点**：社区在共享微调后的模型或LoRA时，仍面临模型发现性差、可复用性低以及相关文档不足的问题。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/self-hosting]]
- [[00-元语/security]]
- [[00-元语/productivity]]
- [[00-元语/cli]]
- [[00-元语/rag]]
- [[00-元语/lora]]
- [[00-元语/ocr]]
- [[00-元语/browser-automation]]
- [[00-元语/wasm]]
- [[00-元语/game]]
- [[00-元语/tool]]

## 正文
在 Hacknews 上有一个讨论很火，就是大家都用小参数的语言模型做什么，有没有什么好的使用案例。

我把这些案例整理汇总了一下，大约有六类：**（1）文本分类与信息提取**、**（2）办公与生产力辅助**、**（3）对话/消息处理与辅助回复**、**（4）网页/应用集成与自动化**、**（5）娱乐、创作与游戏**、**（6）模型部署、技术瓶颈与思考**。

注：常见的“小模型”通常指参数量约0.5B～3B（如Gemma 2 2B、Llama 2/3.x 3B、Qwen 2.5 1.5B等），或者更小/更简单的场景专用模型。

* * *

1. 文本分类与信息提取
------------

1.   **医学文献筛选（Excel Add-In）**

    *   有人（比如楼中提到的 Girlfriend Excel Add-In 项目）使用了 Gemma 2 2B 模型在 Excel 里给几千篇论文的标题和摘要做二元分类。例如判断“如果该论文研究糖尿病神经病变和中风，则标记为‘Include’，否则标记为‘Exclude’”。

    *   用户只需在 Excel 中写一行公式（如 `=PROMPT(A1:B1, "…")`），再往下拖拽几千行即可批量处理，无需手动逐篇审阅。

2.   **医院母婴求助短信识别（政府热线）**

    *   有人对一个 2B 级小模型做了微调（fine-tune），用来筛选新生儿或孕妈的紧急求助短信，并将其标记成高优先级。这在资源受限、需要隐私的场景下很有帮助。

3.   **Hacker News 或论坛内容的自动汇总与分组**

    *   有人做了 GopherSignal 等服务，利用小模型给 Hacker News 的帖子生成简短概述或总结，帮助读者快速了解帖子内容，也有进一步的整合功能（比如过滤、排序等）。

4.   **工作/招聘信息的抓取与分类**

    *   在“Who is hiring”贴子中，用户抓取所有评论，通过小模型提取关键词（例如地点、是否远程、编程语言等），再生成 RSS feed 或其他格式做内部筛选。

5.   **OCR + 结构化解析**

    *   有人结合小模型与 Tesseract（或其他 OCR）来把图片里的文本转成结构化 JSON。例如读取食品营养表、企业票据或论文资料等。这些较小模型只要 prompt 设置好，性能即可满足特定识别需求。

* * *

2. 办公与生产力辅助
-----------

1.   **自动生成或优化 Git 提交信息**

    *   有人用一个小模型读取当前 `git diff` 的内容，然后生成几条候选的 commit message，让开发者挑选并编辑。这在简单场景下可加快写提交说明的速度，但也需要注意“为何修改”的背景，仍要人工补充。

2.   **Excel/表格公式自动完成**

    *   微软曾有论文（FLAME）研究用仅 60M 参数的模型做 Excel 公式修复与智能推荐，在特定细分场景上对比大模型能有出色表现。也有人自己在 Excel 中做简易的 prompt-based 函数处理文本数据。

3.   **用于过滤、审核或重命名**

    *   如有人用小模型来批量重命名 Linux ISO，或者自动归纳各类文件名、提取数据并重新组织后再人工审核。

    *   也有人使用小模型生成产品文案（如营销广告语、H1 标题）或自动翻译（利用更小的本地模型来回输出多种语言、特别是短文本翻译）。

4.   **本地日志/摘要生成**

    *   处理会议纪要、短信记录、或个人待办事项（RAG，Retrieval-Augmented Generation）等，需要本地隐私的场景，小模型速度和资源占用更易控制。

5.   **代码辅助/小脚本生成**

    *   有人将小模型当作命令行小助手，生成 Bash 或 Python 的单行脚本（如 ffmpeg/awk/sed/find 参数），速度够快且不用把内部机密数据发到外部 API。

* * *

3. 对话/消息处理与辅助回复
---------------

1.   **自动回复垃圾短信**

    *   有人用 Ollama + 小模型，对垃圾短信自动回复“伪装成特别感兴趣”，甚至给对方设计不同人设（健身狂、19 世纪英国绅士等），意在“消耗”对方精力。

    *   也有不同观点提醒这可能会“告诉对方号码还活跃”，引来更多垃圾短信，或让运营商认定你对信息“有交互意愿”。

2.   **对话匿名化/去敏处理**

    *   将含有机密信息的文本（例如代码变量名、密码等）先用本地小模型生成占位符再发给大模型，最终再把占位符映射回真实信息。这样可避免直接泄露敏感内容给外部服务。

3.   **实时会议或广播的分析**

    *   有人想用小模型来检测逻辑谬误（logical fallacies），或给对话中的发言“打分”、评论“谁赢谁输”，或者是把大段对话总结成要点。

4.   **聊天机器人本地部署**

    *   例如使用 Raspberry Pi 5 或 PC 小主机给自己做“离线助手”，可有限制地回答提问、执行语音指令等。在资源紧张或敏感数据不想外泄的环境下，小模型成为不错的方案。

* * *

4. 网页/应用集成与自动化
--------------

1.   **Cookie 横幅检测与广告屏蔽**

    *   有人用 Llama 3B 检测网站上的“Cookie 同意横幅”，自动生成过滤规则（例如给 EasyList Cookie）。因为大部分弹窗结构相似，识别文本后做黑名单过滤。若遇到不常见语言或带年龄限制弹窗，会有点不准。

2.   **浏览器扩展与前端 WebGPU/WebAssembly**

    *   有人在尝试把小模型直接跑在浏览器内，甚至用 WebGPU 优化推理速度，做到完全离线的网页端 AI 处理，比如自动纠正文案或翻译等。

3.   **自动检测或跳过赞助内容**

    *   例如有人结合小模型识别 YouTube 视频中的“赞助片段”并自动跳过，减少对人工志愿者的依赖。

4.   **封装成后端服务**

    *   有人专门搭建一个微服务（C++实现），优化了 llama.cpp，在局域网或本地提供更低延迟的推理接口，用于各种实时应用。

* * *

5. 娱乐、创作与游戏
-----------

1.   **故事/小说/对话生成**

    *   有人让 3B 模型实时生成无限故事（如科幻、奇幻等）在小屏幕上滚动，供自己随时“看小说”。或把它当文字冒险脚本、刷段子等。

2.   **游戏 NPC 对话**

    *   在 Godot 游戏引擎中集成 2B～7B 的模型，为 NPC 动态生成对话，提升沉浸感；也用于简单交互（例如与商店老板砍价），比起千篇一律的对话更灵活。

3.   **自动生成音乐/播放列表**

    *   有人让模型根据个人风格和已有歌曲，生成一串可能喜欢的新曲目；再把人不喜欢的去掉、再补充，以迭代形成个人歌单。模型不一定超精准，但能带来一些新灵感。

4.   **娱乐性聊天/角色扮演**

    *   有人让小模型当作 SMS 逗乐器，或针对自己做“语音播报”时带一定人设，诸如星战风格的对话等。

    *   还有人尝试做一个真人主播，给对话或直播“加料”，随机评判发言者、制造看点。

* * *

6. 模型部署、技术瓶颈与思考
---------------

1.   **准确率和评估**

    *   小模型（例如 2B、3B）做二元分类、简单摘要和指令改写往往够用，但若要“更深层次”推理（如复杂逻辑、数学精确度）比较薄弱。

    *   社区常见建议：对任务做“少量人工验证”或“交叉验证”，必要时选更大或更针对性微调的模型。

2.   **小模型的优点**

    *   **隐私**：可离线部署，敏感数据留在本地，不必调用外部 API。

    *   **速度/成本可控**：不依赖云 GPU，尤其在 CPU 性能较好的本地电脑上也能接受速度。

    *   **专用场景适配**：在小数据集或单一任务上微调，往往能媲美大模型通用能力。

3.   **局限与注意点**

    *   **上下文窗口有限**：部分小模型上下文很短，对于长文档处理有难度，需要分块/分段或 RAG。

    *   **不擅长深层逻辑**：对严格逻辑/日期时间推算等，常会出错。

    *   **中文/多语言能力差异**：许多小模型在英语数据上训练得更多，处理其他语言的性能有限，需要自行微调或找多语版本。

4.   **更多人想法**

    *   可以尝试把“小模型”与检索系统（RAG）或专门的垂直工具结合，形成“多agent”或“多工具”流程，取长补短。

    *   也有人呼吁共享微调后的模型或 LoRA，但发现发现性、可复用性和文档不足仍是难点。

* * *

社区对“小模型”（在数亿到几亿或低至几千万参数量级）非常感兴趣，尤其是**在特定领域的分类、提取、翻译、重命名**等场景，表现足以满足需求又便于离线/本地运行。

**隐私和成本**是最大的驱动力：本地运行可避免数据外泄，也能降低（或避免）云 API 费用。

此外在**游戏、聊天娱乐、文本变体生成**等个人或小型项目中，小模型也能充分发挥创意。但若需**更强的推理、可靠回答、强多语言适配**，往往还需大模型或者更精心微调。

整体而言，“小而专”的思路越来越受欢迎：与其求一个通用大模型，不如在本地用小模型+领域数据完成高性价比的AI应用。

相关讨论链接：[https://news.ycombinator.com/item?id=42784365](https://news.ycombinator.com/item?id=42784365)
