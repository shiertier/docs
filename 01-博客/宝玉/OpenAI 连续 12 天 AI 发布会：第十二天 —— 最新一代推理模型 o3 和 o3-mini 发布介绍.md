# OpenAI 连续 12 天 AI 发布会：第十二天 —— 最新一代推理模型 o3 和 o3-mini 发布介绍

## 文档信息
- 来源：https://baoyu.io/blog/openai-day12-o3-o3-mini-model-release
- 发布日期：2024-12-20
- 作者：宝玉

## 摘要

**1) 一句话总结**
OpenAI 发布了新一代高性能推理模型 o3 及高性价比的 o3-mini，在数学、编程和 ARC-AGI 等基准测试中取得突破性成绩，目前正开放外部安全测试申请，预计于 2025 年 1 月底陆续推出。

**2) 关键点**
*   **模型定位**：发布 o3（主打极致推理性能）和 o3-mini（在保持高智能的同时大幅优化性能、延迟与成本）两款新模型。
*   **o3 编程与数学突破**：o3 在 SWE-bench Verified 准确率达 71.7%（比 o1 提升 20%），Codeforces 评分达 2727 ELO，AIME 数学竞赛准确率达 96.7%（o1 为 83.3%）。
*   **o3 科学与逻辑突破**：在 GPQA Diamond（博士级科学问题）得分 87.7%；在极难的 Epoch AI 前沿数学测试中准确率超 25%（当前行业基准低于 2%）。
*   **ARC-AGI 里程碑**：o3 在 ARC-AGI 测试中最高得分达到 87.5%，首次突破了 85% 的人类表现基准线。
*   **o3-mini 核心优势**：支持“低、中、高”三种推理努力级别；中等推理时间下的性能即可超越 o1，且成本极低；“低”级别延迟与 GPT-4o 相当。
*   **开发者功能**：o3-mini 全面支持函数调用（Function Calling）、结构化输出（Structured Outputs）和开发者信息等 API 功能。
*   **安全技术创新**：推出“审慎对齐”（Prudent Alignment）技术，利用模型自身的推理能力来判断安全边界，有效识别用户的隐藏意图或越狱尝试，同时改善了拒绝和过度拒绝指标。
*   **发布时间表**：目前仅开放公共安全测试（申请截止至 2025 年 1 月 10 日）；预计 1 月底正式推出 o3-mini，随后推出 o3 完整版。

**3) 风险/不足**
*   **可用性限制**：新模型目前暂未对公众开放，必须先完成内部及外部的安全测试以防范越狱等风险。
*   **评估基准失效风险**：由于 o3 在多项现有基准测试中已趋于满分，行业目前面临缺乏更具挑战性、更持久的基准测试来准确评估前沿模型的缺口。
*   **长尾问题挑战**：在处理 GPQA 等极其复杂的数据集时，极难的长尾问题仍然是模型评估和表现上的难点。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/evals]]
- [[00-元语/llmops]]
- [[00-元语/security]]
- [[00-元语/risk]]
- [[00-元语/roadmap]]
- [[00-元语/数学]]

## 正文
[https://www.bilibili.com/video/BV1hYkDYrEDx](https://www.bilibili.com/video/BV1hYkDYrEDx)

OpenAI 连续 12 天 AI 发布会：第十二天 —— 最新一代推理模型 o3 和 o3-mini 发布介绍

本视频是 OpenAI 12 天活动的最后一期,主要介绍了新一代推理模型 o3 和 o3-mini。主讲人包括 Sam Altman、Mark Chen、Hongyu Ren 以及特邀嘉宾 ARC Prize Foundation 主席 Greg Kamradt。

主要亮点

1.   新模型发布

*   发布两个新模型 和 o3-mini

*   o3 是高性能推理模型,o3-mini 则在保持智能的同时优化了性能和成本

*   目前仅开放用于公共安全测试,预计一月底推出 o3-mini,随后推出 o3

1.   o3 模型性能突破

*   在软件测试基准 SWE-bench Verified 上准确率达 71.7%,比 o1 提升 20%

*   在 CodeForce 竞赛编程上达到 2727 ELO 分数

*   AIME 数学竞赛准确率达 96.7%(o1 为 83.3%)

*   在博士级科学问题基准 GPQA Diamond 上达到 87.7%

*   在 ARC-AGI 测试上首次突破人类水平阈值(85%),达到 87.5%

1.   o3-mini 特点与优势

*   支持三种推理努力级别:低、中、高

*   性能方面:中等推理时间下性能超过 o1

*   成本效益:以极小成本实现比 o1 更好的表现

*   支持功能调用、结构化输出等开发者功能

*   在 GPQA Diamond 上达到 62% 的分数

1.   安全策略创新

*   推出"审慎对齐"(Prudent Alignment)新技术

*   利用模型推理能力提升安全边界判断

*   显著改善了拒绝基准和过度拒绝指标

*   开放外部安全测试申请(截止至 1 月 10 日)

重要时间节点

*   安全测试申请截止:2024 年 1 月 10 日

*   o3-mini 预计发布:2024 年 1 月底

*   o3 完整版:将在 o3-mini 之后推出

* * *

Sam Altman: 早上好。今天的消息非常令人兴奋。12天前，我们启动了这个为期12天的活动，并发布了我们的第一个推理模型——o1。看到人们如何使用它非常令人振奋，听到大家的喜爱也让我们感到欣慰。我们认为这是AI发展的新阶段的开始，你可以用这些模型来完成需要大量推理的复杂任务。

所以在活动的最后一天，我们决定从一个前沿模型切换到下一个。今天，我们将讨论下一个前沿模型，按逻辑应该是O2，但出于对Telefonica朋友的尊重，加上我们在命名方面一贯创意独特，所以就叫它o3。实际上，我们今天会宣布两个模型：o3和o3-mini。

o3是一个非常智能的模型，而o3-mini在智能的同时在性能和成本上表现卓越。先说坏消息，今天我们不会公开发布这些模型。好消息是，我们会让它们用于公共安全测试。

敬请关注，今天开始，你可以申请测试，我们稍后会谈到这一点。随着我们的模型能力不断增强，我们严肃对待安全测试，并在这种新能力水平上，我们希望尝试在安全测试过程中新增一个部分，就是允许研究人员公开访问，帮助我们进行测试。

我们稍后会讨论这些模型何时会普遍可用。我们很高兴向您展示它们的功能并讨论它们的表现。我们准备了一个小惊喜，给您展示一些演示。事不宜迟，我把话题交给Mark。

Mark Chen: 很好。非常感谢你，Sam。我是Mark。我是OpenAI研究方面的负责人。我想稍微谈谈o3的能力。目前，o3在非常复杂的技术基准测试中表现强劲。

我想从编码基准测试说起，请展示一下。在软件风格的基准测试中，我们有SWE-bench Verified，这是由实际软件任务组成的基准。我们发现o3的准确率约为71.7%，比我们的o1模型高出20%以上。这表明我们在实用性方面也在不断突破。

在竞争性代码领域，o1模型在一个名为CodeForce的竞赛编程网站上达到了大约1891的ELO分数。在我们最激进的高测试时间计算设置下，我们能够实现接近2727的ELO分数。

Sam Altman: Mark是一位竞赛编程高手现在还在指导竞赛编程。非常优秀。你的成绩是多少？

Mark Chen: 我想我在类似的比赛中最好成绩大约是2500。

Sam Altman: 那很不容易。我要说，你知道吗，我们的首席科学家Yaakov的成绩也没有这个高。

Mark Chen: 我想OpenAI有个家伙ELO分数还在3000多。是啊，我们还有几个月的时间可以享受这一成果。

Sam Altman: 希望我们还有几个月的时间来享受这些成果。太好了。

Mark Chen: 这个模型在编程上很出色。不仅是编程，还有数学。我们看到在竞赛数学基准上，就像竞技编程一样，我们取得了非常好的分数。o3在AIME上的准确率是96.7%，而o1的表现是83.3%。你AIME的最好成绩是多少？

Sam Altman: 我曾得到过满分。

Mark Chen: 好的。我很安全。恭喜。实际上这表明o3经常只在测试中错一题这个非常难的美国数学奥林匹克选拔考试中。

还有一个非常难的基准测试，叫做GPQA Diamond，用来衡量模型在博士级科学问题上的表现。我们在这里取得了领先成绩，达到87.7%，比我们o1的表现78%提高了约10%。如果一个博士专家测试他们的强项领域，通常得分是70%左右。

Mark Chen: 你可能注意到,在这些基准测试中,我们在很多方面已经趋于满分。因此,去年确实强调了需要更具挑战性的基准测试来准确评估前沿模型。我认为在过去的几个月里,有几个显得颇具前景。特别想提到一个是Epoch AI的前沿数学测验。

你可以看到这些分数比我们之前展示的基准低很多。因为这是目前公认最难的数学测验。这个数据集包含新颖、未发表且非常难的问题。这些问题极其困难。

Sam Altman: 是的,非常非常难的问题。专业数学家需要几个小时甚至几天才能解决。

Mark Chen: 如今,所有应对此基准的准确率都不到2%。我们看到在o3的积极测试环境中,我们能够超过25%。

Sam Altman: 是的。太棒了。除了Epoch AI的前沿数学测试基准,我们还有一个惊喜给大家。所以我想在这个时候谈谈Arc测试基准,但我想邀请我们的一位朋友Greg,他是Arc Foundation的总裁,来谈谈这个基准。

Greg Kamradt: 太好了。Sam和Mark,非常感谢你们今天的邀请。当然。大家好。我是Greg Kamradt,Arc Prize Foundation的总裁。ArcPrize是一个非营利组织,旨在成为通用人工智能的指引,通过持久的基准。

我们的第一个基准,ARC-AGI,是由Francois Chollet在2019年他关于智力测量的论文中开发的。然而,它已经未被打败有五年了。在AI世界里,这感觉像是好几个世纪。因此,打败ARC-AGI的系统将是重要的。通向通用智能的重要里程碑。

但我很高兴地宣布今天我们有了一个新的最先进的分数。在我讲到这之前我想谈谈ARC-AGI是什么。我想给你展示一个例子。ARC-AGI是关于有输入和输出示例。这些示例都不错。不错?

Sam Altman: 好的。

Greg Kamradt: 输入示例和输出示例。现在目标是理解转换规则并猜测输出。所以,Sam,你觉得这里在做什么?

Sam Altman: 可能是在空白处放一个深蓝色方块。

Greg Kamradt: 对,正是这样。人类容易直观地猜到这个,但AI理解起来很难。我想展示另一个更难的例子。Mark,这次考考你。你认为这个任务中在做什么?

Mark Chen: 好,你数下每个黄色方块的颜色数量,然后创建一个边框。

Greg Kamradt: 没错,而且你比大多数人反应快,恭喜。不过有趣的是,AI还不能解决这个问题,尽管我们验证过一些人类可以做到。

Greg Kamradt: ARC-AGI的独特之处在于每个任务需要不同的技能。我的意思是,我们不会要求你去填补蓝色方块的空隙。我们这样做是有意的。因为我们想测试模型快速学习新技能的能力。我们不希望它只是重复已经记住的内容。这是我们的关键目标。

现在,ARC-AGI版本1用5年时间才从0%达到5%,与前沿的领先模型相比。然而今天,我非常兴奋地宣布o3取得了我们已经验证的新顶尖分数。在低计算的情况下,o3在ARC-AGI的半私有保留集上获得了75.7分。因为这完全符合我们公共排行榜的计算要求,成为了ARC-AGI Pub上的新第一名。向取得这一成就的团队表示祝贺。非常感谢。

现在,作为能力的展示,当我们让o3进行更长时间的思考并提高计算要求时,o3在相同的隐藏保留集中取得了85.7%的得分。这尤其重要。是的,87.5%。这尤其重要,因为人类表现的可比较阈值为85%,因此超过这一点是一个重要的里程碑。我们从未测试过一个系统做到这一点。或任何模型在这之前做到这一点。所以这是ARC-AGI领域的新探索。

祝贺你们,制作了如此出色的基准测试。看到这些分数时,我意识到我需要调整我的世界观,需要重新理解AI实际上能做什么及其潜力,特别是在o3时代。但工作还未完成,这仍然是AI的初期阶段。我们需要更多持久的基准,比如ARC-AGI,以帮助衡量和指导进展。我期待加速这一进展,也期待明年与OpenAI合作开发我们的下一个前沿基准。

Sam Altman: 太棒了。这也是我们一直关注的基准,这个想法早就在我们的脑海里,所以期待与您合作。在未来。

Mark Chen: 值得一提的是,我们并没有刻意追求这个目标,而是认为它是一个很棒的基准。我们并没有做什么具体的工作。这只是o3的一个总体概览。但非常感谢这次的合作。

Greg Kamradt: 是的,这真的是一个有趣的项目。当然,尽管这个项目已经取得了很好的成绩,ArcPrize仍将在2025年继续进行,[任何人都可以在arcprize.org](http://xn--arcprize-te1m1nxh13bu15bb4k6w2t.org/)找到更多信息。太好了,非常感谢你邀请我。

Sam Altman: 当然。好的,接下来我们将讨论o3-mini。o3-mini是我们感到非常激动的一个项目,负责训练这个模型的Hongyu也会加入我们。

Hongyu Ren: 嘿。很高兴见到你。嘿。大家好。我是Hongyu Ren。我是一名从事推理研究的OpenAI研究员。今年九月我们发布了o1-mini,这是o1家族中一个高效的推理模型,特别擅长数学编码。鉴于其低成本,可能是世界上最好的之一。

现在,与o3一起,我很高兴向你们介绍o3-mini,这是o3家族中一个全新的模型,引领了一个具有成本效益的推理新前沿。

Sam Altman: 真是令人难以置信。

Hongyu Ren: 是的,不过今天我们的用户无法使用,我们正在让安全与安保研究人员测试这个模型。随着几天前API中引入了自适应思考时间,针对o3-mini模型,我们将支持三种不同的选项:低、中和高推理努力级别。用户可以自由调整思考时间以适应不同的使用场景。例如,在处理复杂问题时我们可能希望模型有更长的思考时间,而对简单问题则缩短时间。

在此基础上,我很高兴展示所有o3-mini的第一组评估。在左侧展示的编码评估中,我们对比了代码与ELO评分,ELO评分越高说明程序员水平越高。正如我们在图中所见,随着思考时间的增加,o3-mini版取得了更高的收益,并超越了o1-mini版。在中等思考时间下,其性能甚至优于o1。

Sam Altman: 是的,就像在速度和成本上我们以一个数量级的优势,提供了同等代码性能甚至更佳的体验。尽管超迷你版高性能还差几百点就达标,但已经不远了。它的表现可能比我自己还好,这真的是一个令人难以置信的性价比提升。

Mark Chen: 是的,相较于我们通过o1提供的性能,我认为大家会非常喜爱这个。我也希望如此。

Hongyu Ren: 在右侧的图表中,我们展示了估算的成本与Codeforces ELO的权衡。显然,o3-mini定义了一个新的高效成本的编码推理前沿,它以极小的成本实现了比o1更好的表现。

Sam Altman: 令人惊叹。

Hongyu Ren: 话虽如此,我想做一个现场演示。

Sam Altman: 听起来不错。

Hongyu Ren: 在o3-mini上。希望你能测试模型的三种不同推理时间:低、中、高。那么让我粘贴提示词。我首先测试o3-mini(高)。任务是我要求模型使用Python实现一个代码生成器和执行器。运行这个Python脚本时,它将启动一个服务器。并且本地有一个包含文本框的UI。然后我们可以在文本框中发出编码请求。它会发送请求调用o3-mini API,o3-mini API将完成任务并返回代码,然后将代码保存在我桌面上,然后打开终端自动执行代码。这是一个非常复杂的任务,对吧?它输出大量代码。如果我们复制代码并粘贴到我们的服务器上,然后我们想要运行,启动这个服务器。

Sam Altman: 所以当你启动它时,我们应该看到一个文本框。

Hongyu Ren: 是的。

Sam Altman: 好的,很好。

Hongyu Ren: 是的,看来是这样。我希望如此。它似乎正在启动某个东西。很好,我们有一个用户界面,可以输入一些编码提示词。让我们尝试一个简单的,比如训练、OpenAI和随机数。提交。它正在将请求发送到o3-mini(中),应该很快,对吧?当前数值是...41。

Sam Altman: 对,41。那是菜单编号,对吗?

Hongyu Ren: 它将生成的代码保存到桌面上的本地脚本,然后打印出"openai 41"。你们还有其他任务想要尝试测试吗?

Mark Chen: 我想知道是否可以让它获取自己的GPQA编号。

Sam Altman: 这是个好问题,正如我预期的那样。

Hongyu Ren: 是的。现在让我复制代码。然后在代码用户界面中发送。在这个任务中,我们要求模型评估o3-mini在这个复杂的GPQA数据集上的低推理能力。模型需要首先从这个URL下载原始文件,然后需要识别问题、答案和选项部分。然后汇总所有问题,要求模型回答并解析结果进行评分。实际上速度非常快。

Mark Chen: 是的,它真的很快,因为调用了推理努力较低的o3-mini。

Sam Altman: 嗯,是的,让我们看看结果如何。

Mark Chen: 这两个任务确实很难。

Sam Altman: 是的,长尾问题常常是个问题。

Mark Chen: 继续。继续。是的。GPQA是一个很难的数据集。

Sam Altman: 是的。是的。内容可能像是六个简单问题中的一个和两个非常困难的问题。[笑声]

Mark Chen: 在我们等待的时候,你想再次展示一下请求吗?

Hongyu Ren: 嗯哼。哦,它已经返回结果了,是61.62%。这是一个"推理努力"为"低"的o3-mini模型,速度相当快。在一分钟内完成评估。

Sam Altman: 从某种程度上来说,让一个模型来进行自我评估非常酷。

Mark Chen: 是的,完全正确,对吧?

Hongyu Ren: 如果我们要总结一下刚才的操作,就是我们让模型编写脚本来评估自己在这个困难的GPQA数据集上,从这个由模型本身创建的代码生成器和执行器的UI开始。

Sam Altman: 明年我们会请你上台,你将需要要求模型改进它。

Hongyu Ren: 好的,下次我们一定要让模型改进它。也许不会。[笑声]

除了代码力量和GPQA,模型也是一个相当不错的数学模型。我们在这个AMI 2024数据集的图中展示,o3-mini low与o1-mini实现了可比的性能,而o3-mini medium在看实线部分时,比o1-mini性能更佳。如果我们查看实线部分(即谜题)的话。我们还可以用o3-mini high进一步提升性能。

在右侧的图中,当我们测量匿名化的o1预览流量的延迟时,我们展示了o3-mini low大幅减少了o1-mini的延迟,对吧?几乎达到了GPT-4o的同等延迟。在一秒钟内,所以可能是即时响应。而o3-mini medium的延迟是o1的一半。

这里还有我更想展示给大家的另一组评估结果,是API功能。我们收到很多开发者社区的请求,希望支持功能调用、结构化输出、开发者信息,所有迷你系列模型。在这里,o3-mini将支持所有这些功能,与o1一致。值得注意的是,它在所有或大多数评估中实现了更佳的性能,为我们的开发者提供了更具成本效益的解决方案。

Mark Chen: 如果我们真正揭示几天前我执行的GPQA Diamond表现，它实际上也拿到62%的分数，对吧？基本上就是让模型自我评估。

Sam Altman: 是的。下次你最好让模型自动进行评估，而不是...

Hongyu Ren: 好，这就是有关o3-mini的所有内容，希望我们的用户明年能有更好的使用体验。

Sam Altman: 太棒了。嗯，谢谢。酷。我知道你们迫不及待想亲自使用它。我们正在努力完善模型，以便在模型之上进行一些安全干预，目前我们正在做大量的内部安全测试。这次我们新增了一些内容，就是今天开始从o3-mini开放进行外部安全测试，未来也会应用到o3。

那么作为安全研究人员，怎样才能获得早期访问权限呢？你可以访问我们的网站，看到一个这样的表单，就像你在屏幕上看到的。这个表单的申请正在进行中，将在1月10日截止，我们非常欢迎你申请。我们很期待看到你们能用这个探索到哪些内容，发现什么样的越狱和其他东西。

Mark Chen: 好的。另一个让我兴奋的话题是我们发布的一份新报告，应该是昨天或今天发布的，这推进了我们的安全计划。这是一种名为审慎对齐的新技术。

通常在我们的模型上进行安全训练时，我们试图找出什么是安全和不安全的决策边界，对吧？这通常只是通过展示例子，纯粹的例子来表明这是一个安全的提示词，这是一个不安全的提示词。但我们现在可以利用模型的推理能力，在这里找到更准确的安全边界。

这项技术名为审慎对齐，让我们能够使用一个安全规范，让模型对我们的提示词进行推理，并判断这个提示词是否安全。在推理过程中，它常常会发现，嘿，这个用户试图欺骗我，或者他们表达了某种隐藏的意图。所以即使你试图去破译你的提示词，推理常常会破解它。

Mark Chen: 你在这里看到的主要结果如图所示。我们在x轴上有一个拒绝基准，在y轴上有过度拒绝。在这里，右侧的数值表现更优。这代表着我们准确判断何时应该拒绝某些东西的能力，同时判断何时应该拒绝的能力。通常你认为这两个指标之间存在某种权衡。

Sam Altman: 这真的很难做到。

Mark Chen: 是的，确实很难做到。

Sam Altman: 是的。

Mark Chen: 但看起来，通过深思熟虑的对齐，我们可以在右上角获得这两个绿色点，而之前的模型——红色和蓝色点，代表了我们过去模型的性能表现。因此，我们真正开始利用推理来增强安全性。

Sam Altman: 是的，我认为这是在安全性上的一个优秀成果。

Mark Chen: 是的。

Sam Altman: 是的。真是太好了。好的。总结一下，o3-mini和o3，如果你愿意，请申请参与我们的安全测试，帮助我们。作为额外的步骤测试这些模型。我们计划在一月底推出o3-mini，很快之后推出o3。

但如你所知，越多的人能帮助我们进行安全测试，我们就越能够确保实现这一目标，所以请查看一下，感谢你一直以来的关注。这对我们来说很有乐趣。希望你们也喜欢。

所有人: 圣诞快乐！
