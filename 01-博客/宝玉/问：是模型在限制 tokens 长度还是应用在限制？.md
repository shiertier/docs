# 问：是模型在限制 tokens 长度还是应用在限制？

## 文档信息
- 来源：https://baoyu.io/blog/model-token-limit-program-or-model
- 发布日期：2025-01-13
- 作者：宝玉

## 摘要

**1) 一句话总结**
Token长度限制是由模型本身的上下文窗口大小和AI应用程序的输入限制共同决定的。

**2) 关键要点**
* Token数量限制受模型自身和调用程序的双重影响。
* 模型本身具有最大上下文窗口长度限制，该限制是输入和输出Token长度的总和。
* AI聊天应用程序通常会设定更严格的用户输入长度限制。
* 应用程序限制输入（例如128K上下文的模型可能仅允许16K输入）的首要原因是必须为模型的输出预留足够的Token空间。
* 应用程序限制输入长度也是为了平衡模型生成质量与运行成本。

**3) 风险/缺口**
* 输入内容越长，模型生成的质量会下降。
* 长输入会导致调用成本大幅增加。

## 正文
问：模型支持的TOKEN数量是模型本身的限制还是调用模型的程序限制的呢？

答：模型会有上下文窗口长度限制，AI聊天应用也会有会话长度限制。

举例来说你的模型最大上下文窗口长度限制是 128K，但是通常应用程序不会让你输入的内容到128K，可能输入内容最多16K就不让你输入了，因为这个上下文窗口长度是针对输入和输出加起来的长度，所以要留一些空间给输出。

另外输入内容越长，模型生成的质量会下降，成本也会增加很多，所以应用要限制最大输入的长度。

## 关联主题
- [[00-元语/llm]]
- [[00-元语/context-optimization]]
- [[00-元语/prompt]]
- [[00-元语/llmops]]
