---
title: "从 Shopify 构建 Agent 的经验中可以学到的"

来源: "https://baoyu.io/blog/shopify-agent-building-experience"
发布日期: "2025-09-16"
作者: "宝玉"
---

## 摘要

### 1) 一句话总结
本文总结了 Shopify 构建 AI 智能体（Agent）的实践经验，核心涵盖 Agentic Loop 架构、工具数量控制策略（如引入子智能体）以及构建与人类对齐的 LLM 自动评估体系。

### 2) 关键要点
*   **基础架构**：采用主流的 Agentic Loop，通过循环让大模型判断工具调用、执行工具，并根据结果决定是继续调用还是完成任务。
*   **四条核心建议**：架构简单化且工具边界清晰；采用模块化设计（如即时指令）；LLM 评估必须与人类高度相关；提前应对奖励作弊并持续优化评估体系。
*   **限制工具数量**：建议将单个 Agent 调用的工具数量控制在 20 个以内，以保证工具选择的精确度。
*   **JIT（即时指令）方案**：Shopify 目前采用该方案动态生成工具调用指令，并动态修改消息历史以维护 LLM Cache。
*   **SubAgent（子智能体）方案**：被视为更成熟的长期方案，通过将同类工具分摊至子智能体，避免主 Agent 上下文过长，并赋予子智能体一定的自治能力。
*   **结果评估机制**：必须对 Agent 生成的结果进行评估，以驱动其自我改进和优化。
*   **构建自动评估基准**：先由人类专家标注真实环境抽取的样本作为基准数据集，再通过调试提示词让 LLM 的评估结果与人类保持一致，最终实现可靠的 LLM 自动化评估。

### 3) 风险与不足
*   **工具过载风险**：工具数量过多会极其影响 Agent 的能力，导致其难以精确选择合适的工具。
*   **JIT 方案复杂度高**：即时指令方案需要动态修改消息历史，过于复杂，仅为过渡性产物。
*   **奖励作弊（Reward Hacking）**：在评估体系中存在模型作弊的风险，需要提前防范。
*   **LLM 评估误判**：即使 LLM 评估结果与人类基准对齐，在实际自动化评估中仍不可避免地会存在误判情况。

## 正文

Shopify 分享了他们构建 Agent 的经验，整体架构也是目前主流的 Agentic Loop，就是不停的循环，让大模型判断需要调用什么工具，Agent 去调用工具，根据调用工具的结果看是继续调用工具还是任务完成。

![Image 1](https://baoyu.io/uploads/2025-09-16/1758024445125.png)

他们针对打造 AI 智能体给了4条核心建议

1.   架构简单化，工具要清晰有边界

2.   模块化设计（如即时指令）

3.   LLM 评估必须与人类高度相关

4.   提前应对奖励作弊，持续优化评估体系

我看下来主要是两点值得借鉴的地方：

一、工具不要太多，尽量控制在 20 个以内

![Image 2](https://baoyu.io/uploads/2025-09-16/1758024464260.png)

如果工具数量太多会极其影响 Agent 的能力，很难精确选择工具

那么解决方案是什么呢？

不要看他们分享的 JIT 方案，明显是一个过渡性的产物，需要动态的去生成调用工具的指令，为了保证不影响 LLM 的 Cache，还要动态去修改消息历史，过于复杂。

![Image 3](https://baoyu.io/uploads/2025-09-16/1758024484961.png)

真正的靠谱方案其实 PPT 里面也写了（看图3），只是它们还没实现，而实际上 Claude Code 这部分已经很成熟了，就是用 SubAgent（子智能体），通过 Sub Agent 分摊上下文，把一类工具放在一个 SubAgent 中，这样不会影响主 Agent 上下文长度，也可以让子 Agent 有一定自制能力，有点类似于一个公司大了就分部门，每个部门就是一个 SubAgent。

二、Agent 生成的结果要 Evaluate（评估）

Agent 要做得好，很重要的一点就是要能评估它生成的结果是好还是坏，这样 Agent 自己就能对自己的结果进行改进优化。

那么怎么评估 Agent 的优化结果呢？靠人太慢，靠机器太不靠谱。

![Image 4](https://baoyu.io/uploads/2025-09-16/1758024514290.png)

所以他们先找了一些人类专家，从正式环境中抽取了足够多样的结果，来人工标记是好还是坏，然后把这个结果作为基准数据集，再去写提示词让 LLM 来评估，让 LLM 评估的结果和人类的结果保持一致。当 LLM 评估结果和人类一致后，后续就可以放心的让 LLM 来评估 Agent 的生成结果，这样就不需要人工介入。

![Image 5](https://baoyu.io/uploads/2025-09-16/1758024524265.png)

至于会不会误判，我想肯定还是会的，但不管怎么说还是一个比较好的折中方案。

其他还有一些强化学习的训练方法，有兴趣可以自己去看看原文。

[https://baoyu.io/translations/building-production-ready-agentic-systems](https://baoyu.io/translations/building-production-ready-agentic-systems)

## 关联主题

- [[00-元语/Claude]]
- [[00-元语/Agent]]
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/evals]]
- [[00-元语/llmops]]
- [[00-元语/alignment]]
- [[00-元语/prompt]]
- [[00-元语/tool]]
- [[00-元语/workflow]]
- [[00-元语/context-optimization]]
