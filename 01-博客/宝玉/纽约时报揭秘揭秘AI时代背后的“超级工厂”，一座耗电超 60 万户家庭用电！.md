# 纽约时报揭秘揭秘AI时代背后的“超级工厂”，一座耗电超 60 万户家庭用电！

## 文档信息
- 来源：https://baoyu.io/blog/ai-data-centers.html
- 发布日期：2025-03-18
- 作者：宝玉

## 摘要

### 1) 一句话总结
为实现通用人工智能（AGI），科技巨头正斥资数千亿美元重塑以 GPU 为核心的新型数据中心，引发了计算架构、电力供应与冷却技术的全面变革，同时也带来了前所未有的能源与水资源消耗挑战。

### 2) 关键要点
*   **核心硬件更迭**：AI 模型的训练依赖具备强大并行计算能力的 GPU，其单块功率可达 1000 瓦，远超传统 CPU 的 250–500 瓦。
*   **巨额资本投入**：亚马逊、Meta、微软和谷歌等巨头今年的资本开支预计超 3200 亿美元（几乎是两年前的两倍）；OpenAI 及其合作伙伴计划先期投资 1000 亿美元，后续再追加 4000 亿美元建设新数据中心。
*   **物理架构重塑**：为加快芯片间的数据传输速度，新型数据中心将数以万计的 GPU 高密度集中部署，单台 GPU 服务器造价可达数万美元。
*   **用电量呈指数级增长**：美国能源部预测，到 2028 年，专门用于 AI 的数据中心用电量将高达 326 太瓦时（TWh），几乎是 2023 年的 8 倍。
*   **新型能源寻源**：为满足激增的电力需求，微软计划重启三里岛核电站，而马斯克的 xAI 则在孟菲斯直接安装燃气轮机以求快速落地。
*   **冷却技术升级**：因高密度 GPU 产热极高，谷歌等企业已将传统的通道水管冷却，升级为直接贴近芯片的液冷系统。
*   **技术路线差异**：尽管中国企业 DeepSeek 证明了使用较少芯片也能打造顶尖 AI 系统，但美国科技巨头仍坚持通过堆叠庞大算力来追求通用人工智能（AGI）。

### 3) 风险与缺口
*   **资源供需断层**：科技公司对算力和电力的需求已几乎超出全球现有供应能力，北弗吉尼亚州等传统数据中心重镇的可用电力已濒临枯竭。
*   **水资源消耗与生态压力**：液冷系统需消耗大量水资源（谷歌数据中心 2023 年耗水 61 亿加仑，同比增长 17%），已引发加州等干旱地区官员的警示。
*   **火灾与硬件损坏隐患**：高密度机架若无法及时降温，面临设备烧毁及数据中心起火风险；而贴近芯片的液冷水管若发生泄漏，可能造成重大硬件损失（目前通过添加不导电化学物质来降低风险）。
*   **社区抵触情绪**：在部分数据中心项目选址地，当地居民担心项目弊大于利，对建设产生了抵触情绪。

## 正文
当今推动人工智能（A.I.）发展的“发动机”，是一种叫做 **GPU** 的小型硅芯片（图形处理器）。它最初是为了视频游戏而设计的。

![Image 1](https://baoyu.io/uploads/2025-03-18/scale_v007-1600.gif)

现在，各大科技公司正把 GPU（因非常适合运行 A.I. 所需的大量计算）密集地装进 **专用计算机** 里。

由此诞生了一种新型“超级计算机”——在被称为 **数据中心** 的大型建筑里，部署多达 10 万块以上的芯片，通过连接在一起的方式，全力训练和运行功能强大的人工智能系统。

所有这些计算能力都伴随着巨大的能耗成本。ChatGPT 的开发者 OpenAI 希望建造大约 5 座设施，这些设施加在一起所消耗的电量，将超过马萨诸塞州约 300 万户家庭的用电量。

随着科技公司不断追逐 A.I. 的梦想，这些 **数据中心** 已在美国各地涌现……

……并且还在向 **全球** 扩散，迫使科技巨头到处寻找电力和水源，用于为数据中心供电与冷却，以防这些芯片因自身产生的热量而烧毁。

对计算机的这种根本性变革，是自万维网早期以来最重大的一次。正如 20 世纪 90 年代公司为适应新兴的互联网商业模式而彻底重塑计算机系统一样，如今从微小元器件到大型机房与能源供应的方式，也都在为适应人工智能而重新构建。

其实，大型科技公司在过去二十多年里，就已经在全世界各地建造数据中心，用来容纳处理海量网络流量的计算机，比如搜索引擎、电子邮件服务和电商网站。

但与正在到来的这一波 A.I. 数据中心相比，之前的那些显得“轻量”得多。2006 年，谷歌在俄勒冈州达尔斯（The Dalles）建立了它的[首个数据中心](https://www.nytimes.com/2006/06/14/technology/14search.html)，当时估计花费了 6 亿美元。而 2025 年 1 月，OpenAI 和几个合作伙伴[宣布计划](https://www.nytimes.com/2025/02/08/technology/sam-altman-elon-musk-trump.html) 投资大约 1000 亿美元建设新的数据中心，首批在德克萨斯州选址。随后还将陆续在美国各地投入额外 4000 亿美元兴建更多设施。

这场计算革命所影响的远不止科技本身，也包括金融、能源以及各地社区。私募股权公司正将资金大量投入数据中心企业；[电工也在蜂拥前往](https://www.nytimes.com/2024/12/25/technology/ai-data-centers-electricians.html) 这些数据中心项目所在地寻求工作机会；而在某些地方，当地居民[对于这些项目感到抵触](https://www.nytimes.com/2024/10/29/technology/data-center-peculiar-missouri.html)，担心弊大于利。

目前，科技公司需要的计算能力和电力几乎超过了世界所能供应的水平。OpenAI 希望筹集数千亿美元，在中东[建设芯片制造工厂](https://www.nytimes.com/2024/09/25/business/openai-plan-electricity.html)。谷歌和亚马逊最近达成协议，计划[建造并部署新一代核反应堆](https://www.nytimes.com/2024/10/16/business/energy-environment/amazon-google-microsoft-nuclear-energy.html)。而且他们希望一切都能尽快完成。

![Image 2: 一排装载在蓝色主板上的电脑芯片（原文插图）](https://baoyu.io/uploads/2025-03-18-bc4bbd1bf0e6dc222c894eb786876c384326fdf9.jpg)

> 谷歌的 A.I. 芯片与电路板。为构建旗下聊天机器人和其他 A.I. 技术，谷歌需要大量此类芯片。

在 2024 年 12 月，一家名叫 DeepSeek 的中国小公司声称，利用远少于业界预期数量的芯片，就打造出了世界上[最强大的 A.I. 系统之一](https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html)，[颠覆了人们](https://www.nytimes.com/2025/01/27/technology/what-is-deepseek-china-ai.html) 对硅谷疯狂投入是否物有所值的质疑。

然而，美国科技巨头们似乎并不为所动。他们的远大目标是打造“通用人工智能”（A.G.I.）——即能完成任何人类大脑可胜任任务的机器。他们依然相信，为了达到这一点，庞大的计算能力必不可少。

亚马逊、Meta、微软以及谷歌母公司 Alphabet 近期[透露](https://www.nytimes.com/2025/02/08/technology/deepseek-data-centers-ai.html)，他们今年的资本开支（主要用于建设数据中心）合计可超过 3200 亿美元，几乎是两年前的一倍多。

《纽约时报》记者前往加利福尼亚州、犹他州、德克萨斯州和俄克拉何马州的五处新建数据中心园区，并采访了 50 多位高管、工程师、企业家和电工，来呈现科技行业对这类全新计算方式的巨大渴求。

“原本可能需要十年才能完成的事，如今在两年内就压缩完成了。”谷歌首席执行官桑达尔·皮查伊（Sundar Pichai）在接受《纽约时报》采访时说，“A.I. 就像催化剂。”

* * *

为 A.I. 而生的新型计算机芯片
-----------------

推动这次计算机飞跃的核心，是一种极小的要素：名为图形处理器（GPU）的专用芯片。

像英伟达（Nvidia）这样的硅谷芯片制造商，最初为视频游戏设计了 GPU。然而，GPU 在执行神经网络所需的大规模数学运算方面非常在行。神经网络可以通过分析海量数据来“学习技能”，这也是当今聊天机器人和各种先进 A.I. 技术的基础。

### A.I. 模型是如何被“训练”的

神经网络通过分析海量数据学会区分信息，这个过程也被称为机器学习。下面这个示例展示了 A.I. 模型如何基于大量花朵图片，学习识别一朵花的图像。

![Image 3: slides_flowers.gif](https://baoyu.io/uploads/2025-03-18-slides_flowers.gif)

1.   **提供参考素材** A.I. 模型的训练需要分析大量参考数据——这是一个既耗时又需要巨大计算量的过程。

![Image 4: slides_flowers.gif](https://baoyu.io/uploads/2025-03-18-slides_flowers.gif)
2.   **映射数据** 将图片分解成像素，并按照不同标签进行分组，便于模型在后续识别时调用。

![Image 5](https://baoyu.io/uploads/2025-03-18-slides_break.gif)
3.   **对比并预测** 模型能在数百万张图片中找出共同模式，从而可以自主识别不同对象。

![Image 6](https://baoyu.io/uploads/2025-03-18-slides_match.gif)

过去，计算机主要依赖被称为中央处理器（CPU）的芯片。CPU 什么都能做，包括运行神经网络所需的简单数学计算。

但 GPU 更能干同样的工作——而且速度更快。在同一时刻，传统芯片只能做一次计算，而 GPU 却可以同时进行成千上万次。计算机科学家把这种能力称为“并行处理（parallel processing）”，这使神经网络能处理更多数据。

“它们跟用来提供网页的芯片有非常大区别。”Together AI（一家科技咨询公司）的首席执行官 Vipul Ved Prakash 说，“GPU 能在同一时间完成数百万次运算，从而让机器有‘思考’问题的能力。”

因此，科技公司开始使用越来越多的 GPU 来训练功能越来越强的 A.I.。

### CPU 与 GPU 的差异

![Image 7](https://baoyu.io/uploads/2025-03-18/chips_v4-920.gif)

**传统 CPU 计算****GPU 并行计算**
数据按顺序依次处理，前一个任务完成后才进行下一个任务。通过多处理器并行把任务切分成更小的部分，同时执行多个计算，从而大幅加快速度。

随着使用需求的提升，英伟达也在对其 GPU 进行改造，使其更适合 A.I.，在每块芯片中塞进更多晶体管，以便在每秒进行更多计算。2013 年，谷歌也开始[自行研制 A.I. 芯片](https://www.nytimes.com/2017/09/16/technology/chips-off-the-old-block-computers-are-taking-design-cues-from-human-brains.html)。

这些谷歌与英伟达的芯片，并非用于运行操作系统，也不能在像 Windows 笔记本或 iPhone 上承担各种应用功能。但它们与其他硬件协同工作时，却能极大加速 A.I. 的发展。

“过去的模式大概延续了 50 年左右。”负责谷歌 A.I. 芯片项目的工程师 Norm Jouppi 表示，“如今，我们有了完全不同的做事方式。”

* * *

芯片放得越近，速度就越快
------------

不仅仅是芯片本身发生了变化。为了让 GPU 发挥最大的性能，科技公司必须想方设法加快数据在各个芯片之间的传输速度。

“每块 GPU 都需要和其他所有 GPU 之间进行高速通信。”Cirrascale Cloud Services 的首席技术官 Dave Driggers 如此描述。这家公司在德克萨斯州奥斯汀运营一家数据中心，为知名 A.I. 研究机构艾伦人工智能研究所（Allen Institute for Artificial Intelligence）提供计算服务。

芯片彼此之间距离越近，数据传输就越快。所以各公司都在同一个数据中心里塞进尽可能多的芯片，并且研发新的硬件和线缆，来加快芯片与芯片之间的数据流。

![Image 8: 坐落在犹他州湖山（Lake Mountains）山脚下的 Meta Eagle Mountain 数据中心，远处山峰上有积雪（原文插图）](https://baoyu.io/uploads/2025-03-18-832c78a5993a50dd82c412cfcbe31b7cd0ecbd6b.jpg)

> Meta 的 Eagle Mountain 数据中心，位于盐湖城以南的一个山谷中。这栋大楼是在 A.I. 热潮爆发后破土动工的。

这也改变了数据中心的运作方式——传统上，它们就是大型厂房，里面整齐摆放着一排排的服务器机架。

2021 年，在 A.I. 热潮到来之前，Meta 曾在盐湖城以南约一个小时车程的地方新建了两座数据中心，并正在那里建设另外三座。这些设施——每栋建筑的占地面积相当于把帝国大厦横放在沙漠上——最初是为支撑该公司社交媒体应用（如 Facebook、Instagram）所需的网络服务。

但是在 2022 年 [OpenAI 推出 ChatGPT](https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html) 后，Meta 重新审视了它的 A.I. 计划。公司需要在一座全新的数据中心里塞进数千块 GPU，才能支撑训练单个神经网络可能长达数周甚至数月的计算需求，进而推进 A.I. 的研发。

“所有设备必须像一个‘数据中心级超级计算机’那样统一协作。”Meta 数据中心副总裁 Rachel Peterson 说，“这对基础设计是全新的挑战。”

几个月内，Meta 又动工兴建了第六座与第七座犹他州数据中心，规模均达 70 万平方英尺。在这些数据中心里，技术人员把专门用于训练 A.I. 的硬件（里面装满 GPU 的设备）一台台装到机架上，而这些 GPU 服务器每台价格就可能高达数万美元。

2023 年，Meta [计入了 42 亿美元的重组费用](https://www.nytimes.com/2023/02/01/technology/meta-restructuring-charge.html)，部分原因就是要调整未来许多数据中心项目的设计，以迎接 A.I. 需求。事实上，不止 Meta，一股行业大潮正在兴起。

* * *

A.I. 机器需要更多电力——多得多
------------------

把新的数据中心装满 GPU 意味着新的电力需求——而且需求量极大。

2023 年 12 月，Cirrascale 租下了奥斯汀一处占地 13.9 万平方英尺的传统数据中心。原本，这里可用约 5 兆瓦电力，足以为约 3600 户美国家庭供电。那时，里面有 80 行左右的机架，用来放置常规的 CPU 服务器。

现在，为了满足 A.I. 需要，Cirrascale 拆掉了旧设备，用 GPU 系统来替换。结果，原先给整幢建筑供电的 5 兆瓦，如今只够带动其中大约 8 到 10 行装满 GPU 的机架。

该数据中心可以向当地电网申请升级到约 50 兆瓦，但即便如此，也无法全部换成 GPU。而即使有 50 兆瓦的负荷能力，对 A.I. 数据中心来说仍算小规模。相比之下，OpenAI 的目标是建设约 5 座数据中心，总耗电量将[超过大约 300 万户美国家庭](https://www.nytimes.com/2025/02/08/technology/sam-altman-elon-musk-trump.html) 的总和。

![Image 9: 一张位于德州奥斯汀 Cirrascale 数据中心内部的照片，前景是空荡的白色房间，远处有机架设备（原文插图）](https://baoyu.io/uploads/2025-03-18-f6c77a848a78c09d8e6e0a6fc1ee67decec94913.jpg)

> Cirrascale 在德州奥斯汀的这个数据中心，最大可从电网获取 5 兆瓦电力，但这只能为 8 到 10 行 GPU 服务器供电。

这不仅因为数据中心里服务器更密集，还因为支撑 A.I. 的芯片本身需要更多电力。普通 CPU 功率大约在 250 到 500 瓦之间，而 GPU 则可以高达 1000 瓦。

建设数据中心时最终要与当地电力公司“谈判”：它能提供多少电力？电费多少？如果需要投入巨资升级电网设备，又由谁出钱？

2023 年，美国数据中心的用电量约占全国总耗电的 4.4%，超过了加密货币“挖矿”中心用电量的两倍。根据美国能源部 2023 年 12 月发布的报告，这一数字到 2028 年可能会增加三倍。

### A.I. 数据中心的用电量

能源部下属的劳伦斯伯克利国家实验室（Lawrence Berkeley National Laboratory）推算，专门用于 A.I. 的数据中心，到 2028 年的用电量可能高达 326 太瓦时（TWh），几乎是 2023 年用电量的 8 倍。

![Image 10](https://baoyu.io/uploads/2025-03-18/1742264692671.png)

数据来源：劳伦斯伯克利国家实验室，美国能源部

“时间才是现在行业里最宝贵的资源。”负责这份报告的研究员 Arman Shehabi 说，“大家都在抢着建，我短期内看不到放缓的迹象。”

在美国的部分地区，数据中心运营商正遭遇电力短缺。比如全球最大数据中心聚集地北弗吉尼亚州（这里有通往欧洲的海底电缆），可供调用的电力几乎被消耗殆尽。

一些 A.I. 巨头转而寻求核能。微软[正在重启](https://www.nytimes.com/2024/10/30/business/energy-environment/three-mile-island-nuclear-energy.html) 位于宾夕法尼亚州的三里岛（Three Mile Island）核电站。

也有公司另辟蹊径。埃隆·马斯克（Elon Musk）和他的 A.I. 创企 xAI 最近在孟菲斯自己[安装了燃气轮机](https://www.npr.org/2024/09/11/nx-s1-5088134/elon-musk-ai-xai-supercomputer-memphis-pollution)，没有追求清洁能源，只求更快落地。

“现在我的对话主题已经不是‘要到哪里搞到最先进的芯片’，而是‘哪儿能弄到足够的电力’。”A.I. 风险投资机构 Radical Ventures 的合伙人 David Katz 说。

* * *

A.I. 散热太高，只有水才能降温
-----------------

这些密度极高的 A.I. 系统还带来了另一场变革：全新的冷却方式。

A.I. 系统运转时会产生大量热量。当空气从服务器机架的前方经过，横穿计算芯片后，就会变得很热。在 Cirrascale 奥斯汀的数据中心，一台机架前侧温度约 71.2 华氏度（约 21.8 摄氏度），后方则达到了 96.9 华氏度（约 36 摄氏度）。

如果机架无法及时降温，整台机器，甚至整个数据中心都可能面临起火危险。

在俄克拉何马州东北角的普赖尔镇（Pryor）附近，谷歌大规模地解决了这个问题。

这里宽阔平坦的草地上矗立着 13 座谷歌数据中心大楼。整个园区容纳数以万计的服务器机架，电力从一座座金属铁塔和电缆输送站大量涌入，功率以百兆瓦计。为了防止机架过热，谷歌在 13 座大楼里都安装了冷却水管。

过去，谷歌的数据中心通常把水管敷设在机架之间的通道里，让冷水流经管道，吸收四周空气中的热量。然而，当机架里布满 A.I. 芯片后，水管距离芯片太远，不足以吸走多余热量。

### 传统数据中心

![Image 11](https://baoyu.io/uploads/2025-03-18/1742264713717.png)

现在，谷歌让水管直接贴近芯片。只有这样，水才能真正吸走热量，保证芯片安全运行。

### A.I. 数据中心

![Image 12](https://baoyu.io/uploads/2025-03-18/1742264729736.png)

在装满电子设备的地方跑水管有风险：一旦漏水，就可能造成重大损失。为降低风险，谷歌会给水中添加不导电的化学物质，减少芯片的损坏可能性。

水吸收完热量后，还得再冷却下来。通常做法是在数据中心屋顶加装巨大冷却塔。部分水在此蒸发，带走热量，就像人出汗后汗液蒸发带走热量一样。

“我们把它称作‘自然免费冷却’，在清晨干燥、温度较低的环境下，水会自然蒸发降温。”谷歌数据中心副总裁 Joe Kava 介绍。

![Image 13](https://baoyu.io/uploads/2025-03-18-5deea794f5bdedd1fe8e947f75f5fa07604ec99b.jpg)

> 谷歌数据中心内部，堆满了使用谷歌自研 A.I. 芯片的服务器。

但这也意味着需要源源不断地补充新水来维持循环，对当地供水造成压力。2023 年，谷歌数据中心消耗了 61 亿加仑的水，比前一年增长 17%。在经常面临干旱的加利福尼亚州，有超过 250 个数据中心每年会消耗大量水，当地官员[已对此发出警示](https://www.sacbee.com/opinion/op-ed/article297294554.html)。

一些公司，比如 Cirrascale，会采用大型冷水机（本质上是空调）来冷却循环水，从而几乎可以 100% 回收冷却水，不会大量消耗当地水资源，但这样会耗费更多电力。

目前，这一切都还只是开端。去年，谷歌在南卡罗来纳州、印第安纳州、密苏里州等地又开工兴建了 11 座新数据中心。Meta 也宣布在路易斯安那州 Richland Parish 建造新的数据中心，面积之大足以覆盖纽约中央公园、中城曼哈顿、格林威治村和下东区等区域的总和。

“这是 A.I. 决定性的一年，”Meta 首席执行官马克·扎克伯格（Mark Zuckerberg）在今年 1 月的一则 [Facebook 帖子](https://www.facebook.com/zuck/posts/pfbid0219ude255AKkmk4JAueXZeZ9zpjNYio2tBkd7bNmCaRbJ6iJaVVjypUgDg78CNdq5l) 中写道，“让我们投入建设吧！”

## 关联主题
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/llmops]]
- [[00-元语/hardware-control]]
- [[00-元语/risk]]
