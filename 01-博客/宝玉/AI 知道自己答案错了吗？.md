# AI 知道自己答案错了吗？

## 文档信息
- 来源：https://baoyu.io/blog/ai-knows-its-answers-wrong
- 发布日期：2024-11-15
- 作者：宝玉

## 摘要

**一句话总结**
大语言模型本身无法确切知道答案对错，只能基于概率输出或借助外部工具验证，但通过可验证的数学和代码问题构建明确的奖励函数，能够有效实现自我训练并提升推理能力。

**关键要点**
* 大语言模型无法直接判断答案的绝对对错，只能得出某个答案正确的大概概率，验算需借助外部工具。
* 让 AI 具备判断对错的能力是提升其模型能力的关键所在。
* 模型训练中的奖励函数至关重要，通过对模型行为进行奖励或惩罚来优化输出结果。
* AlphaGo 能够通过自我对弈提升，是因为棋局胜负易于判断，奖励函数明确，无需人工干预。
* 传统大语言模型在后训练阶段依赖人工标记数据进行微调，以评判输出结果的好坏；也可以用强模型来标记弱模型的输出。
* 新一代 o1 推理模型借鉴了 AlphaGo 的思路，利用数学和代码问题进行训练。
* 数学和编程题的结果可以通过程序直接验证，使得奖励函数易于编写，模型无需大量人工干预即可自我训练提升。

**风险/不足**
* 大语言模型存在认知局限，其本身并不能真正理解对错，仅依赖概率。
* 模型标记存在单向性限制：可以让强模型标记弱模型的输出，但弱模型无法反过来标记强模型。
* 当前推理模型的能力提升存在领域局限，主要集中在数理化和编程等结果可程序验证的领域。

## 正文
问：宝玉老师，好奇一个问题，因为学生平时考试的时候，会有一个检查的过程，这个时候可以检查出自己的某些题写错了，将错误答案改为正确的答案。 很多数学，物理，化学的考试，学生在时间充裕的情况下，都可以进行这样的改正过程。不知道目前的AI对于这个部分是怎么做的，它知道自己的答案错了吗？

答：这是个好问题，简单来说大语言模型是不能知道对错的，只能知道某个答案正确的大概概率，除非它借助外部工具，就像人做数学题也是需要去用纸笔或者计算器验算的。

就这个问题发散一下，其实让 AI 知道对错是很重要的事情，这是 AI 能力提升的关键所在。

所以训练模型的时候，奖励函数很重要，也就是对于模型什么样的行为该奖励，什么样的行为该惩罚，才能让模型结果越来越好。

当年 AlphaGo 通过自己跟自己下棋对弈提升，是因为可以判断每一步的胜率和最终输赢，所以奖励函数很明确，自己训练也可以得到很好的结果，不需要人工去标记干预。

那么对于大语言模型，在后训练阶段，是需要人工标记数据去微调，每次大模型输出结果人工判断是好还是坏。当然现在也可以让能力强的模型去标记能力弱的模型输出结果的好坏，但反过来不行。

而新的 o1 推理模型，则是利用了数学问题和代码问题来进行训练，类似于 AlphaGo，由于数学题和编程题的结果是可以通过程序验证的，所以奖励函数好写，这样就不需要太多人工干预，自己训练自己也能提升。所以目前推理模型的能力强，主要还是集中在数理化和编程上面。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/evals]]
- [[00-元语/tool]]
- [[00-元语/数学]]
