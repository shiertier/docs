# OpenAI Operator 的工作原理

## 文档信息
- 来源：https://baoyu.io/blog/openai-operator-how-it-works
- 发布日期：2025-01-25
- 作者：宝玉

## 摘要

### 1) 一句话总结
OpenAI Operator 依托结合了 GPT-4o 视觉与强化学习推理的 CUA（Computer-Using Agent），在专属虚拟机中通过“感知-推理-动作”的循环，直接处理屏幕像素并模拟键鼠操作来自动完成用户任务。

### 2) 关键要点
*   **核心驱动**：由 CUA（Computer-Using Agent）提供支持，融合了 GPT-4o 的视觉能力和通过强化学习获得的高级推理能力。
*   **运行环境**：接收请求后会启动用户专属的虚拟机（内置 Chrome 浏览器并保留 Session 和 Cookie），并将虚拟机截图实时同步至网页端供用户查看。
*   **指令生成**：CUA 综合系统提示词、用户任务、当前状态和截图，借助链式思考（CoT）生成具体的屏幕操作指令（如鼠标移动、点击、拖动）。
*   **像素级定位**：模型通过处理原始像素数据理解屏幕内容，通过计算光标在垂直或水平方向需要移动的像素距离来实现精准点击，无需依赖专门的 API。
*   **感知（Perception）**：执行循环的第一步，将屏幕截图加入模型上下文，提供当前计算机状态的视觉快照。
*   **推理（Reasoning）**：执行循环的第二步，利用链式思考推断下一步行动，跟踪中间步骤并动态调整，遇到阻碍时能够自我纠正并重新尝试。
*   **动作（Action）**：执行循环的第三步，通过虚拟鼠标和键盘在数字环境中执行点击、滚动、键入等多步骤导航操作。

### 3) 风险与不足
*   **像素计算误差风险**：准确计算像素对于执行鼠标命令至关重要，模型偶尔会出现类似“数错单词内字母数量”的错误，导致难以准确完成鼠标操作。
*   **敏感操作受限**：模型无法完全自动化所有流程，在遇到输入登录信息或处理验证码（CAPTCHA）等敏感操作时，必须中断并请求用户进行人工确认。

## 正文
为 Operator 提供支持的是 Computer-Using Agent (CUA)，它结合了 GPT-4o 的视觉能力与通过强化学习获得的高级推理能力。

![Image 1](https://baoyu.io/uploads/2025-01-25/1737778621891.png)

当用户向 Operator 网页发送请求的时候，Operator 会启动一个用户专属的虚拟主机（这也可能是为什么现在只能面向 pro 用户的原因之一），这个虚拟机上装了一个 Chrome 浏览器，并且 Session、Cookie 会一直给你保留。

虚机的截图会同步到 Operator 网页，所以网页上可以实时看到网页操作的情况。CUA 根据系统提示词+用户输入的任务+当前任务状态+截图会借助 CoT 思考生成可以执行的 Actions，由于系统提示词包含了一系列屏幕操作的指令，比如鼠标移动、点击、拖动等等，最终这些 Action 会编程屏幕操作的指令，这样 CUA 就可以操作屏幕了。

多模态是怎么精准获取坐标位置的？Anthropic 之前有一篇《Devloping a computer use model》里面有提到一些：[https://www.anthropic.com/news/developing-computer-use](https://www.anthropic.com/news/developing-computer-use)

> 我们之前在工具使用和多模态领域的研究为此次“计算机使用”能力打下了基础。让 AI 操作计算机需要能够“看到”并理解图像——在这里，即计算机屏幕的截图；同时也需要推理如何以及何时在屏幕上执行某个具体操作。通过结合这些能力，我们训练 Claude 理解屏幕上的信息，然后使用计算机软件完成相应任务。
>
>
> 当开发者给 Claude 指定要使用某款计算机软件并授予相应访问权限后，Claude 会查看用户所见的屏幕截图，然后计算光标在垂直或水平方向上需要移动多少像素，才能在正确的位置完成点击。在这个过程中，准确数像素非常关键。没有这项技能，模型就很难完成鼠标命令——和模型在回答“banana 里有几个字母 A”这类看似简单问题时偶尔会出错的情况类似。
>
>
> 令我们意外的是，Claude 在只用少量软件（比如计算器和文本编辑器）进行训练后，便能迅速地举一反三（在训练中，为了安全起见，我们没有让模型访问互联网）。结合 Claude 的其他能力，这种训练让它在接收用户书面指令后，可以将其转化为一系列逻辑步骤并在计算机上执行命令。我们还观察到，模型在遇到阻碍时会自我纠正并重新尝试。
>
>
> 虽然在取得最初的突破后，后续进展来得很快，但整个过程还是经历了大量的反复试验。有些研究人员提到，这次开发“计算机使用”模型的过程，和他们最初对 AI 研究的“理想化”想象颇为相似：需要不断迭代、反复回到设计图上，直到逐步取得进展。

CUA 通过处理原始像素数据来理解屏幕上发生的内容，并通过虚拟的鼠标和键盘来执行操作。它能够进行多步骤的任务导航、处理错误并适应意外变化。这让 CUA 能够在各种数字环境中行动，例如填写表单、浏览网站，而无需使用专门的 API。

在接收到用户指令后，CUA 通过一个融合感知、推理和动作的迭代循环来执行操作：

*   **感知（Perception）**：从计算机截取的屏幕截图会加入到模型的上下文中，为模型提供当下计算机状态的视觉快照。

*   **推理（Reasoning）**：CUA 使用链式思考（chain-of-thought）来推断下一步的行动，同时考虑当前和过去的屏幕截图及操作。这种“内在独白”有助于模型评估观察结果、跟踪中间步骤并进行动态调整，从而提升任务完成度。

*   **动作（Action）**：模型执行点击、滚动或键入等操作，直到它判断任务已完成或需要用户进一步输入。虽然大多数步骤能自动完成，但在遇到敏感操作（例如输入登录信息或回答 CAPTCHA 等）时，CUA 会请求用户进行确认。

相关文章：[https://openai.com/index/computer-using-agent/](https://openai.com/index/computer-using-agent/)

系统提示词：[https://baoyu.io/blog/openai-operator-system-prompts-cn](https://baoyu.io/blog/openai-operator-system-prompts-cn)

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/Agent]]
- [[00-元语/browser-automation]]
- [[00-元语/llm]]
- [[00-元语/multimodal]]
- [[00-元语/workflow]]
