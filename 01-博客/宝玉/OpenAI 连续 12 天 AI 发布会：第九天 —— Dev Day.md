# OpenAI 连续 12 天 AI 发布会：第九天 —— Dev Day

## 文档信息
- 来源：https://baoyu.io/blog/openai-dev-day-9
- 发布日期：2024-12-17
- 作者：宝玉

## 摘要

**1) 一句话总结**
OpenAI 在“12天发布会”的第9天（Dev Day）面向开发者推出了功能更全且成本更低的 o1 API 正式版、支持 WebRTC 且大幅降价的实时语音 API，以及全新的偏好微调功能和多语言 SDK。

**2) 核心要点**
*   **o1 API 正式版上线**：新增视觉输入（图像理解）、函数调用（Function Calling）、结构化输出（100%遵循 JSON 架构）、开发者消息（引导模型行为的新系统消息）以及推理深度（控制模型思考时间）功能。
*   **o1 性能与成本优化**：o1 正式版比 o1-preview 减少了 60% 的计算 Token 消耗，速度更快且成本更低。
*   **实时 API 引入 WebRTC**：通过 WebRTC 支持，开发者仅需约 12 行代码即可构建具备网络自适应和回声消除功能的低延迟实时语音应用。
*   **语音 API 大幅降价**：GPT-4o 的音频 Token 价格下调 60%；同时 API 将支持 GPT-4o mini，其音频 Token 成本仅为现价的十分之一。
*   **推出“偏好微调”功能**：基于直接偏好优化（DPO）技术，开发者可通过提供“首选”与“非首选”的回答对比，定制模型的回答风格、语气和格式（GPT-4o 已开放，价格与监督微调相同）。
*   **开发者工具扩充**：全新发布了官方的 Go 和 Java 版本 SDK，并为实时 API 提供了 Python SDK 支持。
*   **开发者体验优化**：简化了注册和获取 API 密钥的流程，并在 YouTube 上线了近期的开发者日演讲视频。

**3) 风险与不足**
*   **专业能力限制**：官方明确提示，o1 模型在处理税务等专业表单时可以检测错误，但不能替代专业判断。
*   **内置数据滞后**：o1 模型自身未获取最新的（如2024年）税务数据，必须依赖开发者提供的函数调用（Function Calling）来获取后台准确信息。
*   **o1 Pro 暂未开放**：尽管开发者对 API 中的 o1 Pro 呼声很高，但该版本目前仍在实验室阶段，尚未提供。
*   **分批推送延迟**：o1 API 正式版目前仅优先向 Tier 5 客户开放，全面覆盖所有开发者还需要几周的时间。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/llm]]
- [[00-元语/llmops]]
- [[00-元语/sdk]]
- [[00-元语/evals]]
- [[00-元语/benchmark]]
- [[00-元语/audio]]
- [[00-元语/multimodal]]
- [[00-元语/protocol]]

## 正文
OpenAI 12天发布会的第 9 天，今天主要是针对开发者的，有多个API相关更新。首先是广受期待的o1 API正式版本，这个版本不仅速度更快、成本更低，还加入了视觉识别、函数调用等新功能，让开发者能够更轻松地构建各类应用。特别值得一提的是，它比之前的版本节省了60%的计算资源，这意味着开发者可以用更低的成本获得更好的性能。

在语音交互方面，OpenAI通过引入WebRTC支持，极大简化了实时语音应用的开发流程。现在，开发者只需要12行代码就能构建基础的语音交互功能。同时，OpenAI 大幅下调了相关服务的价格，其中GPT-4o的音频处理费用降低了60%，这无疑会让更多开发者有机会尝试语音应用开发。

另一个引人注目的更新是"偏好微调"功能。这项技术允许开发者根据用户偏好来定制AI模型的回答风格和内容。比如，一家金融科技公司使用这个功能后，他们的AI助手准确率提升了5个百分点以上。这对于需要个性化AI服务的企业来说是个好消息。

其他还有：新推出了Go和Java版本的开发工具包，简化了API密钥的申请流程，并在YouTube上分享了详细的开发指南。

* * *

Olivier Godement: 大家好！我是Olivier Godement，负责OpenAI的平台产品。今天是OpenAI 12天发布会的第9天，也是最好的一天——当然,我有些偏心——今天的主题完全围绕着开发者和构建在OpenAI API之上的初创公司。放眼过去,我们的API已经发布四年了,规模令人惊叹。来自200多个国家的两百万开发者正在使用。这真的很酷。所以,作为答谢,我们今天宣布几个新模型和功能,感谢你们成为社区的一部分。这有点像是一个小型开发者日,就像北极的圣诞老人特别版。Michelle,交给你了。

Michelle Pogras: 谢谢。我是Michelle Pogras,我在后训练研究团队工作。

Brian John: 嗨,我是Brian John。我也在OpenAI的后训练团队工作。

Michelle Pogras: 太棒了。正如Olivier所说,我们不断在举办开发者日活动。所以,这是另一个小活动。今天,我很高兴地告诉大家,我们在API中推出了o1的正式版。自从我们在九月发布OpenAI o1预览版以来,开发者们一直在API上开发非常酷的东西。如自主应用程序、客户支持或财务分析。这也是一个很棒的编码模型,所以我们对此非常兴奋。

我们还收到开发者反馈,你们缺少一些API中预期的核心功能。今天,我们将在API的o1中推出这些功能。我们会推出函数调用、结构化输出和开发者消息。开发者消息是一种新形式的系统消息。它们是我们指令层次结构工作的一部分,用于教给模型应该遵循哪种指令,以及按什么顺序进行。开发者消息完全由开发者控制,并用于引导模型的行为。

最后,我们还推出了"推理深度"功能。这是一个新的参数,指示模型应该花多少时间进行思考。在处理简单的问题时,这对节省时间和资金很有帮助,然后你可以在更复杂的问题上投入更多的计算。我刚才说"最后",但其实还有一个重磅功能：我们在API中引入了视觉输入。大家非常期待这一功能,我们认为它将在制造或科学领域起到很大作用。因此,非常期待看到你们将会开发出什么。实际上,让我们来看看一个快速演示。

Brian John: 好的。是的,我们通过现场演示来看看API中o1的一些新功能,从视觉开始。为了演示,我准备了一个几天前用虚假数据填写的文本表单的照片扫描,但其中有一些错误,我想知道o1能否帮忙检测这些错误。但在开始之前要注意,虽然o1可以帮我们检测表单中的错误,但它不能替代专业判断。

好的,我将打开我们的开发者游乐场,这是一个非常好的用户界面,用于试验OpenAI的模型。在API中。在顶部,这里是我们新推出的开发者消息。在这里你可以为模型提供一些高层次的指令,或者有时是详细的指令,关于它作为一个模型应该如何表现。我这里有一个相当简单的指令。

然后我上传了表格中的图片,并要求模型找出错误。当o1在思考时,让我们回到我的表格中查看我所遇到的错误。我遇到的第一个错误,是在第11行计算调整后的总收入,AGI。这一行我应该从第9行中减去第10行,但是我做反了,我用了加法。我们都遇到过这种情况。每次我看到AGI这个词时,我都有点太兴奋了。第二个错误是我使用了错误的标准扣除。查看第4页的图表,标准扣除取决于申报状态和第1页上选中的框数。因此,这意味着要找出正确的标准扣除,需参考另外两张图片的内容。让我们看看模型给我们什么反馈。

好的,让我们看一下模型。很好,它注意到了第11行的算术错误。我本应该使用减法,这很好。而且它还发现标准扣除额不正确,这也是对的。因此,我需要调整我的应税收入和随后的税。

好吧,在修正了表格上的这两个错误后,我知道我现在的应税收入是$9,325。接下来,我会根据表格信息询问模型,如果我的应税收入是这个数,我得付多少所得税？

Michelle Pogras: 你认为模型会怎么做呢？真希望它能给出一个好的答案。

Brian John: 嗯,o1没有获取最新的2024年税表的权限,但它可以使用我们在右边提供的一组功能。这些功能是模型与后台API交互的方式。在这里我们可以看一下这些功能之一。我们以JSON格式提供这个功能,并且提供功能用途的高级描述以及模型调用功能所需的参数集。这就是我们的功能调用特性。

Michelle Pogras: 是有道理的。所以模型调用了这个功能,我们可以在后台调用,获取正确的税务数据,然后传回来。

Brian John: 没错。看起来模型用从图像中提取的输入调用了正确的功能。

Michelle Pogras: 用户会看到这些操作吗？

Brian John: 不太会。所以这几乎都发生在你应用的后台。用户看不到任何函数调用或你的API响应。在我们得到响应后,我们会将其传回模型,o1会通过漂亮的用户信息回复用户。此处,它告知用户他们新更新的收入文本。

而这并不是全部。我还想展示最后一点,即结构化输出。我会询问模型表单需要哪些修正？但在按Enter之前,我将为模型提供JSON格式的响应架构。这让模型按照此JSON架构输出。此外,我们在API后端实施了解决方案,确保模型输出总是100%符合此JSON架构。

Michelle Pogras: 理解了。

Brian John: 实际上,快速看一下这个架构。它叫做表单修正,包含修正列表。每个修正都有预期的内容,比如原因,这样我们可以向用户展示他们的错误之处。还有位置。这特别酷。我们可以为PDF渲染一个UI,并准确突出显示问题点,以及新旧值。所以,当你不想从模型渲染markdown,而只是自动提取JSON时,结构化输出非常有用。你可以看到模型已经输出修正。它们是漂亮的JSON格式。我们有新的值、旧的值、所有位置和原因。这在构建复杂功能性应用时非常有用。

我们刚刚展示了一个简短的演示,但演示无法全面展现情况。我们进行了内部评估,类似于测试,查看功能在发布前的表现。对于o1,由于对开发者很重要,我们进行了API用例的评估。让我为你展示一些结果。

让我们看看。首先是函数调用,我们有一些内部评估,结果显示新o1模型在函数调用上明显优于GPT-4。这包括正确调用函数和不必要时不调用。你可以将函数调用与结构化输出结合,o1表现明显好于4o。关于结构化输出,o1在这项评估中也有显著提升,意味着模型在格式遵循上更好,违背分布的情况更少。

接下来是编码,大家常谈论这个。LiveBench是开源的编码评估,我们很高兴看到o1在这次评估中明显优于o1 Preview和4o。

Michelle Pogras: 太棒了！这是重大进步。

Brian John: 是的。最后,我们有AIME。在这个评估中,你可以看到o1再次显著优于o1 Preview。但实际上,有趣的是在右边我们有结构化输出的o1。在构建结构化输出时,我们真的想确保模型表现同样出色在使用结构化输出时同样表现出色。所以你可以看到推理能力即使有这个功能也能保持。因此您可以在您的应用程序中使用它而无需担心结果。

这些是评估,但除了评估之外,还有一个非常有趣的延迟变化。o1实际上比o1 Preview使用少60%的计算Token,这意味着它更快,成本更低。最后,我们听到了你们对API中的o1 Pro的大量需求,很遗憾我们现在还没有,但我们的技术团队正在实验室努力工作,它很快就会推出。

Michelle Pogras: 我们正努力工作。

Brian John: 是的。酷！

Michelle Pogras: 太棒了！

Olivier Godement: 非常感谢,Brian和Michelle。

Michelle & Brian: 谢谢。

Olivier Godement: 回顾一下,API中的o1,功能编码,结构文档,开发者消息,图像理解。我们今天开始向tier 5的客户推出。我们会收到电子邮件,需要几周时间才能覆盖所有人。所以o1并不是我们最近发布的唯一模型。我们对实时API感到非常兴奋。如何实现自然的人类级延迟的语音应用呢？Sean给我们最新资讯。

Sean: 嗨,我是Sean,我和——。

Andrew: 我是Andrew。

Sean: 今天我来谈谈实时API。如果你没用过实时API,它可以让你用OpenAI构建实时语音体验。你可以打造自己的ChatGPT,或者高级语音模式,添加AI助手,用AI做各类酷炫的事情。今天我们有WebSocket支持,你可以做服务器到服务器的语音传输并获取响应。

但今天,我们宣布的亮点是WebRTC支持。这非常令人兴奋,有三个原因。首先,WebRTC是为互联网设计的。无论是视频会议还是低延迟视频流,都用到了WebRTC。它能应对网络的变化,调整比特率并消除回声。所以令人兴奋的是,现在实时API受益于所有这些功能。如果我构建一个应用程序,事情会变得简便且可用。

我将演示一个小的应用程序,展示其简便性。这里有一个小的HTML,用于给你一个结构,其中有一个音频元素。我们有一个点对点连接。点对点连接就是你和实时API之间的一对一连接。今天我们要做的是创建这个点对点连接并且我们会说,当实时API发送音频给你时,把它插入到音频元素中。接下来,我们获取麦克风音频并将其添加到点对点连接中。我们会发送一个音频流到OpenAI。

现在我们已经建立了这个点对点连接,我们进行一个offer/answer。这个offer/answer会本地收集所有信息。你通过HTTP post发送它,然后我们会响应。WebRTC会为你处理所有事情。你不需要处理拥堵控制或音频捕获这些以前的问题了。这非常棒。

Andrew: 那么这与之前的WebSocket集成相比如何呢？

Sean: 如果你之前在使用WebSockets,这段代码可能要有200到250行,而且会有额外的问题。你必须处理后压和其他问题。这是一个你在进入生产环境之前不会意识到有多烦人的问题。现在让我们实际运行代码,看一下效果。这是代码,我的音频元素。我刷新页面。

【演示对话】

*   Hola, como estas?

*   还有多少天到圣诞节？

*   圣诞节是12月25日,今天是12月17日。那就是还有八天到圣诞节。

Sean: 不错。演示成功总是很让人兴奋。好了。是的,这就是它看起来的样子。

Andrew: 所以,你只需要复制粘贴这12行代码,运行脚本,就完成了吗？

Sean: 是的,我们会发布这段代码,你可以直接去下载。你唯一需要更改的是你的API token。这样你就可以下载这段代码并运行。我很期待看到随着我们简化的过程,人们会创造出什么。

好的,那么接下来我要谈到的,我得带出一个小玩伴。因为今年我没能找到一个"架子上的精灵",所以我有一个"草坪上的小鹿",这是新的官方玩具。"草坪上的小鹿"有一个微控制器。为了给大家一个概念...在我插上电源之前,这是它的大小。这个微控制器真的非常小,和硬币差不多。所以我把微控制器放在里面,现在我现在我要插上电源,看看会发生什么。

小鹿: 圣诞快乐。你在说什么？

Sean: 我们正在讨论将WebRTC添加到实时API中。

小鹿: 哦,对我来说那听起来有点复杂。我们聊点更有趣的事情吧,比如送礼物。在平安夜飞越天空,给每个人带来快乐,真是无与伦比的体验。

Sean: 好的。那么,我圣诞节会收到什么呢？

小鹿: 哦,我不该剧透,但我想你今年可能会收到一些非常熟悉的东西。

Sean: 是吗？会不会是……一些煤炭？

[笑声]

Sean: 很酷。我们在驯鹿中使用实时API,但这只是你可以构建的众多用例中的冰山一角。对于这种规模的东西,你可以把它放在可穿戴设备上,比如眼镜,或者家里的摄像头和麦克风里。你可以拥有上下文感知的智能助手。我对于人们能够构建的东西感到非常兴奋,因为使用这个SDK,你只需要设置token并设定一些关于Wi-Fi的细节,它就会连接。这些微控制器你可以从任何主要零售商那里买到。你只需插入USB仅此而已。我真的觉得用30到45分钟,人们就可以开始动手组装了。这不需要焊接,也不需要硬件。我们只是将扬声器插入耳机插孔。

Olivier: 非常酷,而且我们还对实时API进行了其他更新。首先,我们听取了你们关于成本的反馈,并正在降低成本。从现在起,GPT 4o音频tokens将便宜60%,我们也将支持API中的4o mini,其音频tokens成本将是现价的十分之一。其次,我们为LTIM API推出了Python SDK支持,让集成更加容易。最后,我们对API进行了一些更改,使函数编码和保护措施的使用更加方便。因此,我们希望看到你们将构建出的各种应用程序。

好的,我们刚才讨论了模型和API,现在我们想谈谈微调和定制。开发人员很需要这项功能,以便根据自己的使用场景来定制模型。你有什么新推出的,Andrew？

Andrew: 是的,很遗憾我没有驯鹿可以分享,但我们很高兴地宣布我们的API中可用一种新的微调方法,叫做偏好微调。我们将使用一种叫做直接偏好优化的方法,旨在帮助你创建更符合用户偏好的模型,希望能提高性能在用户反馈起关键作用的领域。

Olivier: 你们最近发布了很多东西,过去几个月你们非常活跃,但我们的API已上线一年,包括微调API,新的是什么？有何不同？

Andrew: 当前在我们的API中,我们已有监督微调和新宣布的强化微调。大多数用户一直在使用监督微调根据他们的用例定制我们的模型。在监督微调中,你要提供你希望模型产生的具体输入和输出。因此,如果你在创建一个用户聊天机器人,要给它用户信息并得到确切的响应。

而在偏好微调中,会有所不同。不再是具体的输入输出,而是给出一对响应,其中一个比另一个更被偏好。我们的微调过程将对其进行优化,以学习这些响应的差异。那些差异可能是：响应格式、风格指导,甚至是像有用性或创造力这样的抽象特性。

Olivier: 懂了。那么典型的使用案例有哪些？像是客户支持、文案创作、创意写作。

Andrew: 对,没错。因此,如果你发现我们的模型有些啰嗦,或者给出不相关的答案,通过微调可以引导它给出更简洁和相关的回应,强调偏好的表现,弱化不受欢迎的结果。另一个好的应用可能是内容审核。比如,如果你的组织希望有特定的风格或语气,可以通过偏好微调来实现。

不过,我给你展示一下在我们的API中进行微调有多简单。我现在在平台的UI上,在微调选项卡。点击创建微调后,会看到一个新的方法下拉菜单,我会选择直接偏好优化。接着,我选择基础模型。在这里,我选GPT-4o。最后,只需上传训练数据。

正如我之前提到的,格式稍有不同,所以我会快速演示一个例子。在这个例子里,我们询问助手纽约市的天气如何,并提供了两对回应。可能我们正在设计一个聊天机器人,我们希望它更具对话性。因此,在首选回答中,内容比较详细,并且用华氏度回答。而在非首选回答中,内容比较简洁,并以摄氏度回答。

因此,我将这些例子放到一个JSONL文件中,每个例子包含输入信息、首选输出和非首选输出。现在,我将上传数据。我们还提供一些超参数以供调整,但我现在选择默认值并点击创建。这将启动微调过程。根据数据集的大小,可能需要几分钟到几个小时不等。但是一旦完成,我们就能像使用API中的基础模型那样进行采样。

Olivier: 嘿,这真令人兴奋。你有没有让人们使用它的机会？

Andrew: 是的,我们已给予几位可信赖的合作伙伴偏好微调的早期使用权限,并且看到了一些好效果。例如,Rogo AI正在为金融分析师构建一个AI助手,他们使用我们的模型重写和重构用户查询,以提供更相关的答案。他们使用监督微调时,效果不如基础模型。但通过偏好微调,准确性在内部基准上从基础模型的75%提高到了超过80%。

我们非常期待其他开发者能够利用偏好微调做出些什么成果,这也是为什么我们今天将对GPT-4o开放,很快也会对GPT-4o Mini开放,训练每个Token的价格与监督微调相同。如果你们对开始偏好微调有兴趣,我建议你们在今天晚些时候查看我们的文档。

Olivier: 太棒了！好吧,有很多内容要解读。很多礼物。我们从o1开始。o1在API中具有完整的生产功能集,例如人们期望的函数调用。从今天开始先向tier 5级的用户开放。之后,我们讨论了实时API,新API,更简单的WebRTC集成和降价。现在我们谈到偏好微调,使得为你的用例定制模型更容易。

但这还不是全部。团队不断地发布新内容。他们今天发布了一些新东西,因此我想给你们展示一些即将推出的较小功能。首先,我们不能不谈开发者体验和产品质量。因此,我们将从今天开始宣布发布新的SDK。我们提供Go和Java SDK的可选支持。你可以在这里找到它们。请给我们反馈。和Python SDK及Node SDK类似,它们支持你在OpenAI上需要的所有API端点。这是Go版本,还有Java版本。

第二点,我们非常注重简化过程。我不会详细介绍整个流程,但我们有新的登录、注册和获取API密钥的流程。所以你不需要再签署五个服务条款,仅用几秒钟就能获得API密钥。

接下来,过去几个月我们在全球各地举办了一些开发者日的活动。我们的反馈很好,内容也很棒。所以我们今天刚在YouTube上发布了演讲。去查看OpenAI的YouTube频道,你可以找到许多很酷的内容。

最后,这确实很多,所以我们要进行一个AMA（问我任何事）。从今天开始,与OpenAI的API团队一起,进行一个小时的"随便问我什么"。因此,请在OpenAI开发者论坛上来向我们提问。

最后,抱歉,我有一个糟糕的笑话。我们一直在讨论煤炭。你知道圣诞老人有顽皮孩子名单,我们就像顽皮的孩子。为什么结构化输出在顽皮名单上？

Andrew: 我不知道。告诉我。

Olivier: 圣诞老人听说它是一个模式。最后这一点,明天见,第12天的第10天。再见。
