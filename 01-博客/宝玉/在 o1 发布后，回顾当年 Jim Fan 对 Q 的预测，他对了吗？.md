# 在 o1 发布后，回顾当年 Jim Fan 对 Q*的预测，他对了吗？

## 文档信息
- 来源：https://baoyu.io/blog/ai/o1-and-alphago
- 发布日期：2024-09-15
- 作者：宝玉

## 摘要

### 1) 一句话总结
结合新发布的 o1 模型，Jim Fan 去年关于 Q* 将采用类似 AlphaGo 架构（策略、价值、搜索、判定）并通过数学和代码进行自我强化学习的预测，基本被证实是准确的。

### 2) 关键点
*   **架构类比**：Jim Fan 预测 Q* 类似于 AlphaGo，通过机器自我对弈和学习来不断提升（类似“永动机”）。
*   **传统大模型痛点**：传统大语言模型属于“快思考”，缺乏类似蒙特卡洛树搜索的“慢思考”机制，且难以客观判定生成结果的好坏。
*   **策略神经网络猜想**：由 GPT 生成解决数学问题的思维过程，这与 o1 目前展现出的高质量“思维链（CoT）”表现相符。
*   **价值神经网络猜想**：由另一个 GPT 专门评估思考链中每个中间推理步骤的正确性概率，提供步级反馈而非仅评估最终结果。
*   **搜索机制猜想**：语言模型的搜索即“推理”，通过思维链（CoT）、思维树（ToT）或思维图（GoT）来寻找最优解。
*   **输赢判定猜想**：通过数学答案的正确性、推导过程的合理性，或将数学问题转化为可执行代码（如使用 Lean 工具）来判定对错并给予奖惩。
*   **o1 的实际表现**：o1 在数学、编程以及文字解密领域表现突出，侧面印证了其充分利用了数学和代码问题进行训练。
*   **创造力表现**：o1 在写作方面不如 GPT-4o，符合 Jim 的预测，即推理能力的提升不代表创造力（写诗、角色扮演等）的提升，创造力仍依赖于人类自然数据。

### 3) 风险/不足
*   **泛化能力未知**：仅使用数学和编程领域的数据进行训练，其能力是否能泛化到其他领域仍是问题，这是决定该路径能否通向 AGI 的关键。
*   **写作与创造力短板**：o1 在写作等需要创造力的任务上表现不及 GPT-4o。
*   **技术细节未公开**：o1 的搜索组件是否实际应用了思维树（ToT）或思维图（GoT）目前尚未公开，仍无法完全确认。

## 正文
[去年 Jim Fan 对于 Q* 的预测](https://x.com/DrJimFan/status/1728100123862004105)，现在结合刚发布的 o1 来看基本上都是准确的！

Jim 将 Q* 和 AlphaGo 做了类比，猜测 Q* 可能类似于 AlphaGo，是通过与自己之前的版本进行对弈，自我对弈不断进步，甚至于架构都是类似的。

AlphaGo 的架构核心有四个组件：

1.   策略神经网络（Policy NN，学习部分）：负责选择下一步最有可能赢的走法
2.   价值神经网络（Value NN，学习部分）：评估当前棋局
3.   蒙特卡洛树搜索（MCTS，搜索部分）：模拟从当前位置开始落子的多种可能，类似于人类在算棋步（假如我放在 A 位置，那么对手可能下在哪几个位置，然后我再下一步怎么应对……）
4.   输赢判定：根据围棋规则判定谁赢了。

这个架构的神奇之处在于整个训练过程不需要人类干预，可以像一个“永动机”一样，完全机器自己跟自己之前的版本学习，自己提升自己。这里面的关键在于围棋有很清晰的输赢判定规则，所以机器可以根据输赢结果知道自己选择路径的好和坏，从而可以对模型行为进行奖励或者惩罚，来提升模型的能力。

但对于大语言模型来说，难点在于：

1.   它是快思考，没有类似于蒙特卡洛树搜索这样慢思考
2.   很难判定生成结果的好坏

Jim 认为可以借助数学问题来训练大语言模型，并提出了他对于 Q* 的四个组件的猜想：

1.   策略神经网络：由 GPT 生成解决数学问题的思维过程。（从 o1 的表现来看，这一步应该产生的是思维链）
2.   价值神经网络：另一个 GPT，用于评估每个中间推理步骤的正确性概率。不仅仅对整体输出作出判断，而是对思考链中的每一步提供反馈。（现在 o1 能写出高质量的思维链和这个有很大关系）
3.   搜索：语言模型的搜索不同于围棋中的搜索，这里对应的其实是推理，也就是借助思维链找出推理中的最优解。当然除了 CoT，还有思维树（Tree of Thought），将 CoT 和树搜索结合起来；还有思维图（Graph of Thought），将树进一步变成图。（至于 o1 是不是应用到了思维树和思维图，由于其没有公开，现在还不好肯定）
4.   输赢判定：有几种可能 a) 根据推导数学问题得出的答案是否正确来判断 b) 根据生成的思维链，判断推导过程的结果，即使答案不对，但是推导过程正确也可以获得部分奖励 c) 将数学问题变成代码，执行程序运行结果。现在已经有 Lean 这样的工具可以将数学问题变成代码了。

按照 Jim 的这个设想，负责策略神经网络的大语言模型和负责价值神经网络的大语言模型，就可以相互训练相互促进，有了更强的策略大语言模型后，又可以帮助搜索组件探索出更好的搜索策略，就像 AlphaGo 那样成为一个“永动机”，自己一直训练自己。

现在来看，o1 在数学和编程领域表现突出，也侧面印证了它是充分利用了数学问题和代码问题来进行训练。

但问题在于仅仅使用数学领域和编程领域的数据训练，能力是否可以泛化到其他领域？这也是决定了 o1 这条路径能否通向通用人工智能 AGI 的关键。

从演示和体验来看，在文字解密领域也是很强的，不知道是否是能力泛化的结果，还是也有专门的训练。

另外 o1 在写作方面，表现不如 GPT-4o，也许就像 Jim 说的：

> 我所描述的仅仅是关于推理的部分。并没有说 Q* 在写诗、讲笑话或角色扮演方面会更有创造力。提升创造力本质上是人类的事情，因此我相信自然数据仍会胜过合成数据。

## 相关文档

- [[01-博客/宝玉/推理规模扩展定律（inference scaling law）会成为大力出奇迹的新方向吗？它能带我们走进 AGI 吗？|推理规模扩展定律（inference scaling law）会成为大力出奇迹的新方向吗？它能带我们走进 AGI 吗？]]；关联理由：同一事件；说明：两文同日围绕 o1 发布展开分析，并共同讨论推理链训练与 AGI 路径判断。
- [[01-博客/宝玉/AI 知道自己答案错了吗？|AI 知道自己答案错了吗？]]；关联理由：延伸思考；说明：该文延展了本文“输赢判定/奖励函数”的核心命题，进一步解释可验证任务如何驱动模型自我提升。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/llm]]
- [[00-元语/AI]]
- [[00-元语/数学]]
- [[00-元语/evals]]
- [[00-元语/decision-making]]
