---
title: "Claude Code 的\"懒加载\"更新：AI 终于学会了\"随叫随到"

来源: "https://baoyu.io/blog/2026/01/14/claude-code-mcp-tool-search"
发布日期: "2026-01-14"
作者: "宝玉"
---

## 摘要

**1) 一句话总结**
Claude Code 推出了基于“懒加载”机制的 Tool Search 功能，通过按需搜索和调用 MCP 工具，大幅降低了 AI 上下文窗口的 token 占用，从而提升了响应速度和长对话质量。

**2) 关键要点**
*   **背景痛点**：Claude 的上下文窗口约为 20 万 token，过去预加载大量 MCP（模型上下文协议）工具说明书会消耗极大空间（例如 7 个以上的 MCP 服务器会占用超 6.7 万 token，约占总容量的三分之一）。
*   **核心解决方案**：推出 Tool Search 功能，将工具的“全员预加载”改为“懒加载（按需调用）”。
*   **自动触发机制**：当连接的 MCP 工具说明书总和超过上下文窗口的 10% 时，Claude Code 会自动无感地切换至 Tool Search 模式。
*   **显著的数据提升**：据官方示例，原本需要消耗 15 万 token 的工具定义场景，采用新方式后仅需 2000 token，空间节省高达 98.7%。
*   **用户端收益**：用户可以安装更多 MCP 服务器而不引起卡顿；释放的上下文空间让长对话不易“失忆”；减少 token 处理量带来了更快的响应速度。
*   **开发者建议**：MCP 服务器开发者需要重点优化 `server instructions` 字段，该字段已成为 AI 决定何时调用对应工具的关键线索。
*   **技术演进趋势**：AI 工具生态正从“堆叠功能”向“精打细算”的效率优化转变；例如 Cloudflare 提出的 "Code Mode" 允许 AI 直接编写脚本连接工具（如从 Google Drive 直传 Salesforce），避免中间数据占用模型上下文。

## 正文

![Image 1](https://s.baoyu.io/imgs/2026-01-14/claude-code-mcp-tool-search/cover.png)

想象一下这个场景：你是一个项目经理，手下有 50 个员工，每个人都有不同的专长。每次开会之前，你都要求所有人先到会议室，每个人都要做一遍自我介绍，详细说明自己能干什么。

结果光是听完自我介绍，会议时间就用掉了一大半，真正讨论问题的时间所剩无几。

这就是 AI 工具生态面临的一个真实困境，直到今天，Claude Code 推出了一个叫 **Tool Search** 的功能，终于解决了这个问题。

![Image 2: 会议自我介绍](https://s.baoyu.io/imgs/2026-01-14/claude-code-mcp-tool-search/illustration-meeting-overload.png)

AI 的"工作记忆"有多金贵
--------------

要理解这个更新为什么重要，我们得先聊聊 AI 的”**上下文窗口**“是什么。

你可以把上下文窗口理解成 AI 的”工作记忆“，就像你我同时能记住的事情是有限的一样，AI 在一次对话中能处理的信息量也有上限。这个上限用”token“来衡量，大致可以理解为字词的数量。

Claude 的上下文窗口大约是 **20 万 token**，听起来很多对吧？大概相当于一本 300 页的书。但问题是，这些空间不只是用来记你说的话，还要装很多”基础设施“。

其中一个大头，就是**工具的说明书**。

MCP：给 AI 装的”App Store“
----------------------

过去一年，Anthropic 推出了一个叫 **MCP（Model Context Protocol，模型上下文协议）** 的东西。你可以把它理解成给 AI 装的 App Store。

以前，想让 AI 连接外部工具（比如读取 Google Drive 文件、操作 Salesforce 数据库），每个工具都需要单独开发接口，非常麻烦。MCP 就像是制定了一个统一标准：只要工具支持 MCP 协议，AI 就能直接用。

这个协议火了。短短几个月，社区已经开发了几千个 MCP 服务器，覆盖了各种各样的工具和数据源。

用户也越来越”贪心“：既然能连这么多工具，那我全都要！GitHub 上有用户晒出自己的配置：7 个以上的 MCP 服务器，光是工具说明书就占用了 **67000 多个 token**。

67000 token 是什么概念？大约占了上下文窗口的**三分之一**。还没开始干活呢，AI 的”脑容量“就被说明书塞满了三分之一。

这就像你雇了一堆助手，结果每次开工之前，光是听他们自我介绍就要花一个小时。

![Image 3: 上下文容量被占满](https://s.baoyu.io/imgs/2026-01-14/claude-code-mcp-tool-search/illustration-brain-capacity.png)

从"全员到场"到"随叫随到"
--------------

Tool Search 的解决方案其实很直观：不要让所有工具都”预加载“到上下文里，而是用”**懒加载**“的方式，需要谁再叫谁。

具体怎么工作的呢？

Claude Code 会自动检测：如果你连接的 MCP 工具说明书加起来**超过上下文窗口的 10%**，就会自动启用 Tool Search 模式。在这个模式下，工具不再预先加载，而是通过搜索的方式按需调用。

打个比方：以前是把所有员工都叫到会议室等着，现在是把员工名单和专长简介放在一个通讯录里。需要谁的时候，查一下通讯录，打个电话叫来就行。

这个改变带来的效果是显著的。官方博客里举了个例子：原本需要 15 万 token 来装工具定义的场景，用新方式只需要 2000 token，**节省了 98.7%**。

更重要的是，这个改变对用户来说几乎是无感的。如果你的工具不多，没触发 10% 的阈值，一切照旧。只有当工具真的多到影响效率的时候，系统才会自动切换模式。

![Image 4: 预加载 vs 懒加载](https://s.baoyu.io/imgs/2026-01-14/claude-code-mcp-tool-search/illustration-lazy-loading-comparison.png)

这背后是一个更大的趋势
-----------

说实话，”懒加载“这个概念在软件工程里一点都不新鲜。网页开发者天天用它来优化加载速度。但把它应用到 AI 工具管理上，说明这个领域正在从”能用就行“走向”精打细算“。

Anthropic 官方博客里还提到了一个更激进的思路：让 AI 用**写代码的方式**来调用工具。

传统方式下，AI 调用工具就像接力赛，从 Google Drive 拿到数据，传给模型，模型再传给 Salesforce。每一棒都要经过模型的”手“，占用上下文空间。

而如果 AI 能写代码，它可以直接写一段脚本：从 Google Drive 拿数据，直接存到 Salesforce，中间结果根本不需要经过模型。这就像是从”接力赛“变成了”直达航班“。

Cloudflare 把这种方式叫做"**Code Mode**"，实测下来效率提升非常明显。

![Image 5: 接力赛 vs 直达航班](https://s.baoyu.io/imgs/2026-01-14/claude-code-mcp-tool-search/illustration-relay-vs-direct.png)

对普通用户意味着什么
----------

如果你是 Claude Code 的用户，这个更新带来几个实际好处：

**第一，你可以放心大胆地装更多 MCP 服务器了。** 以前装七八个可能就开始卡顿，现在这个限制大大放宽。

**第二，长对话不再容易”失忆“。** 以前工具说明书占了太多空间，聊久了 AI 可能记不住前面的内容。现在上下文空间释放出来，对话质量会更稳定。

**第三，响应速度会更快。** 少处理那么多 token，自然就省时间。

如果你是 MCP 服务器的开发者，Anthropic 建议好好写你的”`server instructions`“字段。这个字段以前不太重要，现在它变成了 AI 决定什么时候调用你的工具的关键线索。

写在最后
----

这个更新看起来只是一个技术细节的优化，但它反映的是 AI 工具生态正趋向”成熟“的信号。

早期的 AI 应用，大家都在拼”能不能做到“。现在功能越来越多，竞争的焦点开始转向”怎么做得更高效、更省钱“。

这就像智能手机刚出来的时候，大家比的是”能装多少 App“。后来手机成熟了，大家开始比的是”续航多久“、”内存管理好不好"。

AI 工具生态也在经历同样的转变。从粗放到精细，从堆功能到抠效率。

* * *

![Image 6: 信息图](https://baoyu.io/blog/2026/01/14/imgs/infographic.png)

## 相关文档

- [[01-博客/宝玉/MCP 和 Skills 到底什么区别？一篇文章说清楚|MCP 和 Skills 到底什么区别？一篇文章说清楚]]；关联理由：解说；说明：该文系统比较 MCP 与 Skills 的分工，可解释本文 Tool Search 背后的上下文成本矛盾。
- [[01-博客/宝玉/MCP 遇上代码执行：构建更高效率的 AI 智能体|MCP 遇上代码执行：构建更高效率的 AI 智能体]]；关联理由：延伸思考；说明：该文提出“代码执行 + MCP”路线，是本文按需发现工具之外的进一步效率优化方案。

## 关联主题

- [[00-元语/mcp]]
- [[00-元语/context-optimization]]
- [[00-元语/memory]]
- [[00-元语/tool]]
- [[00-元语/protocol]]
- [[00-元语/cloudflare]]
- [[00-元语/Claude]]
- [[00-元语/cli]]
- [[00-元语/AI]]
- [[00-元语/Agent]]
