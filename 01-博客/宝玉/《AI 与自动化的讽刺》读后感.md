# 《AI 与自动化的讽刺》读后感

## 文档信息
- 来源：https://baoyu.io/blog/the-irony-of-ai-and-automation-review
- 发布日期：2025-12-16
- 作者：宝玉

## 摘要

### 1) 一句话总结
1983年关于工业自动化的论文《自动化的讽刺》精准预言了当今 AI Agent 时代面临的困境，表明尽管自动化技术在进步，但人类在技能维持、监控疲劳和角色转换等方面的底层认知限制（硬件限制）依然存在。

### 2) 关键要点
*   **底层逻辑重现**：当今“AI干活，人类监督”的模式，与1983年论文中研究的“机器干活，人类监督”的工厂自动化底层逻辑完全一致。
*   **技能与记忆退化**：专家转变为 AI 监工后，由于缺乏实际动手操作，专业技能和相关知识的记忆提取速度会逐渐萎缩（用进废退）。
*   **下一代人才断层**：当前 AI 系统的准确性依赖老一代操作员的经验老本，而下一代从业者因缺乏实战机会，未来将难以判断 AI 输出的对错。
*   **实践与培训悖论**：脱离真实场景实战的理论培训缺乏效果；越是成功的自动化系统（人工干预越少），反而越需要在人员培训（如模拟器训练通用策略）上投入巨资。
*   **糟糕的 UI 设计**：当前 AI Agent 输出的充满自信的长文本和多步骤计划，是极差的异常检测界面，远不如经过几十年优化的工业控制室设计。
*   **领导力技能转换**：监督 AI 本质上需要“领导力”（设定目标、分配任务、给反馈），这对习惯亲自执行的员工是一次巨大的角色与技能体系重建，但目前企业缺乏相关培训。
*   **核心结论**：解决自动化带来的人机交互问题，可能比实现自动化本身需要更大的技术创造力。

### 3) 风险/缺口
*   **监控疲劳风险**：人类生理结构决定了无法对“很少出错”的系统保持超过半小时的警觉，极易漏掉 AI 隐藏在自信长文本中的隐蔽错误（如小前提错误）。
*   **成本转化风险**：决策者试图用 AI 节省的人力成本，可能会加倍转化为维持人类应对罕见异常能力的培训成本。
*   **心理与地位风险**：员工从“专家/创造者”降级为“监工/审核员”，会带来心理层面的冲击、职业认同感及社会地位的下降。
*   **报警系统失效风险**：若引入自动报警系统来对抗监控疲劳，一旦报警系统本身出现故障，操作员往往无法及时察觉。

## 正文
今天看了篇文章，叫：《AI 与自动化的讽刺》，内容跟当前 AI 的发展很应景。

1983年，一位认知心理学家 Lisanne Bainbridge 写了篇论文，题目叫《自动化的讽刺》。四十多年后的今天，这篇论文上预言的问题，正一字一句地在 AI Agent 身上应验。

当年她研究的是工厂自动化：机器干活，人类监督。

今天我们面对的是AI Agent自动化：AI干活，人类监督。场景变了，但底层逻辑一模一样。而她当时在论文中指出的那些问题，又重新来了一遍。

论文中都提到了哪些问题呢？

1.   技能退化困境：不用就会忘，专家变监工后技能会萎缩

用进废退，这四个字我们都懂。但放到AI时代，它有个更残酷的版本。

以前你是某个领域的专家，天天做这件事，手到擒来。现在公司说，让AI Agent来做吧，你负责盯着它，出了问题再介入。

听起来很美好对不对？从打工升级成监工，岂不是更轻松？

问题来了：你不做这件事了，但你的技能不止不会进步，甚至还会退化。

像我这样天天用 AI 写代码的，我能感觉得到这两年是没啥进步，而且对 AI 有依赖，很多以前信手拈来随手就可以写出来的代码，现在没有 AI 就啥都不想干了。

真的是有点用进废退了。

无论是 OpenAI 还是 Anthropic 都在吹他们的 Coding Agent 多厉害，他们的员工只要验证 AI 写的结果就好了，但是他们故意没提的是，这些人都是万里挑一的高手，他们有足够的经验判断AI对不对。但如果他们接下来几年都只是验证 AI 做的对不对，那么他们的技能会慢慢倒退。

像我们这一代老程序员还好，更要命的是下一代。

今天的老程序员们好歹是从实战中成长起来的。明天的程序员呢？他们从入行第一天就在盯AI，没怎么亲手做过。他们既没有技能，也没有机会学。那他们怎么判断AI对不对？

论文原话是：

> 当前这代自动化系统，正在吃老一代操作员的技能老本。下一代操作员不可能有这些技能。

这个问题今天看不出来，三五年后可能就会凸显出来了。

1.   记忆提取困境：不常用的知识，调取速度也会变慢

还有个问题就是相关技能的记忆也会退化。

想想我们高中时哪些滚瓜烂熟的公式，现在还能想起来几个了。放到 AI 监督的场景，随着 AI 能力越来越强，大部分时候都是对的，这意味着大多数时候不需要用到你的知识，随着你的知识越用越少，相关的记忆就会退化。

1.   实践悖论：理论培训没用，必须实战才能学会，但AI在干活人没机会练

这时候你可能会想：那培训是不是有用？ 但是《自动化的讽刺》论文中的结论是：培训并没有太大用。

因为专业技能不是听课听出来的，是在真实场景里靠实战锻炼出来的。课堂上学的理论，如果没有配套的实战练习，你很可能听不懂，因为没有相应的经验框架。就算当时懂了，很快也会忘，因为没有和真实任务绑定的记忆提取路径。

要保持监督AI的能力，你得定期亲自干活。但如果公司追求的是让 AI 自动化运转以提升效率，那人就没多少机会练手。

这是个死循环。

就像论文里面说的：

> 我们训练操作员按指令行事，然后把他们放进系统，指望他们提供智慧。

你不能指望平时不需要怎么思考和练习的人类，在关键时刻能想出什么好办法。

1.   监控疲劳：人类无法长时间对"很少出错"的系统保持警觉

心理学研究早就发现，人类无法对一个很少出问题的目标保持长时间警觉，半小时是极限。这不是意志力的问题，这是生理结构决定的。

从进化角度看，这其实是个生存优势：如果你盯着一个地方什么都没发生，大脑会自动降低警觉，把注意力资源省下来应对真正的威胁。但放到监控场景里，这就成了问题。

AI Agent大部分时候是对的，偶尔会犯错。这恰好是最难监控的模式。如果它经常出错，你会保持警惕。如果它从不出错，你不用监控。但它很少出错这种情况，正好落在人类注意力的盲区里。

更糟的是，AI Agent犯错的方式特别隐蔽。它不会说"我不确定"，它会用一种极其自信的语气告诉你它的计划，洋洋洒洒几十上百行。错误可能藏在第87行的一个小前提里，比如"因为2大于3，所以我们应该……"。被那么多看起来正确的内容包裹着，被那种自信满满的语气麻痹着，你很难注意到。

那加个自动报警系统呢？

论文说：谁来监控报警系统？如果报警系统本身出了问题，操作员不会注意到，因为报警系统已经正常运转了很久。

那让人做记录呢？

论文说：人可以机械地抄数字而完全没注意数字是什么。

所有试图对抗监控疲劳的手段，都会撞上同一堵墙：人类的注意力就是无法长时间锁定在一个很少出事的目标上。这是硬件限制，不是软件问题。

1.   地位问题：从专家降级为监工，心理冲击和社会地位下降

你曾经是专家，公司里有什么难题找你，同事尊重你，你自己也有职业认同感。现在你是AI的看门人。

技能层面的损失是一回事，心理层面的冲击是另一回事。从专家降级为监工，从创造者变成审核员，从被需要变成备胎。这种转变对很多人来说是很难接受的。

论文里说，被这样降级的人会出现各种复杂的应对反应，有些看起来甚至是自相矛盾的。这部分内容展开讲太长，有兴趣的可以去读原论文。

1.   糟糕的UI：当前AI Agent界面是最差的监控设计

工业自动化领域花了几十年时间优化控制室设计：显示屏怎么布局能让操作员最快发现异常，急停按钮为什么是红色的、为什么那么大、为什么放在那个位置。每一个细节都是用事故和教训换来的。

现在看看AI Agent的界面？

一堆自信满满的长文本，一个接一个的多步骤计划，几十上百行洋洋洒洒的解释。你要在这些文字里找出那个藏着的错误。

这大概是人类设计过的最糟糕的异常检测界面。

1.   训练悖论：越成功的自动化系统，越需要投资培训人类

论文中谈到自动化带来的训练问题：

> 如果不能让操作员定期接管工作亲自干，就得用模拟器训练。但模拟器有个根本问题：你只能模拟你能预见的故障。未知的故障模拟不出来，已知但没经历过的故障也很难准确模拟。

那怎么办？

> 只能培训通用策略而不是具体应对方法。但这又带来新问题：你不能指望操作员光靠查操作手册来应对异常，因为手册不可能涵盖所有情况。

> 越是成功的自动化系统，越少需要人工干预，反而越需要在人员培训上投入巨资。 因为干预越少，人的技能退化越快，应对罕见异常的能力越弱，每次培训的成本就越高。

决策者想用AI省钱，但省下的人力成本可能得加倍投入到培训成本里。

1.   领导力困境：监督AI不只是被动看，还要主动"领导"它们

监督AI Agent不只是被动地盯着看，还得主动地指挥它们。告诉它们做什么、不做什么、分几步做、怎么调整方向。

这其实是一种领导技能。

为什么LinkedIn上夸AI Agent最起劲的往往是管理者？因为他们本来就习惯间接工作：设定目标、分配任务、给反馈、调方向，但不亲自动手。对他们来说，指挥AI Agent和指挥下属没有本质区别。

但对于一直亲自干活的执行者来说，这是一个巨大的角色转换。你得从一个做事的人，变成一个让别人做事的人。这不是改几条 prompt就能解决的，这是一整套技能体系的重建。

公司会给新晋经理做领导力培训。但有谁见过公司给AI监督者做领导力培训？

四十年前那篇论文的结尾是这样的：

> 没有时间压力时，人类可以是令人印象深刻的问题解决者。困难在于，一旦有时间压力，效率就会大打折扣。我希望这篇论文说清楚了两件事：第一，自动化不一定会消除困难，这是讽刺所在；第二，解决这些问题需要的技术创造力，可能比自动化本身还要大。

四十年后，我们换了个场景，但面对的是同一组问题。

AI Agent的能力在进步，但人类的认知结构没变。监控疲劳还是半小时，技能退化还是用进废退，注意力盲区还在那里。这些是硬件限制，不是软件更新能解决的。

推荐阅读原文： 《Ironies of Automation》：[https://ckrybus.com/static/papers/Bainbridge_1983_Automatica.pdf](https://ckrybus.com/static/papers/Bainbridge_1983_Automatica.pdf) 《AI and the ironies of automation - Part 1》 [https://www.ufried.com/blog/ironies_of_ai_1/](https://www.ufried.com/blog/ironies_of_ai_1/) 《AI and the ironies of automation - Part 2》 [https://www.ufried.com/blog/ironies_of_ai_2/](https://www.ufried.com/blog/ironies_of_ai_2/)

## 关联主题

- [[00-元语/Agent]]
- [[00-元语/AI]]
- [[00-元语/paper]]
- [[00-元语/risk]]
- [[00-元语/career]]
- [[00-元语/知识工作]]
- [[00-元语/decision-making]]
