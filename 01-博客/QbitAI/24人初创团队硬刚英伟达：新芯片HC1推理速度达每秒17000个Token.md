# 24人初创团队硬刚英伟达：新芯片HC1推理速度达每秒17000个Token

## 文档信息
- 来源：https://www.qbitai.com/2026/02/381552.html
- 发布日期：2026-02-38

## 摘要
**1) 一句话总结**
24人初创团队Taalas推出将大模型直接硬编码至硅片的专用AI推理芯片HC1，实现了17000 Token/s的峰值速度，并大幅降低了功耗与成本。

**2) 关键要点**
*   **极致性能**：HC1搭载Llama 3.1 8B模型，单芯片推理速度达17000 Token/s，比Cerebras快约10倍，远超英伟达B200（350 Token/s）。
*   **硬件规格与能效**：采用台积电N6工艺，面积815mm²；单芯片典型功耗仅250W，单服务器装配10颗总功耗仅2.5kW，支持常规空气冷却部署。
*   **核心技术路线**：采用“芯片即模型”的结构化ASIC理念，通过掩模ROM将模型与权重直接物理硬连线至芯片，仅保留部分可编程SRAM用于微调（如LoRA）和KV缓存。
*   **生产周期缩短**：通过不改变底层电路、仅调整两层掩模的制造方式，将芯片生产周期从6个月缩短至2个月。
*   **多芯片扩展方案**：针对DeepSeekR1-671B模型规划了30颗定制HC1的方案，速度可达12000 Token/s，成本（每百万token 7.6美分）不到同等吞吐量GPU方案的一半。
*   **团队与资金**：公司由AMD前高管Ljubiša Bajić等人创立，团队仅24人；产品研发投入3000万美元，目前已累计筹集2亿美元投资。
*   **未来路线图**：计划今年春季发布集成中型模型的HC1第二代变体，今年冬季部署密度更高、速度更快的HC2芯片。

**3) 风险/差距**
*   **推理深度不足**：据网友实测反馈，HC1在实现高速推理的同时，存在推理深度表现糟糕的问题。
*   **芯片易过时风险**：在当前大模型迭代极快的背景下，HC1放弃通用性、采用模型硬编码的模式，可能导致芯片极易被淘汰。

## 正文
造芯片的领域又迎来了新的“高手”。近日，一款代号为 **HC1** 的最新芯片冲上硅谷热榜，其峰值推理速度高达每秒 17000 个 token。

作为对比，当前公认最强的 Cerebras 速度约为 2000 token/s。这意味着 HC1 的速度直接快了 10 倍，同时成本骤减 20 倍、功耗降低 10 倍，让大语言模型（LLM）真正来到了亚毫秒级的即时响应速度。

这块一夜之间刷屏硅谷的芯片并非出自英伟达或 AMD 之手，而是来自一家成立仅两年、团队仅有 24 人的初创公司——**Taalas**。不同于所有竞争对手，Taalas 选择了迄今为止最极端的技术方案：**模型不再加载到内存里，而是直接刻在硅片上。换言之，芯片即模型。**

### 速度快10倍，功耗降至十分之一

HC1 是 Taalas 的首款产品，目前搭载了 Llama 3.1 8B 模型。单颗芯片即可满足 8B 模型的需求，用户每秒最高可生成 17000 个 token，远高于主流 GPU 和 ASIC。

在同一模型下，各家芯片的速度对比如下：
*   **HC1**：17000 token/s
*   **Cerebras**：约 2000 token/s
*   **SambaNova**：约 900 token/s
*   **Groq**：约 600 token/s
*   **英伟达 B200 (Blackwell架构)**：350 token/s

在硬件规格上，HC1 采用台积电 N6 工艺，面积为 815mm²，体积小巧且开源。其能效表现极为出色：每颗芯片典型功耗仅为 250W。如果一个服务器同时装配 10 颗 HC1，总功耗也仅为 2.5kW，完全可以直接使用常规的空气冷却机架进行部署。

### 极端技术路线：用灵活性换取极致效率

HC1 之所以能实现如此巨大的性能飞跃，是因为它借鉴了 2000 年代初期的“结构化 ASIC”芯片理念。

传统的结构化 ASIC 采用门阵列和固化 IP，仅通过改变互连层就能适应特定工作负载，比全定制 ASIC 更便宜，也比 FPGA 性能更优。HC1 采用了类似思路：不改变底层电路，只通过调整两层掩模，就能低成本、快速地制造出专用 AI 推理芯片。

具体而言，HC1 放弃了大多数可编程功能：
*   **掩模 ROM**：将模型连同权重一起，通过基于掩模 ROM 的调用架构直接存储在芯片上，固化执行。
*   **可编程 SRAM**：仅保留一部分 SRAM，用于保存微调后的权重（如 LoRA）和 KV 缓存。

这一策略省去了传统存算分离的成本，将一个完整的大模型通过物理硬连线直接植入芯片中。这不仅在设计成本可控的前提下实现了模型到芯片的快速转化，还将芯片的生产周期从原先的六个月缩短到了两个月。

为了弥补激进量化方式对性能的影响，研究团队保留了最低限度的灵活性：用户可以通过 LaRA 适配器进行重新训练，并支持可配置的上下文窗口。

除了 Llama 3.1，Taalas 也在尝试将其他模型集成到 HC1 上。例如针对 **DeepSeekR1-671B** 的多芯片解决方案：
通过将 SRAM 部分拆分到单独的芯片上，每片 HC1 的存储密度可提高到约 20 位参数，总计需要 30 个定制的 HC1。该方案的整体处理速度可达每用户每秒 12000 个 token。考虑到 30 颗芯片的成本约为每百万 token 7.6 美分，该方案的成本不到同等吞吐量 GPU 方案的一半。即使假设 GPU 更新周期为四年，而 HC1 每年都需要重新更换，其总成本依然具备显著优势。

### AMD前高管“梦之队”与未来规划

Taalas 成立于两年前，其创始团队堪称“AMD前高管梦之队”：
*   **Ljubiša Bajić**：AMD 前集成电路设计总监。他曾在 AMD 和英伟达担任高级职位，负责高性能 GPU 研发设计，同时也是知名 AI 芯片公司 Tenstorrent 的创始人兼首任 CEO（2020年芯片教父 Jim Keller 加入 Tenstorrent 接任 CEO 后，他转任 CTO，随后创立了 Taalas）。
*   **Leila Bajić**：AMD/ATI/Altera 前技术经理和工程师。
*   **Drago Ignjatović**：AMD 前 ASIC 设计总监。

公司致力于开发专为 AI 推理和训练设计的全新架构，强调分层设计和晶格网络，使芯片能像大脑一样根据任务需求动态处理数据。Taalas 试图通过类似硅基编译器的方式，直接将 AI 模型转化为硅芯片。

这支仅有 24 名成员的团队，在产品投入仅 3000 万美元的情况下首战告捷，创造出了比通用 AI 芯片高出几个数量级的能效比。

目前，Taalas 已筹集 2 亿美元投资，并公布了后续路线图：
*   **今年春季**：预计发布基于 HC1 的第二代变体，将集成一款中等规模的推理大模型。
*   **今年冬季**：预计部署上线 HC2，密度更高、运行速度更快。

### 网友评价两极分化

尽管 HC1 性能惊人，但外界对其评价却呈现两极分化：

*   **看好者**：认为 HC1 的超低延迟将极大地有益于推动具身智能等领域的发展。
*   **质疑者**：有网友实测后发现，HC1 高速推理的背后是糟糕的推理深度。此外，在当前大模型迭代周期极快的背景下，HC1 的硬编码模式可能会导致芯片极易过时——这也是当前主流芯片厂商普遍坚持推出通用型芯片的核心原因之一。

## 关联主题
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/lora]]
