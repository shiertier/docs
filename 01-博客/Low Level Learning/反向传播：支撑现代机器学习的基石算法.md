---
title: "反向传播：支撑现代机器学习的基石算法"
发布日期: "2026-02-19"
原文链接: "https://lowleveldesign.io/News/backpropagation-intuition-explained-2026-02-19"

来源: "用户提供的字幕转写"
作者: "Low Level Learning"
---

## 摘要

**1) 一句话总结**
反向传播是现代机器学习训练的基石算法，其核心是通过在计算图上系统应用链式法则高效计算参数梯度，并结合梯度下降机制最小化损失函数，从而实现模型的持续优化。

**2) 关键要点**
*   **广泛应用**：从 GPT 到 AlphaFold，尽管底层架构和数据不同，现代机器学习系统大多依赖反向传播算法进行训练。
*   **历史节点**：1970 年 Seppo Linnainmaa 提出了现代形式的高效导数计算方法；1986 年 Rumelhart、Hinton 与 Williams 将其成功应用于多层感知器训练。
*   **优化目标**：模型训练的本质是最小化损失函数（如均方误差 MSE），即通过调整参数让模型的预测值尽可能贴近真实数据。
*   **梯度下降**：利用函数的可微性计算梯度（偏导数集合），参数按照梯度的反方向（即损失下降最快的方向）结合学习率进行迭代更新。
*   **链式法则**：面对复杂的神经网络，反向传播利用微积分的链式法则，将复杂的全局求导拆解为一系列简单的局部求导过程。
*   **计算图机制**：计算过程可抽象为节点图。前向传播顺向计算出损失值；反向传播则逆向传递，通过“上游梯度 × 本节点局部导数 = 下游梯度”的规则算出所有参数的梯度。
*   **计算高效性**：反向传播之所以高效，是因为它在逆向遍历时复用了前向传播产生的中间结果，且每个节点只进行局部的规则化计算（如加法节点原样分配梯度，乘法节点交叉相乘）。
*   **标准训练循环**：整个机器学习的训练过程被概括为“前向传播 → 反向传播 → 参数更新 → 重复”的闭环。

**3) 风险与不足**
*   **生物学合理性缺失**：从生物大脑学习机制的角度来看，反向传播常被认为缺乏生物学合理性（biologically implausible）。

## 正文

从 GPT、Midjourney 到 AlphaFold，尽管这些机器学习系统解决的问题、底层架构和训练数据截然不同，但它们大多都依赖同一个核心算法：反向传播（Backpropagation）。

反向传播是现代机器学习训练的基石：它让人工神经网络能“学会”任务；但从生物学习机制的角度看，它也常被认为缺乏生物学合理性（biologically implausible）。这篇文章聚焦反向传播在人工系统中的直观理解：它为何有效，以及你如何从头推导出它。

## 反向传播的历史里程碑

反向传播很难说有单一“发明者”。一些数学思想可追溯到更早的微积分传统；而在现代计算里，常被提及的关键节点包括：

- 1970 年：Seppo Linnainmaa 在硕士论文中给出了现代形式的高效导数计算方法（当时并未以“神经网络训练”作为主要叙事）。
- 1986 年：Rumelhart、Hinton 与 Williams 将反向传播用于多层感知器训练，并展示它能在隐藏层学到有意义的表示。

此后，模型的架构不断演进、规模不断扩大，但训练时“算梯度 → 更新参数”的核心机制延续至今。

## 从一个优化问题开始：曲线拟合与损失函数

先从一个更直观的任务出发：你在平面上收集了一堆点 $(x, y)$，想用一条曲线描述它们的关系。为了让问题可解，我们先限制曲线族，比如用一个五次多项式：

$$
\hat y(x)=k_0+k_1x+k_2x^2+k_3x^3+k_4x^4+k_5x^5
$$

问题变成：如何找到一组系数 $k_0,\dots,k_5$，让曲线“最好地拟合”数据？

要让“最好”可计算，我们需要一个数值标准。一个常见选择是用误差的平方来衡量（在实践中常见的形式是均方误差 MSE 或其等价变体）：

- 误差大：曲线离数据点远，拟合差。
- 误差小：曲线贴近数据点，拟合好。

把这个误差标准写成函数，就是损失函数 $L(k_0,\dots,k_5)$。注意这里有两类“函数”不要混淆：

- 曲线本身 $\hat y(x)$：输入 $x$，输出预测值 $\hat y$。
- 损失函数 $L$：输入一组参数（系数），输出一个标量损失。

训练/拟合，本质上就是最小化损失函数。

## 用导数“看见”下降方向：梯度与梯度下降

想象一台“曲线拟合器”，有 6 个旋钮分别控制 $k_0\dots k_5$。你当然可以靠“随机试错”：拧一下某个旋钮看看损失变大还是变小。但这种方法效率很低。

更聪明的做法是利用可微性：在当前参数位置，直接计算损失对每个旋钮的“敏感度”。在一维情况下，这就是导数；在多维情况下，这就是偏导数组成的梯度：

$$
\nabla L = \left(\frac{\partial L}{\partial k_0},\dots,\frac{\partial L}{\partial k_5}\right)
$$

梯度指向损失上升最快的方向，因此要最小化损失，就沿着梯度的反方向走。最常见的更新形式写作：

$$
\theta \leftarrow \theta - \eta \nabla L(\theta)
$$

其中 $\theta$ 表示所有参数的集合，$\eta$ 是学习率。

## 链式法则：把复杂求导拆成局部求导

困难在于：真实模型里的损失函数非常复杂，我们怎么高效算出这些偏导数？

关键工具是链式法则。若 $y=f(u)$ 且 $u=g(x)$，那么：

$$
\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}
$$

直觉上，你可以把它理解成“影响的传递”：$x$ 先影响 $u$，$u$ 再影响 $y$；因此总影响等于两段影响的乘积。只要把复杂计算拆成一串简单计算，每个简单计算的“局部导数”都容易写出，那么整体导数就能按链式法则组合出来。

## 计算图：前向传播与反向传播

把“损失怎么由参数和数据算出来”的过程画成图，每个节点是一个简单运算（加法、乘法、幂、激活函数等），这就是计算图。

### 前向传播：算出损失

前向传播就是沿着计算图把数算出来：从参数和数据出发，得到预测 $\hat y$，再得到损失 $L$。

### 反向传播：算出梯度

反向传播做的是同一张图上的“求导”。它从损失 $L$ 出发，反方向把梯度一路传回去，得到每个参数对损失的偏导数。

反向传播之所以高效，是因为它复用前向传播中间结果，并且在每个节点只做局部的、规则化的计算。可以用一句话概括每个节点在反传时做什么：

> 上游梯度 × 本节点局部导数 = 下游梯度

举两个常见局部规则：

- 加法节点 $z=a+b$：$\frac{\partial z}{\partial a}=1,\ \frac{\partial z}{\partial b}=1$，所以上游梯度会原样分配到两个输入。
- 乘法节点 $z=a\cdot b$：$\frac{\partial z}{\partial a}=b,\ \frac{\partial z}{\partial b}=a$，所以上游梯度会分别乘以“另一边的值”再传回去（这是“局部导数”的特例形式）。

当一个中间变量分叉流向多个后续计算时，它对损失的影响来自多个路径，梯度会在该点相加。

## 把梯度用起来：训练循环

一旦我们有了 $\nabla L$，就可以做一次参数更新；然后在新的参数处重新做一次前向/反向，继续更新。训练循环可以概括为：

前向传播 → 反向传播 → 参数更新 → 重复。

对神经网络来说，哪怕结构很复杂，本质上仍是一连串可微运算的组合；因此同一套“计算图 + 链式法则 + 梯度下降”的框架就能通用地工作。

## 小结

反向传播并不神秘：它本质上是在计算图上系统地应用链式法则，用一次反向遍历高效求出所有参数的梯度；而梯度下降（及其变体）则把这些梯度变成“该往哪儿更新”的具体操作。

至于“这套算法与生物大脑到底像不像”，就涉及神经科学中的突触可塑性等机制了；那是另一个值得单独展开的话题。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/数学]]
- [[00-元语/learning-resource]]
