---
title: "OpenAI 绕过英伟达：在“餐盘大小”的芯片上推出超快编程模型"
---

## 摘要

**1) 一句话总结**
OpenAI 首次在非英伟达硬件（Cerebras 芯片）上推出了专为编程任务微调的超快 AI 模型 GPT-5.3-Codex-Spark，其生成速度超过 1,000 Token/秒，标志着 OpenAI 在大幅提升推理速度的同时，正加速摆脱对英伟达的硬件依赖。

**2) 关键要点**
*   **首发非英伟达硬件模型**：GPT-5.3-Codex-Spark 运行在 Cerebras 的 Wafer Scale Engine 3 芯片上，这是双方于今年 1 月宣布合作后诞生的首款产品。
*   **极致生成速度**：代码生成速度超过 1,000 Token/秒，比上一代产品快约 15 倍，远超 OpenAI 现有模型在英伟达硬件上的速度（如 GPT-4o 约 147 Token/秒）。
*   **产品定位与取舍**：Spark 基于完整版 GPT-5.3-Codex 构建，它放弃了处理通用任务和知识深度，是一个专为编程任务微调的纯文本模型，以追求极致的低延迟。
*   **可用性与规格**：目前作为研究预览版向 ChatGPT Pro 订阅用户（200美元/月）开放；具备 128,000 个 Token 的上下文窗口；可通过 Codex 应用程序、CLI 和 VS Code 扩展使用，API 正向部分合作伙伴开放。
*   **基准测试表现**：在 SWE-Bench Pro 和 Terminal-Bench 2.0 测试中，Spark 的表现优于旧版 GPT-5.1-Codex-mini，且完成任务耗时极短。
*   **多元化硬件战略**：OpenAI 正在系统性减少对英伟达的依赖，具体举措包括：2025年10月与 AMD 签署多年协议、2025年11月与亚马逊达成 380 亿美元云协议，以及设计计划交由台积电代工的定制 AI 芯片。

**3) 风险与不足**
*   **缺乏独立验证**：OpenAI 尚未分享 Spark 在软件工程基准测试中表现的独立验证结果。
*   **能力与准确性妥协**：Spark 的体量和综合能力不及竞争对手 Anthropic 的 Claude Opus 4.6，且追求极致速度可能会以牺牲一定的准确性为代价。
*   **系统开销限制**：尽管速度极快，但 1,000 Token/秒仍低于 Cerebras 在其他模型上测得的 2,100 至 3,000 Token/秒，反映了 Spark 作为复杂模型带来的系统开销。
*   **合作协议停滞**：OpenAI 原计划与英伟达达成的 1,000 亿美元基础设施协议目前已无下文，且据报道 OpenAI 对部分英伟达芯片在推理任务上的速度感到不满。

## 正文

OpenAI 于周四发布了其首个在非英伟达（Nvidia）硬件上运行的生产级 AI 模型——全新的编程模型 **GPT-5.3-Codex-Spark**。该模型部署在 Cerebras 的芯片上，代码生成速度超过每秒 1,000 个 Token，据称比其上一代产品快约 15 倍。

相比之下，Anthropic 的 Claude Opus 4.6 在其全新推出的高价位“快速模式”下，速度也仅达到其标准速度（每秒 68.2 个 Token）的 2.5 倍。虽然 Spark 的体量和综合能力不及 Opus 4.6，但其速度优势极为显著。

OpenAI 计算主管 Sachin Katti 在一份声明中表示：“Cerebras 是一位出色的工程合作伙伴，我们很高兴能将快速推理作为一项新的平台能力加入进来。”

### 模型特性与可用性

目前，Codex-Spark 作为研究预览版向公众开放：
*   **适用人群**：ChatGPT Pro 订阅用户（200美元/月）。
*   **使用渠道**：可通过 Codex 应用程序、命令行界面（CLI）以及 VS Code 扩展使用。同时，OpenAI 正向部分设计合作伙伴逐步开放 API 访问权限。
*   **技术规格**：首发版本仅支持文本处理，具备 128,000 个 Token 的上下文窗口。

### 专为速度而生的编程利器

Spark 建立在本月早些时候发布的完整版 GPT-5.3-Codex 模型基础之上。完整版模型主要处理重量级的智能体编程任务，而 Spark 则在“知识深度”与“速度”之间果断选择了后者。它是一个纯文本模型，专为编程任务进行了微调，并不处理大型 GPT-5.3 模型所负责的通用任务。

据 OpenAI 称，在评估软件工程能力的两个基准测试（SWE-Bench Pro 和 Terminal-Bench 2.0）中，Spark 的表现优于旧版的 GPT-5.1-Codex-mini，且完成任务的时间仅为后者的一小部分（注：OpenAI 尚未分享这些数据的独立验证结果）。

从过往经验来看，Codex 的速度曾是一个痛点。去年 12 月，在测试四个 AI 编程智能体构建《扫雷》克隆游戏时，Codex 生成可运行游戏的时间大约是 Anthropic 的 Claude Code 的两倍。

### 编程智能体的军备竞赛

每秒 1,000 个 Token 的速度，对于 OpenAI 过去在自有基础设施上提供的任何服务来说，都是一次巨大的飞跃。根据独立基准测试，OpenAI 在英伟达硬件上运行的最快模型远低于这一水平：GPT-4o 约为 147 Token/秒，o3-mini 约为 167 Token/秒，GPT-4o mini 约为 52 Token/秒。

然而，按照 Cerebras 的标准，每秒 1,000 个 Token 其实相对“保守”。该公司曾在 Llama 3.1 70B 上测得 2,100 Token/秒的速度，并在 OpenAI 自家的开源权重模型 gpt-oss-120B 上报告了 3,000 Token/秒的速度。这表明 Codex-Spark 相对较低的速度，反映了其作为更大或更复杂模型所带来的系统开销。

AI 编程智能体迎来了爆发的一年。OpenAI、谷歌和 Anthropic 都在竞相推出更强大的编程智能体，而**延迟（Latency）**已成为决定胜负的关键——模型写代码越快，开发者的迭代速度就越快。

面对 Anthropic 的激烈竞争，OpenAI 一直在快速迭代其 Codex 产品线。在 CEO Sam Altman 针对谷歌的竞争压力发布内部“红色警报”备忘录后，OpenAI 于 12 月发布了 GPT-5.2，并在几天前刚刚推出了 GPT-5.3-Codex。

### 摆脱对英伟达的依赖

相比于基准测试的分数，Spark 背后的硬件布局可能具有更深远的影响。该模型运行在 Cerebras 的 Wafer Scale Engine 3 上，这是一款“餐盘大小”的芯片，也是 Cerebras 自 2022 年以来的核心业务基础。OpenAI 与 Cerebras 于今年 1 月宣布合作，Codex-Spark 正是双方合作诞生的首款产品。

过去一年里，OpenAI 一直在系统性地减少对英伟达的依赖：
*   **2025年10月**：与 AMD 签署了大规模的多年合作协议。
*   **2025年11月**：与亚马逊达成了价值 380 亿美元的云计算协议。
*   **自研芯片**：一直在设计定制的 AI 芯片，计划最终交由台积电（TSMC）代工。

与此同时，一项原计划与英伟达达成的 1000 亿美元基础设施协议目前已无下文（尽管英伟达随后承诺了 200 亿美元的投资）。据报道，OpenAI 对某些英伟达芯片在推理任务上的速度感到不满，而推理任务正是 OpenAI 设计 Codex-Spark 的核心应用场景。

无论底层使用的是哪种芯片，速度都至关重要，尽管这可能会以牺牲一定的准确性为代价。对于那些整天在代码编辑器里等待 AI 建议的开发者来说，每秒 1,000 个 Token 的速度可能不再像是小心翼翼地操作曲线锯，而更像是开启了一把狂野的纵切锯——使用时，请务必看清你正在切割什么。

## 相关文档

- [[01-博客/OpenAI/推出 GPT-5.3-Codex-Spark：专为实时编程打造的超快模型|推出 GPT-5.3-Codex-Spark：专为实时编程打造的超快模型]]；关联理由：同一事件；说明：两文都围绕 Codex-Spark 首次发布，本文是媒体解读，该文提供官方技术细节与可用性范围。
- [[01-博客/OpenAI/介绍 GPT-5.3-Codex：迄今最强大的智能体编程模型|介绍 GPT-5.3-Codex：迄今最强大的智能体编程模型]]；关联理由：版本演进；说明：本文将 Spark 与完整版 GPT-5.3-Codex 对照，该文补充其上游主模型能力与基准背景。
- [[01-博客/Simon Willison/GPT-5.3-Codex-Spark 处理速度显著提升|GPT-5.3-Codex-Spark 处理速度显著提升]]；关联理由：版本演进；说明：该文记录 Spark 后续提速至 1200+ Token/秒，可对照本文发布时 1000 Token/秒的初始性能基线。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/Codex]]
- [[00-元语/Agent]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/llmops]]
