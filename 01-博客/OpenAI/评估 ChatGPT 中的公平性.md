---
title: "评估 ChatGPT 中的公平性"
---

## 摘要

**1) 一句话总结**
本研究通过语言模型研究助手（LMRA）分析了数百万次真实对话，发现 ChatGPT 在处理带有不同性别或种族属性的用户名字时，回复质量保持一致，且产生有害刻板印象的概率极低（约 0.1%）。

**2) 关键要点**
*   **研究焦点：** 侧重于“第一人称公平性”，即评估用户提供的名字（通常带有文化、性别和种族属性）如何直接影响 ChatGPT 的回复内容。
*   **隐私保护与分析方法：** 使用 GPT-4o 作为“语言模型研究助手”（LMRA）分析数百万次真实请求的模式，研究人员仅获取趋势数据而不接触具体聊天记录。
*   **评估一致性：** 在性别刻板印象评估上，LMRA 与人类评分者的一致率超过 90%。
*   **回复质量一致：** 无论用户的名字带有何种背景含义，ChatGPT 在各组之间的准确率和幻觉率等质量指标均保持一致。
*   **偏见发生率极低：** 总体案例中，被评估为反映有害刻板印象的回复比例仅约为 0.1%。
*   **开放式任务更易受影响：** 回复较长的开放式任务（如“写一个故事”）比其他提示词更容易包含有害刻板印象。
*   **新模型表现更优：** GPT-3.5 Turbo 表现出的偏见水平最高，而较新的模型在所有任务中的偏见率均低于 1%。
*   **成果应用：** 该测量方法已纳入标准模型性能评估套件，官方同时开源了详细的系统提示词以支持外部可重复性研究。

**3) 风险与不足**
*   **LMRA 准确性局限：** LMRA 在检测种族和民族刻板印象时，与人类评分者的一致率较低，未来需进一步界定有害刻板印象并提升检测准确性。
*   **样本与文化局限：** 本研究仅限于英语交互、基于美国常见名字的二元性别关联，以及四种主要种族和民族（黑人、亚裔、西班牙裔和白人），未涵盖更广泛的语言、文化背景及其他人口统计特征。
*   **模态局限：** 研究目前仅涵盖文本交互，未涉及其他交互方式。
*   **潜在的累积危害：** 尽管有害刻板印象的发生率极低（单用户难以察觉），但在庞大的总体使用量下，罕见的模式仍可能产生有害影响。

## 正文

我们分析了 ChatGPT 如何根据用户的名字做出响应，并在此过程中使用语言模型研究助手来保护隐私。

开发模型不仅需要数据，我们还精心设计了训练过程以减少有害输出并提高实用性。研究表明，语言模型有时仍会吸收并重复训练数据中的社会偏见，例如性别或种族刻板印象。

在这项研究中，我们探讨了关于用户身份的细微线索（如名字）将如何影响 ChatGPT 的回复。这一点非常重要，因为人们使用聊天机器人的方式多种多样（从起草简历到寻求娱乐建议），这与传统 AI 公平性研究的场景（如筛选简历或信用评分）有所不同。

以往的研究主要集中在“第三人称公平性”（即机构使用 AI 对他人做出决策），而本研究考察的是“第一人称公平性”，即偏见如何直接影响 ChatGPT 的用户。作为起点，我们测量了在**请求完全相同**的情况下，ChatGPT 对不同用户名字的感知会如何影响其对每个用户的回复。名字通常带有文化、性别和种族属性，这使其成为调查偏见的重要因素——尤其是用户在起草电子邮件等任务时经常与 ChatGPT 分享他们的名字。除非用户关闭了记忆功能，否则 ChatGPT 会在跨对话中记住名字等信息。

为了将研究重点放在公平性上，我们考察了使用名字是否会导致**反映有害刻板印象的回复**。虽然我们期望并希望 ChatGPT 能够根据用户偏好定制回复，但我们希望它在这样做的同时不会引入有害的偏见。

### 研究方法

为了测量刻板印象差异是否会在极小概率下发生（超出纯偶然的预期），我们研究了 ChatGPT 在数百万次真实请求中的回复情况。为了在了解真实世界使用情况的同时保护隐私，我们指示一个语言模型（GPT-4o）分析大量真实 ChatGPT 对话记录中的模式，并在研究团队内部分享这些趋势（而不是底层的具体聊天内容）。通过这种方式，研究人员能够分析和了解真实世界的趋势，同时维护聊天的隐私。我们在论文中将该模型称为“语言模型研究助手”（LMRA），以区别于在 ChatGPT 中生成我们所研究聊天的语言模型。

为了验证语言模型的评分是否与人类评分者的意见一致，我们让语言模型和人类评分者评估了相同的公开聊天记录。结果显示，在性别方面，语言模型给出的答案与人类评分者的一致率超过 90%；而在种族和民族刻板印象方面，一致率较低。LMRA 检测到的有害种族刻板印象比例低于与性别相关的比例。未来还需要进一步的工作来界定有害刻板印象并提高 LMRA 的准确性。

### 研究发现

*   **回复质量一致：** 当 ChatGPT 知道用户的名字时，无论该名字带有何种性别或种族含义，它都能提供同等高质量的答案（例如，各组之间的准确率和幻觉率保持一致）。
*   **有害刻板印象发生率极低：** 名字与性别、种族或民族的关联确实会导致回复出现差异，但被语言模型评估为反映有害刻板印象的比例在总体案例中仅约为 0.1%（在旧版模型的某些领域中最高约为 1%）。
*   **开放式任务更易受影响：** 在各个领域中，LMRA 识别出最常出现有害刻板印象的任务。回复较长的开放式任务更有可能包含有害刻板印象。例如，“写一个故事”被发现包含刻板印象的频率高于测试的任何其他提示词。
*   **新模型表现更好：** 虽然刻板印象的发生率很低（在所有领域和任务中平均不到千分之一），但我们的评估可以作为一个基准，用于衡量我们随着时间的推移在降低这一比例方面的成效。按任务类型划分并评估各模型的任务级偏见时，我们发现表现出最高偏见水平的模型是 GPT-3.5 Turbo，而较新的模型在所有任务中的偏见率均低于 1%。
*   **回复差异的具体表现：** LMRA 提出了关于各项任务中差异具体是什么的自然语言解释。它强调了 ChatGPT 在所有任务的回复中，偶尔会在语气、语言复杂性和细节程度上存在差异。除了一些明显的刻板印象外，这些差异还包括一些部分用户可能欢迎而其他用户可能反感的内容。例如，在“写一个故事”任务中，对听起来像女性名字的用户的回复，比对听起来像男性名字的用户的回复更常出现女性主角。

尽管单个用户不太可能注意到这些差异，但我们认为测量和理解它们非常重要，因为即使是罕见的模式，在总体上也可能是有害的。这种方法也为我们提供了一种在统计上跟踪随时间变化的新途径。我们为本研究创建的研究方法，也可以推广到研究 ChatGPT 中除名字之外的其他偏见。

### 研究局限性

了解语言模型中的公平性是一个庞大的研究领域，我们承认我们的研究存在局限性。并非所有人都会分享自己的名字，除了名字之外的其他信息也可能对 ChatGPT 的第一人称公平性产生影响。本研究主要集中在英语交互、基于美国常见名字的二元性别关联，以及四种主要种族和民族（黑人、亚裔、西班牙裔和白人）。此外，本研究仅涵盖文本交互。虽然我们认为该方法向前迈出了一步，但要了解与其他人口统计特征、语言和文化背景相关的偏见，还有更多工作要做。我们计划在此研究的基础上进一步广泛地提升公平性。

### 结论与未来展望

尽管很难将有害的刻板印象归结为一个单一的数字，但我们相信，开发测量和理解偏见的新方法，是随着时间的推移跟踪和减轻偏见的重要一步。我们在此次研究中使用的方法现已成为我们标准模型性能评估套件的一部分，并将为未来系统的部署决策提供参考。这些经验也将支持我们进一步阐明系统中公平性的操作含义。公平性仍然是一个活跃的研究领域。

我们相信，透明度和持续改进是解决偏见以及与用户和更广泛的研究社区建立信任的关键。为了支持可重复性和进一步的公平性研究，我们还分享了本研究中使用的详细系统提示词，以便外部研究人员可以进行他们自己的第一人称偏见实验。

我们欢迎反馈与合作。如果您有任何见解或希望与我们合作改善 AI 的公平性，我们非常乐意倾听您的意见；如果您希望与我们一起专注于解决这些挑战，我们的安全系统与准备团队正在招聘相关人才。

## 相关文档

- [[01-博客/OpenAI/OpenAI 的人工智能对齐研究方法|OpenAI 的人工智能对齐研究方法]]；关联理由：解说；说明：该文系统解释了通过人类反馈与评估机制降低偏见和有害输出的对齐方法，可作为本文公平性评测结论的方法论背景。
- [[01-博客/OpenAI/前沿AI风险与防范准备|前沿AI风险与防范准备]]；关联理由：延伸思考；说明：本文聚焦名字线索导致的刻板印象风险，彼文补充了前沿模型风险评估与治理框架，形成从微观偏见到宏观治理的延伸。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/ChatGPT]]
- [[00-元语/AI]]
- [[00-元语/alignment]]
- [[00-元语/llm]]
- [[00-元语/evals]]
- [[00-元语/risk]]
