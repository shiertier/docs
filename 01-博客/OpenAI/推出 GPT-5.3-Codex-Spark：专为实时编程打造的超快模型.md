---
title: "推出 GPT-5.3-Codex-Spark：专为实时编程打造的超快模型"

来源: "https://openai.com/index/introducing-gpt-5-3-codex-spark"
---

## 摘要

### 一句话总结
OpenAI 推出了专为实时编程打造的轻量化模型 GPT-5.3-Codex-Spark，该模型基于 Cerebras 硬件运行，生成速度超过 1000 Token/秒，现已向 ChatGPT Pro 用户开放研究预览。

### 关键要点
*   **核心定位**：作为 GPT-5.3-Codex 的轻量化版本，专为实时协作与编程设计，支持快速迭代、打断或重定向任务。
*   **极致速度与性能**：生成速度超过 1000 Token/秒；在 SWE-Bench Pro 和 Terminal-Bench 2.0 基准测试中表现优异，且耗时远低于原版模型。
*   **底层延迟优化**：通过引入持久 WebSocket 连接及重写推理栈，使首个 Token 生成时间缩短 50%，客户端/服务器单次往返开销降低 80%，单 Token 处理开销降低 30%。
*   **硬件架构**：运行于专为高速推理打造的 Cerebras Wafer Scale Engine 3 加速器上，与现有的 GPU 基础架构形成互补。
*   **模型规格**：首发版本具备 128k 上下文窗口。
*   **工作流特性**：默认保持轻量化，仅进行最小幅度的针对性修改，且除非用户主动要求，否则不会自动运行测试。
*   **可用性**：现已向 ChatGPT Pro 用户（涵盖 Codex 应用、CLI 及 VS Code 扩展）开放，并向少数设计合作伙伴提供 API 访问。
*   **安全性**：经过与主线模型相同的安全训练与评估，在网络安全或生物学领域的高风险能力极低，远未触及备灾框架阈值。

### 风险与局限（Risks/Gaps）
*   **访问受限与排队风险**：由于运行在专用的低延迟硬件上，该模型采用独立的速率限制；在需求较高时，用户可能会遇到访问受限或短暂排队的情况。
*   **输入模态局限**：发布初期，该模型仅支持文本输入，尚未支持多模态输入。

## 正文

今天，我们正式发布 **GPT-5.3-Codex-Spark** 的研究预览版。作为 GPT-5.3-Codex 的轻量化版本，这是我们首款专为实时编程设计的模型。

Codex-Spark 标志着我们在今年 1 月宣布与 Cerebras 达成合作后的首个重要里程碑。该模型针对超低延迟硬件进行了优化，在处理实际编程任务时不仅保持了强大的能力，还能提供每秒超过 1000 个 Token 的生成速度，带来近乎瞬时的交互体验。

目前，我们已在 Cerebras 平台上向 ChatGPT Pro 用户开放 Codex-Spark 的研究预览。这让开发者能够尽早开始尝试，同时我们也将继续与 Cerebras 合作，提升数据中心容量、优化端到端的用户体验，并部署我们更大规模的前沿模型。

### 专为实时协作与编程设计

我们最新的前沿模型在处理长时间运行的任务时表现出了显著的优势，能够在无需人工干预的情况下自主工作数小时、数天甚至数周。而 Codex-Spark 则是我们首款专为**实时工作**设计的模型——你可以用它进行针对性的代码修改、重构逻辑或优化接口，并立即看到结果。

随着 Codex-Spark 的加入，Codex 现在既能支持耗时较长的宏大任务，也能满足即时的工作需求。

**速度与智能的结合**
Codex-Spark 专为交互式工作流优化，在这种场景下，低延迟与模型的智能程度同等重要。你可以与模型进行实时协作，在它工作时打断或重定向任务，并通过近乎瞬时的响应进行快速迭代。为了追求极致速度，Codex-Spark 默认保持轻量化的工作方式：它只进行最小幅度的针对性修改，并且除非你主动要求，否则不会自动运行测试。

**卓越的编程性能**
作为一款针对快速推理优化的高性能小模型，GPT-5.3-Codex-Spark 在评估代理式软件工程能力的两个基准测试（SWE-Bench Pro 和 Terminal-Bench 2.0）中展现了强大的性能，且完成任务所需的时间仅为 GPT-5.3-Codex 的一小部分。

### 突破性的延迟优化：惠及所有模型

在训练 Codex-Spark 的过程中我们发现，要实现真正的实时协作，仅仅提升模型本身的速度是不够的，我们还需要降低整个“请求-响应”数据链路的延迟。

为此，我们在底层架构中实施了端到端的延迟优化，这些改进最终将惠及所有模型。我们在底层简化了响应从客户端到服务器再返回的流式传输方式，重写了推理栈的关键部分，并重新设计了会话初始化的方式。这使得首个可见的 Token 能够更快出现，并确保 Codex 在你迭代代码时始终保持响应。

通过引入持久的 WebSocket 连接以及对 Responses API 进行针对性优化，我们实现了以下突破：
*   客户端/服务器单次往返的开销降低了 **80%**
*   每个 Token 的处理开销降低了 **30%**
*   首个 Token 的生成时间（time-to-first-token）缩短了 **50%**

目前，WebSocket 路径已默认在 Codex-Spark 中启用，并将在不久后成为所有模型的默认设置。

### 由 Cerebras 提供硬件支持

Codex-Spark 运行在 Cerebras 的 Wafer Scale Engine 3 上。这是一款专为高速推理打造的 AI 加速器，为 Codex 提供了“延迟优先”的服务层级。我们与 Cerebras 合作，将这一低延迟路径无缝集成到了我们现有的生产服务架构中，这不仅适用于当前的 Codex，也为支持未来的模型奠定了基础。

> “GPT-5.3-Codex-Spark 最让我们感到兴奋的，是能够与 OpenAI 及开发者社区携手，共同探索快速推理所带来的可能性——全新的交互模式、新的应用场景，以及一种截然不同的模型体验。这次预览仅仅是个开始。”
> —— Sean Lie，Cerebras 联合创始人兼 CTO

在我们的训练和推理流程中，GPU 依然是基础，能为广泛的使用场景提供最具成本效益的 Token 生成。而 Cerebras 则完美补充了这一基础，它在需要极低延迟的工作流中表现出色，能够缩短端到端的循环时间，让 Codex 在迭代时感觉更加灵敏。在单一工作负载中，GPU 和 Cerebras 甚至可以结合使用，以达到最佳性能。

### 规格、可用性与安全性

*   **可用性：** Codex-Spark 研究预览版今日起向 ChatGPT Pro 用户开放，支持最新版本的 Codex 应用程序、CLI 以及 VS Code 扩展。此外，我们还通过 API 向少数设计合作伙伴开放了 Codex-Spark，以了解开发者希望如何将其集成到自己的产品中。未来几周，我们将根据实际工作负载继续优化集成，并逐步扩大访问权限。
*   **使用限制：** 由于该模型运行在专用的低延迟硬件上，其使用受独立的速率限制管理。在研究预览期间，该限制可能会根据需求进行调整。当需求较高时，为了平衡所有用户的稳定性，您可能会遇到访问受限或短暂排队的情况。（注：Codex-Spark 的使用量不计入标准速率限制）。
*   **模型规格：** 发布初期，Codex-Spark 仅支持文本输入，上下文窗口为 128k。它是我们超快模型系列的第一款产品。未来，随着我们与开发者社区共同探索快速模型在编程领域的最佳应用场景，我们将引入更多功能，包括更大的模型、更长的上下文以及多模态输入。
*   **安全性：** Codex-Spark 接受了与我们主线模型相同的安全训练（包括网络安全相关的训练）。我们按照标准部署流程对其进行了评估，确定其在网络安全或生物学领域达到高风险能力的几率极低，远未触及我们的“备灾框架（Preparedness Framework）”阈值。

### 未来展望

Codex-Spark 迈出了打造“双模式 Codex”的第一步：一种模式负责长周期的推理与执行，另一种模式负责快速迭代的实时协作。

随着时间的推移，这两种模式将会融合。Codex 可以在保持紧密交互循环的同时，将耗时较长的工作委托给后台的子代理（sub-agents），或者在你需要兼顾广度与速度时，将任务并行分发给多个模型。这意味着你不再需要提前在两种模式之间做单项选择。

随着模型变得越来越强大，交互速度已经成为一个明显的瓶颈。超快推理打破了这一循环限制，让 Codex 的使用体验更加自然，也为每一个想要将创意转化为实际软件的人拓展了无限可能。

## 相关文档

- [[01-博客/OpenAI/介绍 GPT-5.3-Codex：迄今最强大的智能体编程模型|介绍 GPT-5.3-Codex：迄今最强大的智能体编程模型]]；关联理由：上下游；说明：本文将 Spark 定位为 GPT-5.3-Codex 的实时轻量化分支，该文补齐主模型能力与基准背景。
- [[01-博客/OpenAI/GPT-5.3-Codex 系统卡：迄今最强大的代理编程模型|GPT-5.3-Codex 系统卡：迄今最强大的代理编程模型]]；关联理由：解说；说明：本文提到 Preparedness Framework 的低风险结论，该文提供同代模型的风险评估与防护细则。
- [[01-博客/OpenAI/超越速率限制：扩大 Codex 和 Sora 的访问规模|超越速率限制：扩大 Codex 和 Sora 的访问规模]]；关联理由：延伸思考；说明：本文提到 Spark 的独立限流与高峰排队，该文展开 Codex 访问控制与扩容机制的工程实现。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/Codex]]
- [[00-元语/Agent]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/rate-limiting]]
- [[00-元语/protocol]]
- [[00-元语/workflow]]
