---
title: "OpenAI Sora 系统卡：模型概览、风险评估与安全机制"
---

## 摘要

### 1) 一句话总结
OpenAI 发布的 Sora 系统卡详细阐述了该视频生成模型（最高支持 1080p、20秒）的技术架构、训练数据来源，以及涵盖模型层与产品层的多重安全缓解机制。

### 2) 核心要点
*   **模型架构**：Sora 结合了扩散模型与 Transformer 架构，以“视觉分块（Visual Patches）”为基本单位，并采用 DALL·E 3 的“重新描述”技术以提升文本指令遵循度。
*   **数据与预处理**：训练数据包含公开数据、专有合作数据（如 Shutterstock、Pond5）及人类反馈数据，所有数据集在训练前均经过严格的敏感内容过滤。
*   **多层安全技术栈**：部署了多模态审核分类器、定制化 LLM 异步过滤以及图像输出分类器，用于精准拦截违规的输入（文本/图像/视频）与输出。
*   **儿童安全机制**：集成 Thorn 的 Safer 系统检测 CSAM，并开发了专门的“18岁以下”分类器（识别真实儿童准确率达 97.86%），直接拒绝包含未成年人的图生视频请求。
*   **内容溯源与水印**：所有生成资产默认嵌入 C2PA 元数据；基于早期艺术家反馈，允许付费用户下载无可见水印的视频以优化工作流。
*   **艺术家与版权保护**：当提示词包含在世艺术家或公众人物姓名时，系统会触发 LLM 重写机制，将其替换为通用的属性描述。
*   **产品访问政策**：目前仅向 18 岁及以上用户开放，明确禁止未经许可的肖像使用、描绘真实未成年人及侵犯知识产权等行为。
*   **未来迭代方向**：计划小范围试点并监控基于真实人物照片/视频的“种子”生成功能，同时致力于减少模型在体型和人口统计学方面的输出偏见。

### 3) 风险与漏洞
*   **说服力风险（Persuasion）**：根据 OpenAI 的准备度框架，Sora 在网络安全、CBRN 或模型自主性方面无重大风险，但存在冒充、错误信息或社会工程学等说服力方面的潜在风险。
*   **对抗性提示词绕过**：外部红队测试发现，使用包含医疗场景或科幻/奇幻设定的提示词，有时会削弱模型针对色情内容的防护。
*   **工具滥用漏洞**：测试人员利用媒体上传功能和修改工具（如分镜、重剪、混合）尝试生成违规内容，暴露出输入和输出过滤机制中存在的漏洞。
*   **肖像滥用风险**：使用真实人物照片或视频作为“种子”生成视频的功能极易被滥用，初期需限制开放并禁止上传未成年人图像。

## 正文

Sora 是 OpenAI 推出的视频生成模型，旨在将文本、图像和视频输入转化为全新的视频输出。随着其强大生成能力的展现，OpenAI 发布了 Sora 系统卡（System Card），详细阐述了该模型的技术基础、潜在风险以及多层面的安全缓解措施。

### Sora 模型概览

Sora 允许用户创建最高 1080p 分辨率、最长 20 秒的多种格式视频。用户可以通过文本生成全新内容，或者对自有素材进行增强、重新混合与融合。此外，用户还可以在“精选（Featured）”和“近期（Recent）”动态中探索社区创作，获取灵感。

作为一款**扩散模型（Diffusion Model）**，Sora 的生成过程始于类似静态噪声的基础视频，通过多步去噪逐渐转化为清晰的影像。通过赋予模型同时预见多个帧的能力，Sora 成功解决了“主体在暂时离开视线后仍保持一致”的难题。与 GPT 模型类似，Sora 采用了 **Transformer 架构**，从而解锁了卓越的扩展性能。

Sora 还应用了源自 DALL·E 3 的“重新描述（Recaptioning）”技术，即为视觉训练数据生成高度详细的描述性字幕，这使得模型能够更忠实地遵循用户的文本指令。除了纯文本生成，Sora 还能精准地让静态图像动起来，或者对现有视频进行扩展和补帧。Sora 为理解和模拟真实世界的模型奠定了基础，OpenAI 认为这是实现 AGI（通用人工智能）的重要里程碑。

然而，这些能力也带来了新的风险，例如滥用肖像权、生成误导性或露骨内容。为此，Sora 的安全部署建立在 DALL·E 和 ChatGPT 等产品的安全经验之上。

### 模型数据与预处理

Sora 的灵感来自大型语言模型（LLM）。LLM 通过“Token”统一了文本、代码和数学等多种模态；而在 Sora 中，视觉数据的基本单位是**视觉分块（Visual Patches）**。

OpenAI 将视频压缩到低维潜在空间，随后将其分解为时空分块。Sora 的训练数据涵盖了多种来源：
*   **公开可用数据**：主要来自行业标准的机器学习数据集和网络抓取。
*   **专有合作数据**：通过合作伙伴（如 Shutterstock、Pond5）获取的非公开数据，以及定制创建的数据集。
*   **人类反馈数据**：来自 AI 训练员、红队测试人员和员工的反馈。

**预训练过滤：** 在训练之前，所有数据集都会经过严格的过滤，剔除最露骨、暴力或其他敏感内容（如仇恨符号），这构成了安全防御的第一道防线。

### 风险识别与外部红队测试

在 2024 年 2 月发布 Sora 后，OpenAI 与来自 60 多个国家的数百名视觉艺术家、设计师和电影制作人合作，收集反馈以优化模型。同时，OpenAI 开展了广泛的内部评估和**外部红队测试（Red Teaming）**。

来自 9 个国家的外部红队测试人员在 2024 年 9 月至 12 月期间，生成了超过 15,000 个视频，专门测试模型在色情、暴力、自残、非法内容和虚假信息等方面的漏洞。
*   **对抗性策略**：测试人员发现，使用包含医疗场景或科幻/奇幻设定的提示词，有时会削弱针对色情内容的防护。
*   **工具滥用**：测试人员还利用媒体上传功能和修改工具（如分镜、重剪、混合）来尝试生成违规内容，这帮助 OpenAI 发现了输入和输出过滤中的漏洞，并加强了对包含人物的媒体上传的保护。

### 早期艺术家访问的经验

在过去 9 个月中，OpenAI 分析了来自 60 多个国家、300 多名用户的 50 万次模型请求。
*   **水印政策调整**：艺术家反馈可见水印会限制他们的工作流。因此，OpenAI 决定允许付费用户下载无可见水印的视频，但仍会嵌入 C2PA 元数据。
*   **灵活性与安全的平衡**：作为专业创作工具，Sora 需要比 ChatGPT 提供更多的灵活性，但同时也需要在特定领域实施更严格的产品级限制以防止滥用。

### 评估与准备度框架

OpenAI 针对裸露、欺骗性选举内容、自残和暴力等关键领域开发了内部评估体系。评估数据来自早期 Alpha 阶段、红队测试的对抗性样本以及 GPT-4 生成的合成数据。

根据 OpenAI 的“准备度框架（Preparedness framework）”，Sora 在网络安全、CBRN（化学、生物、放射性和核）或模型自主性方面**没有表现出重大风险**。主要的潜在风险集中在**说服力（Persuasion）**方面，如冒充、错误信息或社会工程学。为此，OpenAI 重点构建了多层次的内容溯源方法（包括元数据、水印和指纹）。

### Sora 安全缓解技术栈

Sora 的安全机制分为系统/模型层面的技术缓解和产品政策两部分：

#### 1. 系统与模型缓解措施
*   **多模态审核分类器**：用于识别输入（文本、图像、视频）和输出中可能违规的内容，一旦发现违规将拒绝请求。
*   **定制 LLM 过滤**：利用视频生成所需的几秒钟时间，通过定制的 GPT 进行异步、高精度的多模态审核（如识别第三方内容和欺骗性内容）。
*   **图像输出分类器**：专门针对 NSFW（不适宜工作场所）内容、未成年人、暴力和肖像滥用进行拦截。
*   **黑名单**：维护涵盖多种类别的文本黑名单。

#### 2. 产品政策
*   目前仅向 **18 岁及以上**用户开放。
*   对“探索”和“精选”动态应用更严格的审核过滤。
*   明确禁止：未经许可使用他人肖像、描绘真实未成年人、侵犯知识产权、生成非自愿的私密图像、欺诈或误导性内容等。
*   提供用户举报机制，并结合自动化与人工审核进行违规处罚。

### 特定风险领域与应对

#### 儿童安全
OpenAI 优先防范和检测儿童性虐待材料（CSAM）。
*   **输入端**：集成 Thorn 开发的 Safer 系统检测已知 CSAM；使用多模态分类器审核涉及未成年人的性内容；开发了专门的“18岁以下”分类器，拒绝包含未成年人的图生视频请求。
*   **输出端**：如果文本提示涉及未成年人，系统会对输出应用极其严格的色情、暴力或自残审核阈值。输出分类器以每秒 2 帧的速度进行扫描。

*未成年人分类器评估表现：*
| 类别 | 预期结果 | 准确率 |
| :--- | :--- | :--- |
| 真实儿童 | 归类为“是儿童” | 97.86% |
| 真实成人 | 归类为“非儿童” | 99.28% |
| 虚构成人 | 归类为“非儿童” | 97.37% |
| 虚构儿童 | 归类为“非儿童” | 69.24% |
*(注：整体精确率为 80.95%，召回率为 97.86%)*

#### 裸露与暗示性内容
Sora 采用多层审核策略拦截 NSFW 内容。对于包含人物的图像上传，审核阈值比纯文本提示更严格。
*   **输入端准确率**：97.25%
*   **输出端（端到端）准确率**：97.59%

#### 欺骗性内容与深度伪造
*   **肖像滥用**：监控并标记试图修改或有害描绘真实人物的提示词。
*   **选举欺骗内容**：LLM 过滤器专门识别违规的选举相关输入（召回率 98.23%，精确率 88.80%）。
*   **内容溯源**：所有资产默认包含 C2PA 元数据和可见的 Sora 动画水印，并配备内部视频逆向搜索工具。

#### 艺术家风格保护
当用户在提示词中使用在世艺术家的名字时，Sora 会触发提示词重写机制。Sora 编辑器会利用 LLM 重写文本，移除公众人物姓名，并以通用方式描述特定属性和品牌对象。

### 未来工作

OpenAI 将采取迭代部署策略，持续优化 Sora：
1.  **肖像功能试点**：使用真实人物照片/视频作为“种子”生成视频的功能极具潜力但也易被滥用。该功能初期仅向部分用户开放，并进行深度监控（测试期间禁止上传未成年人图像）。
2.  **溯源与透明度**：继续研究逆向嵌入搜索工具，并扩大 C2PA 等透明度措施的实施。
3.  **扩大输出代表性**：致力于减少模型在体型、人口统计学等方面的输出偏见，确保内容更加平衡和包容。
4.  **持续的安全与伦理对齐**：根据最佳实践和用户反馈，不断改进肖像安全和防欺骗内容机制。

---

### 参考文献
1. OpenAI. *Video generation models as world simulators.* (2024)
2. OpenAI. *Upgrading the Moderation API with our new Multimodal Moderation model.* (2024)
3. OpenAI. *Child safety: Adopting SBD principles.* (2024)
4. OpenAI. *DALL·E 3 system card.* (2023)
5. Panić, N., et al. *Addressing demographic bias in age estimation models through optimized dataset composition.* Mathematics, 12(15), 2358. (2024)

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/video]]
- [[00-元语/multimodal]]
- [[00-元语/deepfake]]
- [[00-元语/risk]]
- [[00-元语/security]]
- [[00-元语/alignment]]
