---
title: "OpenAI 的人工智能对齐研究方法"
---

## 摘要

**1) 一句话总结**
OpenAI 采用迭代与实证的方法，通过基于人类反馈的训练、AI 辅助人类评估以及训练 AI 自主进行对齐研究这三大支柱，旨在构建一个能够自动解决通用人工智能（AGI）对齐问题的系统。

**2) 关键要点**
*   **核心三大支柱**：OpenAI 的对齐方法侧重于设计可扩展的训练信号，具体包括：使用人类反馈训练 AI、训练 AI 协助人类评估、训练 AI 进行对齐研究。
*   **RLHF 的高效性**：基于人类反馈的强化学习（RLHF）是当前主要技术。InstructGPT 的微调成本不到 GPT-3 预训练计算量的 2%，仅需约 20,000 小时的人类反馈，其表现比大 100 倍的预训练模型更受人类偏好。
*   **当前 RLHF 的局限**：目前的 InstructGPT 仍未完全对齐，存在无法遵循简单指令、产生偏见或有害回复、以及创造力下降等问题；RLHF 被认为是核心构建块，但不足以单独对齐 AGI。
*   **AI 辅助评估（RRM）**：随着模型能力超越人类评估水平，OpenAI 采用递归奖励建模（RRM）等技术训练 AI 协助人类。例如，AI 对自身输出撰写批评性评论，使人类发现的缺陷平均增加了 50%。
*   **自动化对齐研究**：由于目前没有无限可扩展的对齐方案，最终目标是构建能比人类更快、更好进行对齐研究的 AI 系统。人类的角色将从“生成研究”转变为“审查 AI 完成的研究”。
*   **语言模型的适用性**：语言模型（如未来的 WebGPT、InstructGPT 和 Codex）特别适合对齐研究，因为它们已通过互联网预装了人类价值观知识，且不是会在现实世界中追求自身目标的独立代理。
*   **开源承诺**：OpenAI 承诺在安全前提下公开分享对齐研究成果，并计划在未来将有用的对齐研究辅助模型开放给外部社区使用。

**3) 风险与缺口**
*   **鲁棒性与可解释性不足**：当前路径对鲁棒性和可解释性研究的重视不够，OpenAI 承认目前在这两个领域投资不足。
*   **缺陷放大风险**：使用 AI 协助评估可能会扩大或放大 AI 助手本身存在的细微不一致、偏见或漏洞。
*   **范式转变风险**：对齐 AGI 的问题可能与当前系统截然不同；如果出现重大的不连续性，从现有模型（如 InstructGPT）中获取的经验可能无法直接适用。
*   **核心难点偏差**：对齐问题中最困难的部分，可能根本不在于为 AI 系统设计可扩展且对齐的训练信号。
*   **模型自身的危险性**：能够有意义地加速对齐研究的最低能力模型，如果未经过适当对齐，可能本身就已经过于危险，从而导致人类无法利用其解决对齐问题。

## 正文

我们的目标是不断提升 AI 系统从人类反馈中学习的能力，并让其能够协助人类评估 AI。我们的最终愿景是构建一个足够对齐的 AI 系统，从而帮助我们解决所有其他的对齐问题。

我们的对齐研究旨在使通用人工智能（AGI）与人类价值观保持一致，并遵循人类的意图。我们采取的是一种迭代的、实证的方法：通过尝试对齐能力极强的 AI 系统，我们可以了解哪些方法有效、哪些无效，从而不断完善我们构建更安全、更对齐的 AI 系统的能力。通过科学实验，我们研究对齐技术将如何扩展，以及它们会在何处失效。

我们不仅致力于解决当前最强大的 AI 系统中的对齐问题，也着眼于在通往 AGI 道路上预期会遇到的对齐问题。我们的主要目标是将现有的对齐理念推向极致，并准确理解和记录它们成功的方式或失败的原因。我们相信，即使没有根本性的全新对齐理念，我们也很可能构建出足够对齐的 AI 系统，从而大幅推进对齐研究本身的发展。

未对齐的 AGI 可能会对人类构成重大风险，而解决 AGI 的对齐问题可能极其困难，甚至需要全人类共同努力。因此，我们承诺在安全的前提下公开分享我们的对齐研究成果：

> “我们希望对我们的对齐技术在实践中的实际效果保持透明，并且我们希望每一位 AGI 开发者都能使用世界上最好的对齐技术。”

从宏观层面来看，我们的对齐研究方法侧重于为非常聪明的 AI 系统设计一种可扩展的、与人类意图对齐的训练信号。该方法包含三大支柱：

1. 使用人类反馈训练 AI 系统
2. 训练 AI 系统协助人类评估
3. 训练 AI 系统进行对齐研究

使 AI 系统与人类价值观对齐还带来了一系列其他重大的社会技术挑战（例如决定这些系统应该与谁的价值观对齐）。解决这些问题对实现我们的使命同样重要，但本文暂不深入探讨。

### 使用人类反馈训练 AI 系统

基于人类反馈的强化学习（RLHF）是我们目前用于对齐已部署语言模型的主要技术。我们基于 GPT-3 等预训练语言模型，训练了一类名为 InstructGPT 的模型。这些模型被训练为遵循人类意图：既包括指令给出的显性意图，也包括真实性、公平性和安全性等隐性意图。

我们的结果表明，目前在以对齐为重点的微调方面，有许多唾手可得的成果：人类更偏好 InstructGPT，而不是比它大 100 倍的预训练模型；而其微调成本不到 GPT-3 预训练计算量的 2%，且仅需约 20,000 小时的人类反馈。我们希望这项工作能启发业内其他人增加对大型语言模型对齐的投资，并提高用户对已部署模型安全性的期望。

我们的自然语言 API 为对齐研究提供了一个非常有用的环境：它为我们提供了关于对齐技术在现实世界中实际效果的丰富反馈循环，这些反馈建立在客户愿意付费的各种任务之上。平均而言，我们的客户已经更倾向于使用 InstructGPT 而不是预训练模型。

然而，目前版本的 InstructGPT 距离完全对齐还很远：它们有时无法遵循简单的指令，并不总是真实的，不能可靠地拒绝有害任务，有时还会给出带有偏见或有害的回复。一些客户发现 InstructGPT 的回复在创造性上明显不如预训练模型，这是我们在公开基准测试中运行 InstructGPT 时没有意识到的。我们也在努力发展对 RLHF 的更详细的科学理解，并探索如何提高人类反馈的质量。

对齐我们的 API 比对齐 AGI 要容易得多，因为 API 上的大多数任务对人类来说并不难监督，而且我们部署的语言模型并不比人类聪明。我们不认为 RLHF 足以对齐 AGI，但它是我们最看好的可扩展对齐方案的核心构建块，因此完善这一方法极具价值。

### 训练模型协助人类评估

RLHF 有一个根本的局限性：它假设人类能够准确评估 AI 系统正在执行的任务。今天的人类在这方面做得很好，但随着模型变得越来越强大，它们将能够执行对人类来说难以评估的任务（例如，找出大型代码库或科学论文中的所有缺陷）。我们的模型可能会学会告诉人类评估者他们想听的话，而不是告诉他们真相。为了扩展对齐能力，我们希望使用递归奖励建模（RRM）、辩论（debate）和迭代放大（iterated amplification）等技术。

目前我们的主要方向基于 RRM：我们训练模型来协助人类评估那些对人类来说过于困难而无法直接评估的任务。例如：

*   **总结书籍：** 如果人类对某本书不熟悉，评估书籍摘要需要很长时间，但我们的模型可以通过编写章节摘要来协助人类评估。
*   **协助评估事实准确性：** 我们训练了一个模型，通过浏览网页并提供引文和链接来协助人类。在简单问题上，该模型的输出已经比人类编写的回复更受青睐。
*   **对其自身输出撰写批评性评论：** 在基于查询的摘要任务中，批评性评论的协助使人类在模型输出中发现的缺陷平均增加了 50%。即使我们要求人类编写看似合理但实际上不正确的摘要，这一点也同样成立。
*   **创建难以评估的编码任务：** 我们正在创建一组对无协助的人类来说极难可靠评估的编码任务，并希望很快发布这个数据集。

即使我们的 AI 系统提出了非常有创意的解决方案（比如 AlphaGo 的第 37 手），我们的对齐技术也需要发挥作用。因此，我们特别有兴趣训练模型来协助人类区分正确的解决方案与具有误导性或欺骗性的解决方案。我们相信，了解如何在实践中使 AI 辅助评估发挥作用的最佳方式，就是去构建 AI 助手。

### 训练 AI 系统进行对齐研究

目前还没有已知的、可无限扩展的对齐问题解决方案。随着 AI 的不断进步，我们预计会遇到许多在当前系统中尚未观察到的新对齐问题。其中一些问题我们现在就能预见，而另一些将是全新的。

我们认为，找到一个可无限扩展的解决方案可能非常困难。相反，我们的目标是采取一种更务实的方法：构建并对齐一个系统，使其在对齐研究上取得比人类更快、更好的进展。

随着我们在这一领域的进展，我们的 AI 系统可以接管越来越多的对齐工作，并最终构思、实施、研究和开发出比我们现有技术更好的对齐技术。它们将与人类合作，确保它们自己的继任者与人类更加对齐。

我们认为，评估对齐研究比产生对齐研究要容易得多，尤其是在提供评估协助的情况下。因此，人类研究人员将把越来越多的精力放在审查 AI 系统完成的对齐研究上，而不是自己生成这些研究。我们的目标是训练出高度对齐的模型，以便我们能够卸下几乎所有对齐研究所需的认知劳动。

重要的是，我们只需要在相关领域具有人类水平能力的“较窄”的 AI 系统，就能在对齐研究上做得和人类一样好。我们预计这些 AI 系统比通用系统或比人类聪明得多的系统更容易对齐。

语言模型特别适合用于自动化对齐研究，因为它们通过阅读互联网，已经“预装”了大量关于人类价值观的知识和信息。开箱即用的它们并不是独立的代理，因此不会在世界上追求自己的目标。为了进行对齐研究，它们不需要不受限制地访问互联网。同时，许多对齐研究任务都可以转化为自然语言或编码任务。

未来版本的 WebGPT、InstructGPT 和 Codex 可以作为对齐研究助手的基石，尽管它们目前的能力还不够。虽然我们不知道我们的模型何时才能具备足够的能力对对齐研究做出有意义的贡献，但我们认为提前开始非常重要。一旦我们训练出可能有用的模型，我们计划将其开放给外部的对齐研究社区使用。

### 局限性

我们对这种对齐 AGI 的方法感到非常兴奋，但我们也预计，随着我们对 AI 技术发展的了解不断加深，这种方法需要不断调整和改进。我们的方法也存在一些重要的局限性：

*   **低估了鲁棒性和可解释性：** 本文规划的路径对鲁棒性和可解释性研究的重要性强调不足，而这两个领域正是 OpenAI 目前投资不足的领域（我们也正在积极招募相关领域的研究人员）。
*   **放大潜在缺陷：** 使用 AI 协助评估可能会扩大或放大 AI 助手本身存在的细微不一致、偏见或漏洞。
*   **范式转变的风险：** 对齐 AGI 可能涉及与对齐当今 AI 系统截然不同的问题。我们预计这种过渡在某种程度上是连续的，但如果出现重大的不连续性或范式转变，那么从对齐 InstructGPT 等模型中吸取的大多数经验可能无法直接适用。
*   **核心难题的偏差：** 对齐问题中最困难的部分可能与为我们的 AI 系统设计可扩展且对齐的训练信号无关。但即使如此，这样的训练信号仍然是必要的。
*   **模型本身的危险性：** 对齐能够有意义地加速对齐研究的模型，可能并不比对齐 AGI 容易多少。换句话说，能够帮助进行对齐研究的最低能力模型，如果未经过适当对齐，可能本身就已经过于危险。如果确实如此，我们将无法从我们自己的系统中获得太多帮助来解决对齐问题。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/alignment]]
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/evals]]
- [[00-元语/risk]]
