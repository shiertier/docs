---
title: "OpenAI Operator 系统卡：计算机使用代理的安全与风险评估"
---

## 摘要

**1) 一句话摘要**
本报告概述了 OpenAI 计算机使用代理（CUA）模型 Operator 的安全与风险评估，详细说明了其核心能力、前沿风险评级，以及针对有害任务、模型错误和提示词注入等多层缓解措施。

**2) 关键点**
* **模型能力与训练**：Operator 结合了 GPT-4o 的视觉能力和强化学习推理能力，能通过图形用户界面（GUI）代表用户执行日常任务。
* **备灾风险评级**：根据 OpenAI 备灾框架，其 CBRN（化生放核）、网络安全和模型自主性风险均为“低”，说服力风险为“中”，符合部署政策。
* **安全策略**：实施了高风险任务分类、关键步骤的“人在回路”确认机制（如金融交易、发邮件），并完全限制极高风险任务（如股票交易）。
* **红队测试**：经过内部团队及来自 20 个国家、精通 24 种语言的外部专家测试，重点排查了提示词注入和越狱漏洞。
* **有害任务缓解**：模型在内部评估集中对新型代理类伤害的拒绝率达 97%，初始发布阶段采取了谨慎的高过度拒绝率策略。
* **模型错误缓解**：引入操作确认机制，将高风险操作的风险降低了约 90%（召回率 92%）；并设有“监控模式”，在用户离开敏感页面时自动暂停任务。
* **提示词注入防御**：将提示词注入的易感率从 62% 降至 23%，并部署了提示词注入监视器（召回率 99%，精确率 90%）。
* **API 发布与安全**：以 `computer-use-preview` 形式向部分开发者提供 API，配套提供 Docker 容器化启动设置和增强的违规监控，强烈建议在隔离环境中使用。

**3) 风险与不足**
* **特定任务能力受限**：模型在处理长 DNA 序列、光学字符识别（OCR）、代码编辑和终端任务时表现挣扎（CBRN 和自主性相关任务得分均极低）。
* **复杂任务挑战**：目前在简短、重复性任务上表现最好，但在处理复杂任务（如幻灯片和日历）时仍面临挑战。
* **非浏览器环境错误率高**：在本地操作系统等非浏览器环境中更容易出错，目前在 OSWorld 基准测试中的准确率仅为 38.1%。
* **API 环境的额外风险**：修改系统消息可能增加越狱风险；提示词注入在本地操作系统上的潜在危害更大；且存在自动化垃圾邮件或欺诈等大规模滥用的可能性。
* **对抗性威胁持续存在**：现实世界的复杂性意味着攻击者仍可能设计出新型的提示词注入和越狱方法。

## 正文

本报告概述了在发布 Operator 之前进行的安全工作，包括外部红队测试、基于 OpenAI 备灾框架（Preparedness Framework）的前沿风险评估，以及为应对关键风险领域而构建的缓解措施。

### 核心风险领域与备灾评分

Operator 面临的具体风险领域主要包括：
* 有害任务（Harmful tasks）
* 模型错误（Model mistakes）
* 提示词注入（Prompt injections）

根据备灾评分卡，Operator 的风险评级如下：
* **CBRN（化学、生物、放射性、核威胁）**：低（Low）
* **网络安全**：低（Low）
* **说服力**：中（Medium）
* **模型自主性**：低（Low）

根据 OpenAI 的政策，只有在采取缓解措施后评分在“中等”或以下的模型才能被部署，评分为“高”或以下才能继续开发。

### 模型能力与训练方法

Operator 是我们“计算机使用代理”（CUA）模型的初步研究预览版。它结合了 GPT-4o 的视觉能力和通过强化学习实现的高级推理能力。它能够像人类一样理解屏幕截图，并与计算机屏幕上的图形用户界面（GUI）——如按钮、菜单和文本框——进行交互。

用户可以通过浏览器指示 Operator 执行各种日常任务（如订购杂货、预订餐厅、购买门票），所有操作均在用户的指导和监督下进行。这标志着 ChatGPT 从“仅能回答问题”向“能代表用户采取行动”迈出了重要一步。

Operator 的训练结合了监督学习和强化学习：
* **监督学习**：基于专业数据，教会模型读取屏幕和准确点击界面元素所需的基础感知与输入控制能力。
* **强化学习**：赋予模型高级能力，如推理、纠错以及应对突发事件的适应能力。

### 风险评估与安全策略

虽然 Operator 扩大了技术的适用范围，但也引入了新的风险向量。例如，第三方网站中的恶意指令可能会误导模型（提示词注入），模型可能会犯下难以撤销的错误，或者被用于执行有害任务。

为了应对这些风险，我们实施了多层安全策略：
1. **高风险任务分类**：我们将任务和行动按风险严重程度（对用户或他人的潜在伤害、撤销负面结果的难易程度）进行分类。
2. **人在回路（Human-in-the-loop）**：对于高风险行动（如金融交易、发送电子邮件、删除日历事件），我们要求在关键步骤进行人类监督，并在执行前进行明确确认。
3. **完全限制**：对于风险过高的任务（如买卖股票），我们完全限制模型提供协助。

### 红队测试（Red Teaming）

在外部测试前，OpenAI 内部的安全、产品团队先进行了红队测试，以识别无任何缓解措施下的潜在风险。随后，我们邀请了来自 20 个国家、精通 24 种语言的外部红队专家对 Operator 进行了测试。

专家们尝试了多种绕过模型安全机制的方法（包括提示词注入和越狱）。由于模型已连接互联网，专家们通过模拟网站、数据库或电子邮件来安全地演示漏洞。基于这些发现，我们进一步加强了模型的安全防护。目前，Operator 仅作为研究预览版向有限用户开放，以便密切监控实际使用情况并解决新出现的风险。

### 前沿风险评估

Operator 基于 GPT-4o 基础模型训练，继承了其在说服力和网络安全方面的风险评级。Operator 新增的核心能力是通过键盘和光标进行视觉浏览。我们针对这一新能力评估了 CBRN 和模型自主性风险，结果均为“低”风险。

* **CBRN（生物风险）**：我们测试了模型是否能自动化湿实验室或新型设计工作（如订购合成 DNA）。Operator 在这些任务上的成功率仅为 1%。模型在处理长 DNA 序列和光学字符识别（OCR）时表现挣扎，符合“低”风险标准。
* **模型自主性**：我们测试了模型的资源获取能力（如获取 GPU、运行本地模型等）。虽然模型擅长浏览网页，但由于视觉输入的限制，在代码编辑和终端任务上受到阻碍。Operator 在所有主要任务上的得分均不超过 10%，同样符合“低”风险标准。

### 风险缓解措施

我们针对三类主要安全风险开发了多层缓解措施，涵盖模型训练、系统级检查、产品设计和持续的政策执行。

#### 1. 有害任务（Harmful Tasks）
用户必须遵守 OpenAI 的使用政策，严禁将 Operator 用于非法活动、欺诈、未经授权的受监管活动或伤害他人。
在模型层面，Operator 被训练拒绝有害任务。对于新型代理类伤害（如进行非法活动或违规金融活动），Operator 在内部评估集上的拒绝率达到 97%。在初始发布阶段，我们将其拒绝阈值调得非常谨慎，其过度拒绝率高于 GPT-4o。

#### 2. 模型错误（Model Mistakes）
模型可能会错误地执行与用户意图不符的操作。为了降低这些错误的风险，我们引入了以下机制：
* **操作确认（Confirmations）**：在执行改变现实世界状态的操作（如完成购买或发送邮件）前，模型会要求用户确认。测试显示，该机制将风险降低了约 90%，模型在需要确认的高风险操作中，平均召回率达到 92%。
* **主动拒绝**：对于高风险任务，模型会主动拒绝（召回率 94%）。
* **监控模式（Watch Mode）**：在处理敏感信息（如电子邮件）的网站上，如果用户不活跃或离开页面，系统会自动暂停执行，直到用户返回并恢复任务。

#### 3. 提示词注入（Prompt Injections）
提示词注入是指模型错误地遵循了屏幕上（如恶意网站或邮件中）的不可信指令。
* 我们增强了模型的鲁棒性，将提示词注入的易感率从无缓解措施时的 62% 降低至 23%。
* 我们增加了一个**提示词注入监视器**。该监视器会监督 Operator 的执行，一旦在屏幕上检测到疑似注入，就会暂停任务。在红队测试中，该监视器达到了 99% 的召回率和 90% 的精确率。

### API 可用性与开发者安全

*注：2025年3月11日新增*

我们将 CUA 模型以 `computer-use-preview` 的形式在 API 中发布，作为研究预览版提供给部分开发者，以收集反馈并提升其安全性和可靠性。

由于 API 环境的特殊性，存在以下额外风险：
* 在非浏览器环境（如本地操作系统）中，模型更容易出错（例如在 OSWorld 基准测试中准确率目前为 38.1%），建议在此类场景中加入人类监督。
* 修改系统消息可能增加越狱风险。
* 提示词注入在本地操作系统上的潜在危害更大。
* 存在大规模滥用的可能性（如自动化垃圾邮件或欺诈）。

为此，我们在 API 中引入了以下安全措施：
* **提示词注入与敏感领域安全检查**：提供更高的可见性和早期预警。
* **容器化启动设置**：提供易于使用的 Docker 应用程序，鼓励开发者在隔离环境中使用。
* **增强的监控与执行**：扩大对违规行为（如越狱、大规模滥用）的检测。

我们强烈建议开发者遵循最佳实践，在隔离环境（如虚拟机）中运行，并定期审查模型操作。

### 局限性与未来规划

尽管我们采取了多层缓解措施，但现实世界的复杂性和对抗性威胁的动态变化意味着风险依然存在。Operator 可能会遇到新的用例、产生不同类型的错误，攻击者也可能设计出新的提示词注入和越狱方法。

CUA 模型目前仍处于早期阶段，在简短、重复性任务上表现最好，但在处理复杂任务（如幻灯片和日历）时仍面临挑战。作为迭代部署策略的一部分，我们将：
* 从小范围用户开始部署，密切监控早期使用情况。
* 收集真实世界反馈，持续改进模型的质量、安全性和可靠性。
* 持续评估并改进针对提示词注入等新兴攻击的防御机制。

*(注：本项目的完成得益于 OpenAI 内部多个团队的协作，以及众多外部红队专家和 AI 训练员的贡献与测试。)*

## 相关文档

- [[01-博客/OpenAI/当 AI 代理点击链接时，如何保护您的数据安全|当 AI 代理点击链接时，如何保护您的数据安全]]；关联理由：解说；说明：该文聚焦 URL 数据外泄与提示词注入防护，细化了本文“提示词注入”风险的产品级缓解路径。
- [[01-博客/OpenAI/OpenAI 的人工智能安全策略|OpenAI 的人工智能安全策略]]；关联理由：上下游；说明：该文给出 OpenAI 安全治理总原则，本文是这些原则在计算机使用代理上的具体落地与量化评估。
- [[01-博客/OpenAI/前沿AI风险与防范准备|前沿AI风险与防范准备]]；关联理由：上下游；说明：该文定义了 Preparedness Framework 的风险域与治理思路，是本文备灾评分与部署阈值的上游背景。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/Agent]]
- [[00-元语/security]]
- [[00-元语/risk]]
- [[00-元语/browser-automation]]
- [[00-元语/prompt]]
- [[00-元语/evals]]
