---
title: "OpenAI 的人工智能安全策略"
---

## 摘要

### 一句话总结
OpenAI 致力于在人工智能系统的构建、部署和使用各个层面内置安全机制，通过严格测试、真实世界反馈、隐私保护、提升准确性及推动全球合作来确保 AI 技术的安全与对齐。

### 关键要点
* **发布前安全评估：** 在发布新系统前进行严格测试、引入外部专家反馈并使用 RLHF 等技术；例如，GPT-4 在公开发布前花费了超过 6 个月的时间进行安全对齐。
* **基于真实世界迭代：** 采取谨慎且逐步的发布策略，通过监控自有服务和 API 的真实使用情况来应对滥用行为，并据此制定细化政策。
* **儿童保护与年龄限制：** 要求用户年满 18 岁（或年满 13 岁且获家长同意）；与 GPT-3.5 相比，GPT-4 响应违规内容请求的可能性降低了 82%。
* **打击儿童剥削：** 接入 Thorn 的 Safer 工具检测儿童性虐待材料（CSAM），并向美国国家失踪与受剥削儿童中心（NCMEC）进行报告。
* **隐私保护机制：** 明确不利用数据出售服务或建立人物画像；在可行情况下移除训练集中的个人信息，微调模型以拒绝隐私获取请求，并支持用户删除个人信息。
* **提升事实准确性：** 将用户反馈作为主要数据源以改善模型，GPT-4 生成事实内容的可能性比 GPT-3.5 高出 40%。
* **谨慎的未来部署与治理：** 未来部署更强大的模型时将更加谨慎（安全评估可能耗时超过 6 个月），并呼吁政策制定者和 AI 提供商共同建立全球治理机制。

### 风险/不足
* **实验室测试的局限性：** 仅靠实验室研究和测试无法预测技术在现实世界中的所有有益用途及所有潜在的滥用手段。
* **事实准确性与“幻觉”：** 模型在预测词汇时可能产生事实上不准确的输出（幻觉）；官方承认在进一步降低幻觉发生率及向公众普及 AI 局限性方面仍有大量工作要做。
* **行业竞争风险：** 在缺乏全球有效治理的情况下，存在 AI 提供商为了抢占先机而在安全上偷工减料的风险。

## 正文

确保人工智能系统的安全构建、部署和使用是我们的核心使命。

OpenAI 致力于让强大的人工智能保持安全并造福大众。我们深知当前的 AI 工具（如 ChatGPT）在提高生产力、激发创造力和提供个性化学习体验方面为全球用户带来了诸多便利。但与此同时，我们也认识到，与任何技术一样，这些工具也伴随着真实的风险。因此，我们致力于在系统的各个层面内置安全机制。

### 构建日益安全的 AI 系统

在发布任何新系统之前，我们会采取一系列严谨的措施：
* 进行严格的测试。
* 邀请外部专家提供反馈。
* 利用基于人类反馈的强化学习（RLHF）等技术改善模型行为。
* 构建广泛的安全和监控系统。

例如，在最新模型 GPT-4 完成训练后，我们在全公司范围内花费了超过 6 个月的时间，使其在公开发布前变得更加安全和对齐（aligned）。我们认为，强大的 AI 系统应当接受严格的安全评估。同时，需要通过监管来确保这些实践得到落实，我们正积极与各国政府探讨最佳的监管形式。

### 从真实世界的使用中学习以完善防护机制

虽然我们努力在部署前预防可预见的风险，但在实验室中能学到的东西是有限的。尽管经过了广泛的研究和测试，我们仍无法预测人们使用我们技术的所有有益方式，也无法预见所有的滥用手段。因此，我们认为，从真实世界的使用中学习，是随着时间推移创建和发布日益安全的 AI 系统的关键组成部分。

* **谨慎且逐步的发布：** 我们谨慎地向不断扩大的用户群体发布新的 AI 系统，并配备完善的防护措施，根据学到的经验进行持续改进。
* **监控与应对滥用：** 我们通过自有服务和 API 提供最强大的模型，以便开发者将其直接集成到应用中。这使我们能够监控并采取行动应对滥用行为，并针对人们在现实中滥用系统的真实情况（而非仅仅是理论上的假设）不断构建缓解措施。
* **制定细化政策：** 真实世界的使用经验促使我们制定了日益细化的政策，以防范对人类构成真正风险的行为，同时保留技术的诸多有益用途。

至关重要的是，我们认为社会必须有时间来适应不断进步的 AI 技术，所有受此技术影响的人都应在 AI 的未来发展中拥有重要的话语权。迭代部署帮助我们更有效地将各方利益相关者纳入关于 AI 技术普及的对话中，这比让他们在没有第一手体验的情况下参与讨论要有效得多。

### 保护儿童

保护儿童是我们安全工作的核心重点之一。

* **年龄限制：** 我们要求用户必须年满 18 岁（或年满 13 岁且获得家长同意）才能使用我们的 AI 工具，并且我们正在研究相关的验证方案。
* **内容限制：** 我们严禁将技术用于生成仇恨、骚扰、暴力或成人内容。与 GPT-3.5 相比，最新模型 GPT-4 响应违规内容请求的可能性降低了 82%，并且我们建立了强大的系统来监控滥用行为。
* **打击儿童剥削：** 我们付出了巨大努力，以最大程度地降低模型生成危害儿童内容的可能性。例如，当用户试图向我们的图像工具上传已知的儿童性虐待材料（CSAM）时，我们会使用 Thorn 的 Safer 工具进行检测、审查，并向美国国家失踪与受剥削儿童中心（NCMEC）报告。
* **定制化安全措施：** 除了默认的安全护栏外，我们还与非营利组织可汗学院（Khan Academy）等开发者合作，为其特定用例（如充当学生的虚拟导师和教师的课堂助手）定制安全缓解措施。我们还在开发新功能，允许开发者为模型输出设定更严格的标准。

### 尊重隐私

我们的大型语言模型是在广泛的文本语料库上训练的，包括公开内容、授权内容以及人工审查者生成的内容。我们不会利用数据来出售服务、投放广告或建立人物画像——我们使用数据的目的是让模型对人们更有帮助。例如，ChatGPT 会通过人们与它的对话进行进一步训练来提升性能。

虽然我们的部分训练数据包含了互联网上公开的个人信息，但我们希望模型学习的是关于这个世界的知识，而不是普通个人的隐私。因此，我们采取了以下措施：
* 在可行的情况下从训练数据集中移除个人信息。
* 微调模型以拒绝获取普通个人隐私信息的请求。
* 响应个人从我们的系统中删除其个人信息的请求。

这些步骤最大限度地降低了我们的模型生成包含个人隐私信息回复的可能性。

### 提升事实准确性

当前的大型语言模型是基于它们之前见过的模式（包括用户输入的文本）来预测接下来的一系列单词。在某些情况下，最有可能出现的下一个词在事实上可能并不准确。

提升事实准确性是 OpenAI 及许多其他 AI 开发者的重要关注点，我们正在取得进展。通过将用户对 ChatGPT 错误输出的反馈作为主要数据源，我们提高了 GPT-4 的事实准确性。GPT-4 生成事实内容的可能性比 GPT-3.5 高出 40%。

当用户注册使用该工具时，我们力求尽可能透明地告知用户 ChatGPT 可能并不总是准确的。不过我们也认识到，要进一步降低“幻觉”（hallucinations）发生的可能性，并向公众普及这些 AI 工具目前的局限性，我们还有很多工作要做。

### 持续的研究与合作

我们认为，解决 AI 安全问题的务实方法是投入更多的时间和资源，研究有效的缓解措施和对齐技术，并在现实世界的滥用场景中对它们进行测试。

* **安全与能力并重：** 我们认为提升 AI 的安全性与提升其能力应齐头并进。迄今为止，我们最好的安全工作成果都源于与我们最强大的模型打交道，因为它们更擅长遵循用户的指令，也更容易被引导。
* **保持谨慎：** 随着 AI 系统的演进，我们在创建和部署更强大的模型时将愈发谨慎，并继续加强安全防范措施。虽然我们为了更好地了解 GPT-4 的能力、优势和风险而等待了超过 6 个月才进行部署，但在未来，为了提升 AI 系统的安全性，有时可能需要花费更长的时间。
* **呼吁全球治理：** 政策制定者和 AI 提供商需要确保 AI 的开发和部署在全球范围内得到有效治理，以防止任何人为了抢占先机而偷工减料。这是一项艰巨的挑战，需要技术和制度上的双重创新，而我们非常渴望为此贡献力量。

解决安全问题还需要广泛的辩论、实验和参与，包括探讨 AI 系统行为的边界。我们将继续促进利益相关者之间的合作与公开对话，共同打造一个安全的 AI 生态系统。

## 关联主题

- [[00-元语/OpenAI]]
- [[00-元语/AI]]
- [[00-元语/alignment]]
- [[00-元语/security]]
- [[00-元语/risk]]
- [[00-元语/compliance]]
- [[00-元语/llm]]
