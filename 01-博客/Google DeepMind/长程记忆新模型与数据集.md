# 长程记忆新模型与数据集

## 文档信息

- 发布日期：2025-12-03
- 原文链接：https://deepmind.google/blog/a-new-model-and-dataset-for-long-range-memory/

## 摘要

**1) 一句话摘要**
DeepMind 提出了一种受人类睡眠记忆巩固机制启发的长程记忆新模型 Compressive Transformer，并发布了一个基于近2.8万本书籍的大型语言建模数据集 PG-19，旨在提升人工智能在长跨度时间内的记忆压缩与推理能力。

**2) 关键要点**
*   **传统架构瓶颈**：传统的 LSTM 等循环神经网络面临记忆容量与计算效率的瓶颈（例如 64KB 的记忆会导致参数量呈二次方增长至 8GB）。
*   **现有基准饱和**：现有的长程语言模型基准（如平均约 3600 词的 WikiText-103）正面临被 TransformerXL 等能够利用上千词上下文的模型快速饱和的现状。
*   **新数据集 PG-19**：发布了源自古腾堡计划（1919年以前出版的约 28,000 本书籍）的新基准 PG-19，专为长程记忆研究设计。
*   **PG-19 的数据特征**：该数据集采用极少的预处理（不限制词汇量大小、不审查数字），规模是十亿词基准（Billion Word Benchmark）的两倍多，上下文长度是 WikiText-103 的 10 倍以上。
*   **Compressive Transformer 机制**：与 TransformerXL 丢弃旧激活数据不同，该新模型通过一个由辅助损失引导的神经网络，将旧的短期颗粒状记忆过滤并“压缩”成粗略的压缩记忆，以便在更长时间内检索。
*   **模型性能表现**：在不使用额外训练数据的情况下，该模型在 WikiText-103 和 Enwik8 两个长程基准测试中达到了 SOTA（最先进）性能。
*   **多领域适用性**：除了在 PG-19 上生成连贯的长篇书籍级文本外，该模型还可有效用于语音建模（擅长处理罕见词）以及强化学习智能体的记忆任务。

**3) 风险与局限性（基于原文明确提及）**
*   **缺乏现实世界基础**：尽管 Compressive Transformer 能够生成连贯的长程文本，但其对语言的理解并没有真正扎根于现实世界或其中发生的实际事件。
*   **局部上下文依赖批评**：将语言建模作为长程推理任务曾受到批评，因为传统神经语言模型通常忽略更广泛的上下文，仅从局部上下文中就能捕获其预测的很大一部分。
*   **长期运行的计算不切实际**：随着智能体需要运行数天、数周甚至数年，在每个时间步对所有原始输入数据进行计算是不切实际的，必须依赖更有效的压缩和稀疏架构。

## 正文

本博客介绍了一种新的长程记忆模型 Compressive Transformer，以及一个用于书籍级语言建模的新基准 PG-19。我们将提供必要的概念工具，帮助读者在记忆模型和语言建模最新发展的背景下理解这项新研究。

在我们的一生中，我们会建立起在不同时间尺度上保留的记忆，从几分钟到几个月，再到几年甚至几十年。在阅读一本书时，我们能够回想起许多章节前或系列前作中介绍过的角色，并在当前语境下推断他们的动机和可能的行动。我们甚至可以在繁忙的一周里放下书本，之后再从停下的地方继续阅读，而不会忘记故事情节。

我们之所以能做到这些，并不是因为我们存储了在一生中接收到的关于这个世界的每一个感官输入细节。我们的大脑会根据相关性、惊喜度、感知到的危险以及重复性等因素，对输入刺激进行选择、过滤和整合。换句话说，我们将一生的经历压缩成一组显著的记忆，这些记忆帮助我们理解过去，并更好地预测未来。人工智能研究人员的一个主要目标，就是探索如何在计算系统和需要跨越长时间跨度进行复杂推理的基准测试中实现这种能力。

在过去的二十年里，人工神经网络的记忆系统取得了长足的进步。在这篇文章中，我们将回顾过去的进展，探讨为什么这是一项如此困难的任务，并思考自然语言建模如何为设计更好的长程记忆系统提供有效手段。我们将反思改进压缩记忆架构和稀疏记忆访问机制的必要性，以努力实现在计算系统中融入终身推理的目标。

## 深度学习中记忆的简史

> 不存在基于持久印象的记忆或保持能力。我们所谓的记忆，不过是对重复刺激的反应能力增强罢了。

目前最早且使用最广泛的记忆架构之一是一种被称为长短期记忆网络（LSTM）的循环神经网络（RNN）。LSTM 以数字向量的形式维护一个紧凑的记忆，并通过门控的读取、写入和遗忘操作来访问和修改它。它最初是在一套涉及学习比特流逻辑操作的合成任务上开发出来的。然而，它后来成为了序列数据中无处不在的模型：从识别手写笔记到预测肾损伤的早期发作。

LSTM 以及许多当代 RNN 的一个弱点是容量。它们的设计使得记忆中的每个单元都可以通过一个可学习的权重影响记忆中的所有其他单元。但这导致了一个计算效率低下的系统：模型中可学习参数的数量随着记忆大小呈二次方增长。例如，一个拥有 64KB 记忆的 LSTM 会产生 8GB 大小的参数。规避这种记忆容量瓶颈一直是一个活跃的研究领域。

图 1. 长程推理对通用智能至关重要。在这里，一个智能体在很长一段时间内记住了钥匙的存在和位置，并在发现宝箱时回想起了这个信息——促使智能体返回记住的位置取回钥匙。

DeepMind 的研究人员提出了一种新颖的架构——可微神经计算机（DNC），它用一个大得多的记忆矩阵来增强 LSTM，以解决这些缺陷。DNC 使用注意力（attention）操作从这个记忆矩阵中读取信息。在视觉注意力中，我们的眼睛会被视觉场景中相关的物体所吸引——例如，在一次情绪激动的对话中，人们通常会花更多时间观察朋友的脸，而不是注意他们的鞋子。在这里，记忆模型可以关注过去特定的事件或数据。这种注意力操作需要固定数量的参数，与记忆大小无关，因此模型的记忆容量可以显著增加。

与 DNC 一起，带有额外注意力机制的循环神经网络在翻译和问答领域也展现出了潜力。这些模型能够使用两种记忆结构进行跨时间推理：一个小而紧凑的 LSTM 记忆和一个大型的外部记忆。然而，最近 Google Brain 团队的研究人员提出了 Transformer，它移除了 LSTM，仅使用注意力机制跨时间传递信息。

图 2. 神经网络在英法翻译中注意力机制的可视化。来源：Attention and Augmented Recurrent Neural Networks, Olah & Carter, 2016

Transformer 最初被证明在机器翻译方面显著优于循环神经网络。然而，它后来被应用于自然语言处理的一系列应用中，从问答、文档摘要、情感分类到自然语言建模——这项任务在过去一年中取得了特别令人兴奋的进展。

## 自然语言建模

寻找既能推动更好的记忆架构发展，又能推动我们进一步走向通用人工智能（AGI）的机器学习任务是一项挑战。统计语言建模就是这样一项我们认为对这两个目的都有价值的任务。语言模型通过顺序预测文本流中的下一个词来工作。它们可用于对现有文本进行建模，也可用于生成新文本。随着它们对过去建模的能力越来越强，它们的预测变得更加准确，生成的文本也变得更加逼真。

在克劳德·香农（Claude Shannon）于 1948 年发表的创立了信息论领域的开创性文章《通信的数学理论》（A Mathematical Theory of Communication）中，他讨论了原始的语言模型，并说明了增加更多上下文如何提高生成文本的质量和逼真度。他通过引入最简单的英语文本模型（完全没有上下文建模——一个独立处理每个字符的字符级模型）来做到这一点。通过根据字符的相对频率（‘a’ 出现 8%，‘b’ 出现 1.5% 等）进行采样，我们得到了一串毫无意义的字符串：

XFOML RXKHRJFFJUJ ZLPWCFWKCYJ FFJEYVKCQSGHYD QPAAMKBZAACIBZLHJQD.

然而，他指出，如果改为独立地对单词的概率进行建模，样本质量就会有所提高。现在建模的上下文大约大了 7 倍（一个单词中字符的平均数量）：

REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE THESE.

通过对词对的概率进行建模，上下文长度进一步增加了 2 倍，出现了更逼真的文本：

THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED

换句话说，上下文长度的增加会导致生成文本质量的提高。香农评论了他生成的样本的质量，并推测自然文本样本可能会从一个足够复杂的统计模型中产生：“‘attack on an English writer that the character of this’ 这十个词的特定序列并非完全不合理。由此看来，一个足够复杂的随机过程将能令人满意地表示一个离散源。”

对语言建模作为长程推理任务的一个批评是，模型可以从局部上下文中捕获其预测的很大一部分。传统的神经语言模型通常忽略更广泛的上下文，主要关注短期内容。例如，在 2017 年，Dailuk 等人发现他们的神经语言模型很少关注前面五个词以外的内容。然而，在过去的一年里，大型 Transformer 模型已被证明能够利用数百个词的上下文来生成越来越逼真、连贯范围更长的文本。来自 OpenAI 的 GPT-2（一个拥有 15 亿参数的 Transformer）的演示表明，该模型能够生成逼真的文本，并在多个段落中保留关键实体（例如 Jorge Pérez 博士和独角兽）：

> 科学家以它们独特的角将这个种群命名为奥维德独角兽（Ovid’s Unicorn）。这些长着四只角、银白色的独角兽以前在科学界是未知的。
> 
> 现在，经过近两个世纪，引发这一奇特现象的谜团终于被解开了。
> 
> 来自拉巴斯大学的进化生物学家豪尔赫·佩雷斯（Jorge Pérez）博士和几位同伴在探索安第斯山脉时，发现了一个小山谷，里面没有其他动物或人类。佩雷斯注意到，山谷里似乎有一个天然喷泉，周围环绕着两座由岩石和银色白雪构成的山峰。
> 
> 佩雷斯和其他人随后进一步深入山谷。“当我们到达其中一座山峰的顶部时，水看起来是蓝色的，上面有一些晶体，”佩雷斯说。
> 
> 佩雷斯和他的朋友们惊讶地看到了独角兽群。从空中就能看到这些生物，不需要移动太多就能看清它们——它们离得如此之近，甚至可以触摸到它们的角。
> 
> 在检查这些奇异生物时，科学家们发现这些生物还会说一些相当标准的英语。佩雷斯说：“例如，我们可以看到它们有一种共同的‘语言’，类似于方言或辩证法。”
> 
> 佩雷斯博士认为，这些独角兽可能起源于阿根廷，人们认为这些动物是在人类到达南美洲那些地区之前生活在那里的一个失落人类种族的后代。
> 
> 虽然它们的起源仍不清楚，但一些人认为，也许这些生物是在人类文明出现之前，人类和独角兽相遇时创造出来的。据佩雷斯说：“在南美洲，此类事件似乎相当普遍。”
> 
> 然而，佩雷斯也指出，要确定独角兽是否真的是一个失落外星种族的后代，唯一确切的方法可能就是通过 DNA。“但它们似乎能够很好地用英语交流，我认为这是进化的标志，或者至少是社会组织发生变化的标志，”这位科学家说。

## 知识迁移

距离香农早期的语言模型实验 70 年后，这样的样本可能会让他感到震惊。然而，强大的神经语言模型的真正优势——以及它们与 AGI 目标的相关性——在于它们将知识迁移到一系列任务中的能力。在学习如何对文本进行建模的过程中，神经语言模型似乎建立了一个关联知识库，以及大量的技能。

例如，OpenAI 的研究人员表明，GPT-2 可以应用于自然语言处理任务，如问答、释义或情感分析，并取得了令人惊讶的良好性能——特别是对于一个从未被明确训练来执行此类任务的模型而言。当大型 Transformer 语言模型在特定任务（如问答）上进行微调时，其最终性能明显优于专门为问答设计和训练的模型。Google 著名的自然语言模型 BERT 在广泛的 NLP 基准测试中取得了最先进的性能，现在已成为 Google 搜索的一部分。最近，有研究表明，GPT-2 可以通过在游戏走法字符串上进行训练来学习下初级国际象棋。

## 语言模型基准测试

一个流行的长程语言模型基准是 WikiText-103，它由英文维基百科文章组成，由 Salesforce AI 的研究人员开发。文章平均约 3600 个词，在创建时，这远远超出了当时最先进模型的记忆窗口。

然而，Google 的研究人员最近表明，一种名为 TransformerXL 的 Transformer 变体——它维护着过去网络激活的记忆，并最近在 WikiText-103 上获得了最先进的结果——可以利用跨越一千多个词的上下文。这就提出了一个问题：模型会很快在这些基准测试上达到饱和吗？因此，我们编译并发布了一个基于书籍的、范围更长的新语言模型基准。

## 用于长期记忆研究的新数据集

为了支持人们对长程序列模型日益增长的兴趣，我们发布了一个新的语言建模基准 PG-19，它源自古腾堡计划（Project Gutenberg）在线图书馆中的书籍。

书籍为长程记忆模型的开发提供了丰富的上下文。我们从古腾堡计划中挑选了大约 28,000 本 1919 年以前出版的书籍作为子集。与以往发布的语言建模数据集不同，我们对文本进行的预处理非常少。例如，我们不限制数据的词汇量大小，也不对数字进行审查，以避免过滤掉有用的信息。

PG-19 的规模是以前语言建模基准（如十亿词基准，Billion Word Benchmark）的两倍多，并且包含的文本在上下文长度上是以前的长程语言模型基准 WikiText-103 的 10 倍以上。我们在下面提供了现有语言建模基准的对比表：

## Compressive Transformer

除了新的基准测试，我们还提出了一种名为 Compressive Transformer 的长程记忆模型。我们从睡眠在巩固情景记忆形成中的作用中汲取了灵感。众所周知，睡眠对记忆至关重要，人们认为睡眠有助于压缩和巩固记忆，从而提高记忆任务的推理能力。在 Compressive Transformer 中，类似于情景记忆的颗粒状记忆在模型处理输入序列时被在线收集；随着时间的推移，它们最终会被压缩。

与 Transformer 一样，Compressive Transformer 使用注意力机制从过去中选择信息。它以与最近提出的 TransformerXL 相同的风格，维护着过去激活的短期记忆。TransformerXL 会在过去的激活变旧时将其丢弃，而 Compressive Transformer 则将它们压缩成压缩记忆。这种压缩由一个神经网络执行，该网络由一个辅助损失引导，指示其保留与任务相关的信息。它可以学习过滤掉不相关的记忆，并组合记忆，以便保留显著信息并在更长的时间内可检索。

图 3. Compressive Transformer：一种长程序列注意力序列模型，它用颗粒状的短期记忆和粗略的压缩记忆来表征过去。

我们发现，与不使用额外训练数据源的已发表结果相比，Compressive Transformer 在两个广泛使用的长程基准测试（WikiText-103 和 Enwik8）的自然语言建模中具有最先进的性能。我们还表明，它可以有效地用于语音建模，特别擅长处理罕见词，并且可以在强化学习智能体中使用以解决记忆任务。

我们发现，Compressive Transformer 在对 PG-19 基准测试中的长上下文书籍文本进行建模时，产生了最大的性能提升。该模型的条件样本可用于编写类似书籍的摘录。下面我们展示了一个样本，输入了一段文本作为上下文，摘自拉尔夫·康纳（Ralph Connor）的《太阳舞小径的巡逻》（The Patrol of the Sun Dance Trail），这是模型以前从未见过的。

**来自拉尔夫·康纳《太阳舞小径的巡逻》的上下文**

> 沿着与铁轨平行的运货小道，一个皮肤黝黑、身材瘦小但结实的男人跑了过来，他剧烈的喘息、满是汗水的脸庞和张开的嘴巴宣告着他的筋疲力尽。在一条通向左边的小径处，他停了下来，注意到它通向那面飘扬的旗帜，便转了进去，然后在岩石密布的山坡上挣扎前行，直到来到那座木屋前。木屋周围有一圈深深的门廊，外面围着一圈质朴的栅栏，栅栏里是一个花园，其整洁程度彰显了英国士兵的特征。跑步者穿过大门，沿着砾石小路走上去，开始登上台阶……

**Compressive Transformer 的续写**

> ……仿佛是不约而同，警司部队（Superintendent Force）的三名成员都站在阳台上注视着他的行进。一个男人沿着小路慢慢地溜达过来，经常停下来好奇地四处张望，当他看到阳台上站着的四个人时，又停了下来。他面色黝黑，眼睛突出，额头上的皱纹显示出历经沧桑的痕迹，但他的步伐很机警，开始迈着稳健的步伐登上台阶。
> 
> 这个男人的注意力被两名警察的意外出现转移了；第三个男人溜了上来，嘴里嘟囔着，停下来疑惑地环顾四周，然后又溜走了。与此同时，阳台上的四个人继续注视着新来者的靠近。尽管警司部队极力克制，但在那个陌生人面前，他们还是感到了一种奇怪的感觉——一种近乎恐惧的不安。他不愿意承认这个事实，但他骨子里感觉到了。但对于观察者来说，那个陌生人平凡、日常的面容以及他的到来，似乎正是 Seven White Shee 欠他的——他们的重量、他们的匆忙、他们的冲击……

Compressive Transformer 能够生成各种风格的叙事，包括多角色对话、第一人称日记或第三人称散文。虽然该模型对语言的理解并没有扎根于现实世界或其中发生的事件——但通过捕获更长程的相关性，我们看到了更连贯文本的出现。

## 记忆架构的未来

随着我们努力创建能够运行数天、数周甚至数年的智能体，在每个时间步对所有原始输入数据进行计算将是不切实际的。即使在计算能力不断增长的今天，我们也需要开发用于记忆的压缩和稀疏架构，以构建表征并对行动进行推理。

能够捕获跨越数天、数月或数年经验的相关关联的模型即将出现。我们相信，实现跨时间更强大推理的途径，将源于对过去更好的选择性注意力，以及更有效的压缩机制。在探索这一领域的理念时，我们需要跨越越来越长时间间隔的任务和数据集。PG-19 数据集可以帮助研究人员朝着这个方向前进，它以我们人类通常消费的最长形式呈现文本数据：全本长篇书籍。我们希望它的发布能激发人们对新模型的兴趣，这些模型能够压缩过去，从而预测未来并在当下采取有效的行动。

**阅读更多**

Compressive Transformer 论文

PG-19 基准测试

## 相关文档

- [[01-博客/微信公众平台/告别静态权重：谷歌提出 Nested Learning，让大模型拥有“海马体”|告别静态权重：谷歌提出 Nested Learning，让大模型拥有“海马体”]]；关联理由：延伸思考；说明：两文都围绕将短期上下文压缩为长期记忆展开，但分别代表不同代际的模型记忆架构与实现路径。

## 关联主题

- [[00-元语/memory]]
- [[00-元语/AI]]
- [[00-元语/benchmark]]
- [[00-元语/llm]]
- [[00-元语/paper]]
