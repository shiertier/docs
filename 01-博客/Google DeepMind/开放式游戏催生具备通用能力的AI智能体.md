---
title: "开放式游戏催生具备通用能力的AI智能体"
---

## 摘要

### 1) 一句话总结
DeepMind通过构建包含数十亿程序化生成任务的3D模拟环境XLand，并结合动态迭代的强化学习方法，成功训练出能够在未见过的复杂游戏中展现出零样本泛化和通用启发式行为的AI智能体。

### 2) 核心要点
*   **研发背景：** 传统强化学习智能体（如AlphaZero）通常只能针对单一游戏进行训练，DeepMind旨在创造无需从头训练即可适应新任务的通用AI智能体。
*   **XLand模拟环境：** 构建了一个庞大的3D第一人称多人模拟环境XLand，支持通过编程自动生成数十亿个涵盖不同游戏、世界和玩家的训练任务，解决了跨游戏应用缺乏训练数据的问题。
*   **GOAT神经网络架构：** 采用“目标注意力智能体”（GOAT）架构，通过针对特定子目标的注意力机制引导智能体，使其能够学习到更具通用能力的策略。
*   **动态与迭代训练：** 采用动态任务生成技术确保任务难度“刚刚好”，并结合基于种群的训练（PBT）和多代引导学习（bootstrap），实现不优化单一指标而是优化通用能力谱系的开放式学习。
*   **科学的评估指标：** 放弃传统的平均奖励指标，改用基于纳什均衡的归一化分数，并通过考察分数的百分位数分布和任务参与度来准确评估智能体在保留任务（held-out tasks）上的表现。
*   **庞大的训练规模：** 经过5代训练，最终代智能体在4000个独特世界中游玩了约70万个独特游戏，在340万个独特任务中经历了2000亿个训练步骤。
*   **零样本泛化与涌现行为：** 智能体在未见过的复杂评估任务（如捉迷藏、夺旗）中展现出零样本（zero-shot）适应能力，并涌现出实验探索、工具使用（如制造坡道、遮挡视线）等通用启发式行为。
*   **高效的微调潜力：** 面对全新的复杂任务，该智能体仅需30分钟集中训练即可迅速适应，而从零开始训练的传统强化学习智能体则完全无法学习这些任务。

### 3) 风险与局限性
*   **能力上限受限：** 该开放式学习系统的潜在局限性在于环境空间（XLand）的表达能力以及智能体神经网络的容量。
*   **行为意图难以界定：** 智能体在多人环境（如社会困境任务）中展现出的合作等行为，其真实意图难以确定，观察到的行为通常似乎是偶然发生的。
*   **通用能力仍处早期：** 智能体目前仅在该特定的任务空间内刚刚开始具备通用能力，且仍有少数连人类都无法完成的程序化生成任务是智能体无法参与的。

## 正文

近年来，人工智能智能体在各种复杂游戏环境中取得了巨大成功。例如，AlphaZero 在仅仅掌握基础规则后，便击败了国际象棋、将棋和围棋的世界冠军程序。通过强化学习（RL），这个单一系统通过反复试错，在无数轮游戏中进行学习。然而，AlphaZero 仍然需要对每种游戏进行单独训练——如果不从头开始重复强化学习过程，它就无法直接学习另一种游戏或任务。强化学习的其他成功案例（如 Atari、夺旗游戏、星际争霸 II、Dota 2 和捉迷藏）也存在同样的局限性。

DeepMind 的使命是解决智能问题以推动科学和人类发展，这促使我们探索如何克服这一局限，创造出具备更强通用性和适应性行为的 AI 智能体。我们希望这些智能体不再一次只学习一个游戏，而是能够应对全新的条件，并游玩整个宇宙般浩瀚的游戏和任务，包括那些从未见过的挑战。

在最新发表的预印本论文《开放式学习催生具备通用能力的智能体》（Open-Ended Learning Leads to Generally Capable Agents）中，我们详细介绍了在训练无需人类交互数据即可游玩多种游戏的智能体方面所迈出的第一步。我们创建了一个名为 XLand 的庞大游戏环境，其中包含许多位于一致且符合人类认知的 3D 世界中的多人游戏。该环境使我们能够制定新的学习算法，动态控制智能体的训练方式及其训练的游戏。

智能体的能力在应对训练中出现的挑战时不断迭代提升，学习过程会持续优化训练任务，使智能体永不停止学习。最终，我们得到了一个能够在广泛任务中取得成功的智能体——从简单的寻物问题，到捉迷藏和夺旗等在训练中从未遇到过的复杂游戏。我们发现，该智能体展现出了通用的启发式行为（如实验探索），这些行为可广泛应用于许多任务，而非仅针对单一任务。这一新方法标志着我们在创造更具通用性、能够在不断变化的环境中快速适应的智能体方面迈出了重要一步。

### 构建海量训练任务的宇宙

缺乏训练数据（在这里，“数据”指的是不同的任务）一直是限制强化学习智能体行为具备足够通用性以跨游戏应用的主要因素之一。如果没有足够庞大的任务集来训练智能体，它们就无法将学到的行为适应到新任务中。为了解决这个问题，我们的团队设计了一个支持程序化生成任务的模拟空间，从而能够通过编程创建任务并从中生成经验。这使我们能够在 XLand 中包含数十亿个涵盖不同游戏、世界和玩家的任务。

我们的 AI 智能体以 3D 第一人称化身的形式存在于一个旨在模拟物理世界的多人环境中。玩家通过观察 RGB 图像来感知周围环境，并接收关于其目标的文本描述，随后在一系列游戏中进行训练。这些游戏既有简单的合作寻物和世界导航（例如目标是“靠近紫色立方体”），也有基于多个奖励选项的复杂游戏（例如“靠近紫色立方体或将黄色球体放在红色地板上”），还包括对抗性的多人游戏（例如对称的捉迷藏，目标是“看到对手并让对手看不到我”）。每个游戏都定义了玩家的奖励，而每个玩家的最终目标就是最大化这些奖励。

由于 XLand 可以通过编程指定，游戏空间允许以自动化和算法化的方式生成数据。同时，由于 XLand 中的任务涉及多个玩家，其他玩家的行为会极大地影响 AI 智能体面临的挑战。这些复杂、非线性的交互创造了理想的训练数据源，因为有时环境组件的微小变化也会导致智能体面临的挑战发生巨大改变。

### 动态与迭代的训练方法

我们研究的核心是深度强化学习在训练智能体神经网络中的作用。我们使用的神经网络架构提供了一种针对智能体内部循环状态的注意力机制——通过对当前所玩游戏特有子目标的估计，来引导智能体的注意力。我们发现，这种“目标注意力智能体”（GOAT）能够学习到具备更强通用能力的策略。

我们还探讨了一个问题：在如此庞大的环境中，什么样的训练任务分布才能培养出最优秀的智能体？我们采用的动态任务生成技术允许智能体的训练任务分布不断发生变化：生成的每一个任务既不会太难也不会太简单，而是“刚刚好”适合训练。随后，我们使用基于种群的训练（PBT）方法，根据旨在提高智能体通用能力的适应度来调整动态任务生成的参数。最后，我们将多个训练轮次链接在一起，使每一代智能体都能基于上一代进行引导学习（bootstrap）。

这形成了一个以深度强化学习为核心的最终训练过程，通过每一步的经验来更新智能体的神经网络：

*   经验步骤来自于根据智能体行为动态生成的训练任务。
*   智能体的任务生成函数会根据智能体的相对表现和鲁棒性进行变异。
*   在最外层的循环中，各代智能体相互引导学习，为多人环境提供越来越丰富的对手/队友，并重新定义了进度衡量标准本身。

这个训练过程从零开始，迭代地构建复杂性，不断改变学习问题以保持智能体的学习状态。这种组合学习系统的迭代性质并不优化某个有界的性能指标，而是优化迭代定义的通用能力谱系，从而为智能体带来潜在的开放式学习过程，其局限仅在于环境空间的表达能力和智能体神经网络的容量。

### 科学衡量智能体的进步

为了衡量智能体在这个庞大宇宙中的表现，我们使用与训练数据完全分离的游戏和世界创建了一组评估任务。这些“保留（held-out）”任务包括专门由人类设计的任务，如捉迷藏和夺旗。

由于 XLand 规模庞大，理解和描述智能体的表现是一项挑战。每个任务涉及不同的复杂程度、不同的可实现奖励规模以及智能体的不同能力。因此，仅仅对保留任务的奖励求平均值，会掩盖复杂性和奖励的实际差异，并等同于将所有任务视为同样有趣——这对于程序化生成的环境来说并不一定成立。

为了克服这些限制，我们采取了不同的方法：
1.  我们使用当前训练玩家集计算出的纳什均衡值，对每个任务的分数进行归一化。
2.  我们考察归一化分数的整个分布——不是看平均归一化分数，而是看归一化分数的不同百分位数，以及智能体至少获得一步奖励的任务百分比（即参与度）。

这意味着，只有当一个智能体在所有百分位数上的表现都超过另一个智能体时，才会被认为更优秀。这种衡量方法为我们评估智能体的表现和鲁棒性提供了有意义的途径。

### 展现出更强通用能力的智能体

在对智能体进行了五代训练后，我们看到它们在保留的评估空间中的学习和表现出现了持续的提升。在 XLand 的 4000 个独特世界中游玩了大约 70 万个独特游戏后，最终代的每个智能体在 340 万个独特任务中经历了 2000 亿个训练步骤。此时，我们的智能体已经能够参与几乎每一个程序化生成的评估任务（除了少数连人类都不可能完成的任务）。我们观察到的结果清晰地展示了跨任务空间的通用、零样本（zero-shot）行为——归一化分数百分位数的边界在不断提高。

从定性角度观察我们的智能体，我们经常看到通用的启发式行为的涌现，而不是针对单个任务的高度优化、特定的行为。智能体在面对新情况时，并不是确切知道“最好的做法”是什么，而是展现出实验和改变世界状态的行为，直到它们达到获得奖励的状态。

我们还看到智能体依赖于使用其他工具，包括使用物体来遮挡视线、制造坡道以及找回其他物体。由于环境是多人的，我们可以在保留的社会困境任务（如“胆小鬼博弈”）中检查智能体行为的演变。随着训练的进行，我们的智能体在与自身的副本进行游戏时，似乎表现出更多的合作行为。考虑到环境的性质，很难确定这些行为的意图——我们看到的行为通常似乎是偶然的，但它们确实在持续发生。

通过分析智能体的内部表征，我们可以说，通过在庞大的任务空间中采用这种强化学习方法，我们的智能体意识到了自身躯体的基础情况和时间的流逝，并且理解了它们所遇到游戏的高层结构。也许更有趣的是，它们能清晰地识别出环境中的奖励状态。

这种在新任务中表现出的行为通用性和多样性，暗示了在下游任务上微调这些智能体的潜力。例如，我们在技术论文中展示，只需对新呈现的复杂任务进行 30 分钟的集中训练，智能体就能迅速适应；而从头开始使用强化学习训练的智能体则完全无法学习这些任务。

通过开发像 XLand 这样的环境以及支持开放式复杂性创建的新训练算法，我们已经看到了强化学习智能体具备零样本泛化能力的明确迹象。虽然这些智能体在这个任务空间内刚刚开始具备通用能力，但我们期待继续我们的研发工作，以进一步提高它们的性能并创造出更具适应性的智能体。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Agent]]
- [[00-元语/game]]
- [[00-元语/evals]]
- [[00-元语/benchmark]]
- [[00-元语/paper]]
