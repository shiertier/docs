---
title: "为什么大语言模型会产生幻觉？"
---

## 摘要

**1) 一句话总结**
大语言模型的幻觉是其基于统计概率进行文本补全的结构性副产品，目前无法被彻底根除，用户应将其视为辅助思考的工具并保持事实核查的习惯。

**2) 核心要点**
* **幻觉的定义**：指大语言模型（LLM）生成缺乏证据或现实支持，但听起来非常合理的输出内容（如捏造格式逼真但不存在的学术论文）。
* **底层机制**：LLM 并非在理解现实世界，而是作为统计模式匹配器，通过逼近条件概率（$P(x_t \mid x_{<t})$）来补全文本序列。
* **训练目标的局限**：模型的优化目标是最小化预测误差（交叉熵损失），只要续写在统计学上概率高即可，不验证事实或物理有效性。
* **缓解策略 - 外部信息支撑**：通过检索增强生成（RAG）和工具调用（如搜索引擎、计算器）引入语言分布之外的约束。
* **缓解策略 - 推理时自我检查**：采用多路径推理聚合（自洽性）和自我修正机制，在输出前进行内部验证。
* **缓解策略 - 不确定性建模**：利用基于人类反馈的强化学习（RLHF）等方法重塑激励机制，鼓励模型在置信度低时弃权。
* **缓解策略 - 结构与流水线控制**：近期研究侧重于基于归因的缓解技术、因果推理增强以及解码层面的概率控制。
* **用户实践建议**：将 LLM 作为推理支架、起草和头脑风暴的工具，而非真理引擎；对高置信度回答保持适度怀疑，并要求提供可核实的信息来源。

**3) 风险与不足**
* **无法根除**：目前学术界没有任何一种方法或决定性的架构突破能够彻底解决 LLM 的幻觉问题，现有手段仅能约束、检测或减少幻觉。
* **目标函数缺失**：模型的底层目标函数中缺失了保证物理一致性、逻辑有效性或现实可行性的明确项。
* **先天缺陷**：语言本身只是现实世界的不完美代理，仅基于语言训练的系统出现一定程度的错误是不可避免的结构性副产品。

## 正文

### 错误时的迷之自信

今天，我向 ChatGPT 问了一个在中文社交媒体上非常常见的问题：

> 如果你要去洗车，而洗车店就在 50 米外，你会步行还是开车去？

从人类的视角来看，答案显而易见：不把车开过去，你怎么洗车呢？

但 ChatGPT 的原话是这样回答的：

> 如果只有大约 **50 米远**，毫无疑问我会步行。
> 
> * 🚶 步行大概只需要 30到45 秒。
> * 🚗 如果算上上车、发动引擎、挪车和停车的时间，开车反而会花更久。
> * ⛽ 步行能省油（哪怕只是一点点）。
> * 🧠 为了这么短的距离去移动一台一两吨重的机器，感觉完全没有必要。
> 
> 我会选择开车的唯一理由是：
> 
> * 外面倾盆大雨，且我需要搬运重物回来。
> * 我有行动不便的问题。
> * 出于安全考虑（比如天黑、经过不安全的区域等）。
> 
> 否则，步行绝对是更好的选择。

这听起来很诡异，不是吗？大语言模型（LLM）能够生成让人感觉具备“智能”的内容，但它怎么会在如此简单的问题上翻车？

这就引出了本文的主题：**幻觉（Hallucination）**。在 LLM 的语境中，幻觉指的是那些缺乏证据或现实支持的输出内容，哪怕它们听起来非常合理。

### 是补全，而非理解

当 LLM 生成文本时，它们并没有真正在脑海中模拟现实世界的场景。相反，它们是在识别并延伸其训练文本中的模式。

在这个例子中，**“50 米外”**这个词组很可能激活了以下模式：
* 距离短
* 步行效率高
* 没必要开车

在日常对话中，这些都是统计学上非常常见的文本延续方式。

但这个问题中真正的限制条件并非距离，而是动作本身：**洗车**。核心逻辑很简单：你不可能在不把车带到店里的情况下洗车。

这种错位表明，LLM 并没有真正理解上下文。模型只是激活了特定的模式来补全序列，在这个例子中，它把这个问题当成了一个“距离优化”问题，而不是一个“物理限制”问题。

在 LLM 的底层逻辑中，它是在逼近条件概率：$P(x_t \mid x_{<t})$。

在每一步中，模型都会根据之前的上下文选择概率最高的下一个词（Token）。但请注意这个目标函数中缺失了什么：它没有任何明确的项来保证**物理一致性**（输出应符合现实世界的约束，如因果关系或客观可行性）、**逻辑有效性**（结论应在不矛盾的情况下从前提推导出来）或**现实可行性**。

这是因为在训练之初，模型的优化目标是最小化预测误差（交叉熵损失），而不是验证事实或物理的有效性。只要一个续写在给定上下文中具有统计学上的高概率，它就能获得较低的损失值。

在许多语境下，关于短距离的讨论确实会引出“步行与开车”的权衡，所以模型只是顺应了这种概率分布。

### 为什么会产生幻觉？

这正是目前学术界积极研究的领域。幻觉是一个众所周知的问题，虽然可以被减少，但无法被彻底消除。大多数缓解策略可以归入以下几个大类，而非单一的技巧：

#### 1. 外部信息支撑（External Grounding）
一个主要的方向是将生成过程与外部信息源绑定，而不是纯粹依赖内部的词元统计概率。例如，**检索增强生成（RAG）**通过检索到的文档来辅助生成。同样，**工具调用（Tool use）**允许模型调用搜索引擎、计算器或代码运行环境，用可验证的信息源来取代猜测。这两种方法的核心思想都是从语言分布之外引入约束。

#### 2. 推理时的自我检查（Inference-Time Self-Checking）
另一个方向是在生成过程中增强可靠性。像**多路径推理与聚合（自洽性）**这样的方法，会采样多个推理轨迹并对答案进行聚合。同样，**自我修正（Self-refinement）**让模型在最终输出前对自己的内容进行批判和修改。不过，这些方法并没有改变训练目标，而是试图通过引入冗余或内部验证循环来减少错误。

#### 3. 不确定性建模（Uncertainty Modeling）
第三类方法直接针对模型的“过度自信”。诸如**感知不确定性的生成与校准**等方法，鼓励模型在置信度低时选择弃权，而不是自信地瞎猜。同样，**基于人类反馈的强化学习（RLHF）**改变了奖励机制，倾向于让模型承认自己的不确定性，而不是捏造看似合理的答案。这些方法本质上是在试图重塑模型的激励机制。

#### 4. 结构与流水线级别的控制（Structural and Pipeline-Level Control）
近期（2025–2026年）的研究更侧重于在生成流水线内部对幻觉风险进行建模。
* **基于归因的缓解技术**：追踪幻觉在检索系统中的起源，并强制要求证据对齐。
* **因果推理增强**：在推理流程中引入明确的结构化约束。
* 其他研究还包括**解码层面的概率控制**和**混合检索机制**，旨在从采样和检索层降低风险。

总而言之，虽然有许多充满希望的研究方向，但没有任何一种方法能保证彻底解决 LLM 的幻觉问题。目前的大多数手段都是在约束、检测或减少幻觉，而非根除它。

时至今日，还没有出现能够完全解决幻觉的决定性架构突破。大部分进展来自于更好的外部支撑、更好的验证、更好的校准和更好的控制。在不久的将来，新的训练目标或结合现实世界基础的混合架构可能会改变这一现状。但就目前而言，幻觉看起来不像是随机的 Bug，而更像是“优化语言概率”这一机制所带来的结构性副产品。

### 这对我们意味着什么？

听起来很让人沮丧，对吧？这是否意味着我们无能为力，作为用户只能被动接受？

并非如此。

如果幻觉是结构性的，那么我们真正需要调整的是我们的预期。

我们必须意识到，LLM 不是事实数据库，也不是真理引擎，而是强大的统计模式匹配器。归根结底，语言本身只是现实世界的一个不完美的代理，因此，一个仅仅基于语言训练的系统出现一定程度的错误是意料之中的。

对于用户来说，有几个非常实用的建议：

* **反复核查事实声明。**
* **以适度的怀疑态度对待看似自信的回答。**（既不盲目信任，也不全盘否定：对影响重大的声明进行核实，尤其是当模型表现得非常确定时。）
* **将它们用作推理支架、起草、头脑风暴和探索的工具**，而不是最终的正确性检验器。
* **要求模型提供信息来源**，然后亲自去核实这些来源。

换句话说，我们应该把它们当作思考的“倍增器”，而不是判断力的“替代品”。

幻觉并不意味着这个系统毫无用处，它只是意味着我们需要成为知情且谨慎的用户。我确实希望未来的架构能整合更强的外部支撑、世界模型或混合推理系统，但在那之前，最安全的做法既不是盲目信任，也不是彻底拒绝，而是“校准后的协作”。

---

### 作者注

我需要承认，本文前面使用的“洗车”例子并不是一个非常严谨的案例。事后看来，它更像是一个“语境框架误解”，而不是一个纯粹的幻觉案例，我在最初写作时应该更加严谨。对此我表示歉意。

在早期版本的 LLM 中，一个更典型的幻觉模式是：生成完全不存在的学术论文，但看起来却无比真实（有着合理的标题、发表期刊和类似 DOI 的字符串）。其潜在机制是相同的：模型补全了一个在统计学上极具可能性的格式，但这种补全并不能保证在现实证据中是真实的。

## 相关文档

- [[01-博客/Si Cheng/如果大语言模型只是预测下一个Token，它们为什么能起作用？|如果大语言模型只是预测下一个Token，它们为什么能起作用？]]；关联理由：解说；说明：该文解释“预测下一个 Token”如何产生类智能输出，为本文分析“幻觉的结构性来源”提供机制背景。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/ChatGPT]]
- [[00-元语/rag]]
- [[00-元语/alignment]]
- [[00-元语/risk]]
