# 现代大语言模型中最奇怪的瓶颈及TiDAR的破局之道

## 文档信息
- 来源：https://towardsdatascience.com/the-strangest-bottleneck-in-modern-llms/
- 发布日期：2026-02-16

## 摘要
**1) 一句话总结**
英伟达提出的TiDAR架构通过结合扩散模型的并行起草能力与自回归模型的验证准确性，成功打破了大语言模型因内存传输导致的算力闲置瓶颈，实现了最高近6倍的推理加速且保持输出质量无损。

**2) 关键点**
*   **核心痛点**：传统LLM逐词生成时，内存与GPU显存间的数据传输耗时远超实际计算时间，导致GPU算力严重闲置。
*   **架构创新**：TiDAR（在扩散中思考，在自回归中表达）将顺序生成转化为并行过程，结合了自回归（AR）的准确性与扩散模型的速度。
*   **输入构造**：输入序列被重构为三个部分：前缀（历史信息）、草稿（待验证的猜测）和未来掩码（待填补的新猜测空位）。
*   **自回归验证（表达）**：利用因果注意力掩码，在一次前向传递中并行验证多个草稿Token；若草稿错误，可利用已计算的概率分布“免费”纠错并截断后续错误草稿。
*   **扩散起草（思考）**：利用双向掩码，在验证的同时结合全局语境为未来的掩码空位并行起草多个Token，形成“提前起草+后方验证”的连续循环。
*   **显著提速**：相比标准AR模型，TiDAR在1.5B参数模型上实现4.71倍加速，在8B参数模型上实现5.91倍加速。
*   **质量无损**：在HumanEval（编程）和GSM8K（数学）基准测试中，得分与基线AR模型持平，部分指标因起草的“前瞻”特性而略优。
*   **高效率与“免费Token”**：相比依赖外部小模型的投机解码（如EAGLE-3），TiDAR使用主干网络起草，草稿接受率更高；在达到计算瓶颈前，单次前向传递最多可“免费”起草约60个Token而不增加延迟。

## 正文
### 引言与背景

人工智能，尤其是像ChatGPT这样的大语言模型（LLM），已经深度融入我们的日常生活和工作流。它们能够处理从编写复杂代码到简单文本总结的各种任务。然而，这些模型令人惊叹的能力却在很大程度上被一个单一的瓶颈所制约。尽管运行这些模型的硬件速度极快，但获取响应的实际过程依然让人感觉缓慢而迟滞。

从本质上讲，模型每生成一个词，都需要将模型权重从系统内存加载到GPU显存中进行计算，然后再将数据移回系统内存。由于实际计算所需的时间远少于内存之间的数据传输时间，芯片不得不处于闲置状态，等待下一批数据的到来，这造成了极大的资源浪费。

为了让芯片保持忙碌而不是在内存传输期间闲置，业界尝试了多种算法。其中一种技术是“投机解码”（Speculative Decoding），它使用一个较小、通常也较弱的模型来起草多个未来Token，再由主模型一次性进行验证。但由于小模型往往不够智能，会犯很多错误，主模型不得不拒绝这些草稿，从而失去了加速的意义。另一方面，纯并行的扩散模型（Diffusion models）可以一次性写出数百个Token，但这往往以牺牲准确性和语言连贯性为代价。

理想的架构应该介于自回归（AR）模型的准确性和扩散模型的速度之间。

### 解决方案：TiDAR架构

英伟达（Nvidia）的研究人员提出了一个名为TiDAR的新颖架构，全称为“在扩散中思考，在自回归中表达”（Think in Diffusion, Talk in Autoregression）。

TiDAR的巧妙之处在于，它将传统LLM中通常是顺序执行的过程转化为并行过程。它证明了尽管自回归和扩散是两种完全不同的设计理念，但依然可以被统一并发挥各自的优势。

要理解其核心，我们需要看看该模型的输入是如何构造的。在标准LLM中，我们输入所有过去的词来逐个预测Token。而在TiDAR中，输入序列被构造为特殊的三个部分。

假设我们有句子“The cat sat”，拼接在一起后，完整的输入序列如下：
*   **前缀（The Prefix）**：“The”, “cat”, “sat”（来自用户的历史信息）。
*   **草稿（The Drafts）**：“on”, “the”（上一步的猜测，需要在当前迭代中进行验证）。
*   **未来掩码（The Future Masks）**：[MASK], [MASK]（需要填入新猜测的空位）。

了解了输入张量的背景后，我们来看看实际的处理过程是如何发生的。

### 组件一：“表达”（自回归验证器）

这是模型架构中最关键的第一部分。在此阶段，模型的任务是验证上一次迭代生成的草稿（"on", "the"），并决定它们是否足够好以被保留。

你可能会问：“如果模型必须检查草稿的好坏，这怎么会比直接生成它们更快呢？”

在普通的自回归模型中，如果你想生成5个词，你必须运行模型5次。你输入第1个词得到第2个词，然后输入前2个词得到第3个词，依此类推。GPU必须5次从内存中加载庞大的模型权重。这就是需要消除的主要瓶颈。

TiDAR在验证草稿Token时恰好解决了这个问题，因为它可以在一次前向传递中完成验证。这意味着2个词（["on", "the"]）只需一次传递就能添加到输出中。它使用因果注意力掩码（Causal Attention Mask）来确保：
*   检查“on”时，模型只能看到“The cat sat”。
*   检查“the”时，模型只能看到“The cat sat on”。

由于GPU是强大的并行处理器，它可以在一次操作中同时计算所有这些草稿的“正确性”。这相当于花1步的时间做了2步的工作，巨大的速度提升正来源于此。

**如果草稿错了怎么办？** 比如草稿是["in", "pizza"]而不是["on", "the"]？
最棒的一点是，草稿错了也没关系，纠错几乎是免费的。

模型通过计算基于上下文的词汇概率分布来验证草稿。如果草稿是模型可能会选择的合理预测，就会被选中；如果不是，模型会从刚刚计算出的分布中选择概率最高的词。

由于计算是在同一次前向传递中完成的，我们不需要重新运行模型，只需：
1.  丢弃错误的草稿（["in"]）。
2.  立即从刚刚计算的概率列表中换上正确的词（["on"]）。
3.  切断所有后续的草稿（["pizza"]，因为它们是基于错误的词生成的）。

这保证了最终输出在数学上与模型缓慢、逐步运行时一样有效。我们同时获得了并行处理的速度和顺序处理的准确性。

### 组件二：“思考”（扩散起草器）

当自回归的“表达”组件忙于验证保留或拒绝哪些Token时，“思考”组件正在为下一次迭代起草Token。

扩散头（Diffusion head）试图填补输入序列末尾的[MASK]空位，以便自回归头在下一次迭代中验证它们。为此，模型会同时查看序列中的所有词。它专门为这些[MASK] Token使用双向掩码（Bidirectional Mask），而不是通常的因果掩码。

因为扩散头必须一次性起草多个Token，它需要将所有词与所有[MASK]联系起来。它实际上是在捕捉整个序列的“语境”来填补空位。在我们的例子中，扩散头会结合历史信息（“The cat sat on the”）查看所有[MASK] Token，并尝试将它们“去噪”成最合理、连贯的文本，比如猜测接下来是“red mat”。

这创造了一个连续的循环：
*   第1步：扩散头猜测“on the”。
*   第2步：这些猜测移动到“草稿”位置。
*   自回归头验证（并在必要时纠正）它们。
*   同时，扩散头开始猜测下一个短语（“red mat”）。

通过不断地提前起草和在后方验证，TiDAR让GPU保持满负荷运转，确保计算能力不被浪费。

### 测试结果与优势

研究人员对TiDAR进行了多项测试，得出了以下结论：

#### 1. 速度：巨大的飞跃
与标准自回归（AR）模型相比，TiDAR的吞吐量（每秒生成的Token数）显著增加：
*   对于**1.5B参数模型**，TiDAR实现了**4.71倍**的加速。这意味着它生成相同数量文本的速度比标准LLM架构快近5倍。
*   对于更大的**8B参数模型**，加速差距更大，达到了**5.91倍**。
这彻底改变了传统的“下一个词预测”模式，转向了一次性起草多个Token。

#### 2. 质量：弥合差距
纯扩散LLM（如Dream或Llada）通常难以匹敌AR模型的推理能力和连贯性。TiDAR凭借其混合方法，完美地弥合了这一差距。通过使用自回归头验证扩散头生成的草稿，TiDAR同时兼具了AR模型的保真度和纯扩散模型的速度。
*   在HumanEval（编程）和GSM8K（数学）等基准测试中，TiDAR取得了与基线AR模型相比“无损”的分数。
*   在某些指标上，它甚至略微优于基线，这可能是由于起草过程的“前瞻”特性帮助模型在推理任务中做出了更好的规划。

#### 3. 效率：对比投机解码
作者将TiDAR与目前最佳的推理加速方法EAGLE-3（基于投机解码）进行了对比。投机解码依赖较小的外部模型起草Token，错误率高，浪费算力。而TiDAR使用自身的主干网络来起草和验证Token，使得草稿质量更高。
*   TiDAR的“接受率”（草稿正确的频率）显著更高。
*   高接受率意味着模型花在纠错上的时间更少，生成实际文本的时间更多。

#### 4. “免费Token”优势
实验验证了论文的核心假设：我们是否将GPU利用到了极限。
结果表明，与标准的前向传递相比，TiDAR的起草机制几乎不增加任何延迟。在标准传递中，GPU受限于内存带宽，数据加载和卸载是限速步骤。但在TiDAR中，我们可以给GPU增加额外的工作，而不是让它闲置。

数据表明，在计算真正成为GPU瓶颈之前，一次前向传递中大约可以起草60个Token。这意味着理论上我们可以一次性生成60个Token，而不会增加任何延迟。

## 关联主题
- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
