# 多GPU环境下的AI：GPU之间如何通信

## 文档信息
- 来源：https://towardsdatascience.com/how-gpus-communicate/
- 发布日期：2026-02-19

## 摘要
**1) 一句话总结**
本文详细解析了支持AI工作负载中多GPU通信的底层硬件技术栈（包括PCIe、NVLink、NVSwitch和InfiniBand），并阐述了实现高效分布式GPU计算的关键设计原则。

**2) 关键要点**
*   **PCIe总线**：用于连接GPU与主板，Gen6 x16双向带宽约128 GB/s；受限于CPU通道总数（通常128个）及机箱物理/功耗限制，单服务器节点通常最多配备8个GPU。
*   **NVLink技术**：实现节点内GPU间绕过CPU的直接通信，Blackwell架构下单GPU带宽高达1.8 TB/s；部分CPU（如NVIDIA Grace）支持NVLink，可加速CPU-GPU通信并实现高性价比的“CPU卸载”。
*   **NVSwitch交换机**：作为通信枢纽解决NVLink点对点连接时的带宽分割问题，实现“无阻塞”全速通信；第三代产品可跨节点互连最多256个H100 GPU。
*   **InfiniBand网络**：用于跨节点大规模扩展（数千个GPU），支持GPUDirect RDMA以绕过CPU直接访问GPU内存；NDR200单端口带宽约100 GB/s。
*   **线性扩展目标**：理想状态下GPU数量翻倍则吞吐量翻倍，但在AI训练中因通信需求随设备数量增加而增长，完美线性扩展较为罕见。
*   **计算与通信重叠**：为最大化利用算力，通信操作应尽可能与计算过程重叠，未能重叠的通信被称为“暴露操作”。
*   **节点内与节点间差异**：单节点内（最多8个GPU）依赖NVLink可实现接近线性的扩展，而跨节点通信依赖InfiniBand，速度较慢且存在两次PCIe遍历开销。

**3) 风险/缺口**
*   **NVLink点对点带宽分割局限**：在没有NVSwitch的情况下，单GPU的总带宽会被平均分割到多个点对点连接中（例如8卡H100中，900 GB/s总带宽会被分成7份，单连接仅约128 GB/s）。
*   **跨节点“性能悬崖”**：当扩展到8个GPU以上并使用InfiniBand进行跨节点通信时，会引入网络协议开销、更高的延迟和带宽限制，导致性能出现大幅下降。
*   **算力浪费风险**：如果无法实现计算与通信的完美重叠，GPU将处于空闲状态等待数据传输完成，从而造成计算资源的浪费。

## 正文
本文将深入探讨支持AI工作负载中多GPU通信的硬件基础架构。

在深入研究高级并行技术之前，我们需要先了解实现GPU间相互通信的关键技术。为什么GPU需要通信？在跨多个GPU训练AI模型时，每个GPU处理不同的数据批次，但它们都需要通过在反向传播期间共享梯度或交换模型权重来保持同步。现代AI训练是高度通信密集型的，因此高效的GPU间数据传输对整体性能至关重要。

### 通信技术栈

#### PCIe
PCIe（外围组件互连快速总线）使用独立的点对点串行通道将GPU等扩展卡连接到主板。对于使用16个通道的GPU，各代PCIe提供的双向带宽如下：
- **Gen4 x16**：约 32 GB/s
- **Gen5 x16**：约 64 GB/s
- **Gen6 x16**：约 128 GB/s

高端服务器CPU通常提供128个PCIe通道，而现代GPU需要16个通道以获得最佳带宽。这就是为什么一台服务器通常配备8个GPU（128 = 16 x 8）。此外，服务器机箱的功耗和物理空间限制，也使得单节点内超过8个GPU变得不切实际。

#### NVLink
NVLink实现了同一服务器（节点）内GPU之间的直接通信，完全绕过了CPU。这种NVIDIA专有的互连技术在GPU之间创建了具有极高带宽的直接内存到内存路径：
- **NVLink 3 (A100)**：每块GPU约 600 GB/s
- **NVLink 4 (H100)**：每块GPU约 900 GB/s
- **NVLink 5 (Blackwell)**：每块GPU高达 1.8 TB/s

**关于CPU-GPU通信的NVLink：**
某些CPU架构支持将NVLink作为PCIe的替代方案。通过克服数据传输（例如将训练批次从CPU移动到GPU）中的PCIe瓶颈，它极大地加速了CPU与GPU之间的通信。这种能力使得“CPU卸载”（一种通过将数据存储在系统RAM中来节省GPU VRAM的技术）在实际AI应用中变得可行。由于扩展RAM通常比扩展VRAM更具成本效益，这种方法提供了显著的经济优势。支持NVLink的CPU包括IBM POWER8、POWER9和NVIDIA Grace。

**局限性：**
在一台配备8块H100的服务器中，每个GPU需要与其他7个GPU通信。这意味着900 GB/s的带宽会被分成7个点对点连接，每个连接的带宽仅约128 GB/s。为了解决这个问题，NVSwitch应运而生。

#### NVSwitch
NVSwitch充当GPU通信的中心枢纽，根据需要在GPU之间动态路由（交换）数据。借助NVSwitch，每个Hopper架构的GPU可以同时以900 GB/s的速度与所有其他Hopper GPU通信，这意味着峰值带宽不再受限于正在通信的GPU数量。这就是NVSwitch的“无阻塞”特性。每个GPU通过多个NVLink连接到几个NVSwitch芯片，从而确保最大带宽。

虽然NVSwitch最初是作为节点内解决方案，但它已被扩展用于互连多个节点，创建了支持多达256个GPU的集群，并能以接近本地NVLink的速度进行全对全通信。

NVSwitch的各代产品包括：
- **第一代**：每台服务器最多支持16个GPU（兼容Tesla V100）
- **第二代**：同样支持最多16个GPU，但带宽更高、延迟更低
- **第三代**：专为H100 GPU设计，最多支持256个GPU

#### InfiniBand
InfiniBand主要负责节点间通信。虽然它比NVSwitch慢得多（也便宜得多），但常用于数据中心以扩展到数千个GPU。现代InfiniBand支持NVIDIA GPUDirect® RDMA（远程直接内存访问），允许网络适配器直接访问GPU内存，而无需CPU参与，从而避免了向主机RAM进行昂贵的数据复制。

当前的InfiniBand速度包括：
- **HDR**：每个端口约 25 GB/s
- **NDR**：每个端口约 50 GB/s
- **NDR200**：每个端口约 100 GB/s

由于网络协议开销以及需要两次PCIe遍历（发送端和接收端各一次），这些速度明显慢于节点内的NVLink。

### 关键设计原则

#### 理解线性扩展
线性扩展是分布式计算的终极目标。简而言之，它意味着GPU数量翻倍应使吞吐量翻倍，并将训练时间减半。当通信开销与计算时间相比极小时，每个GPU都能满负荷运行，就会出现这种情况。然而，在AI工作负载中，完美的线性扩展很少见，因为通信需求会随着设备数量的增加而增长，并且通常不可能实现完美的计算与通信重叠。

#### 计算与通信重叠的重要性
当GPU处于空闲状态，等待数据传输完成才能进行处理时，计算资源就被浪费了。因此，通信操作应尽可能与计算重叠。当无法实现重叠时，我们将这种通信称为“暴露操作”（exposed operation）。

#### 节点内与节点间：性能悬崖
现代服务器级主板最多支持8个GPU。在这个范围内，得益于高带宽、低延迟的节点内通信，通常可以实现接近线性的扩展。

然而，一旦扩展到8个GPU以上，并开始使用通过InfiniBand连接的多个节点，性能就会出现大幅下降。节点间通信比节点内NVLink慢得多，会引入网络协议开销、更高的延迟和带宽限制。随着GPU数量的增加，每个GPU必须与更多的对等节点进行协调，从而花费更多时间空闲等待数据传输完成。

### 总结

了解CPU-GPU和GPU-GPU通信的基础知识（PCIe、NVLink、NVSwitch和InfiniBand）以及分布式GPU计算的关键设计原则，能够帮助我们在设计AI工作负载时做出更明智的决策。在后续的内容中，我们将进一步探讨具体的并行技术，例如分布式数据并行（DDP）。

## 相关文档
- [[01-博客/Towards Data Science/构建企业级本地 AI 的 GPUaaS 架构：Kubernetes 上的多租户、调度与成本模型|构建企业级本地 AI 的 GPUaaS 架构：Kubernetes 上的多租户、调度与成本模型]]；关联理由：上下游；说明：该文聚焦企业侧多 GPU 资源调度与租户隔离，是本文通信基础设施在平台层落地的上游设计。
- [[01-博客/Mistral/vLLM 内存泄漏排查|vLLM 内存泄漏排查]]；关联理由：延伸思考；说明：该文从 UCX 与 InfiniBand 相关问题切入，补充了跨节点高速通信栈在实际工程中的稳定性风险。

## 关联主题
- [[00-元语/AI]]
- [[00-元语/protocol]]
