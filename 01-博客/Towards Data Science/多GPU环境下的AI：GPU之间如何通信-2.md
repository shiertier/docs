---
title: "多GPU环境下的AI：GPU之间如何通信"

来源: "https://towardsdatascience.com/how-gpus-communicate/"
发布日期: "2026-02-19"
---

## 摘要

**1) 一句话总结**
本文详细解析了多GPU环境下AI训练的底层通信硬件架构（包括PCIe、NVLink、NVSwitch和InfiniBand），并阐述了实现高效分布式计算的关键设计原则。

**2) 关键要点**
*   **PCIe限制与节点规模**：PCIe用于连接GPU与主板（Gen6 x16双向带宽约128 GB/s），受限于高端CPU的128个PCIe通道以及机箱功耗/空间，单服务器节点通常最多配备8个GPU。
*   **NVLink节点内直连**：NVLink实现了节点内GPU之间绕过CPU的直接通信，带宽极高（A100约600 GB/s，H100约900 GB/s，Blackwell高达1.8 TB/s）。
*   **CPU-GPU通信优化**：部分支持NVLink的CPU（如NVIDIA Grace）可克服PCIe瓶颈，使将数据存储在系统内存中的“CPU卸载”技术在经济和性能上变得可行。
*   **NVSwitch无阻塞交换**：NVSwitch作为中央枢纽解决了NVLink点对点带宽被分割的问题，使GPU能以峰值带宽同时通信；第三代NVSwitch支持跨节点互连最多256个GPU。
*   **InfiniBand节点间通信**：用于数据中心的大规模节点间扩展，支持GPUDirect RDMA（网络适配器直接访问显存，绕过CPU），当前端口速度在25 GB/s (HDR) 到 100 GB/s (NDR200) 之间。
*   **计算与通信重叠**：为逼近理想的“线性扩展”，必须尽可能将通信操作与计算过程重叠，以避免GPU闲置。

**3) 风险/缺口**
*   **NVLink的带宽分割局限**：在没有NVSwitch的情况下，单块GPU的总带宽会被平均分割给其他GPU（例如8卡H100中，900 GB/s会被分割成7个仅约128 GB/s的连接）。
*   **跨节点“性能悬崖”**：当扩展超过单节点（8个GPU）并使用InfiniBand时，由于网络协议开销、更高的延迟以及需要两次穿过PCIe总线，会导致性能大幅下降。
*   **暴露操作（Exposed Operation）导致资源浪费**：在AI工作负载中，如果无法实现完美的计算与通信重叠，GPU将处于空闲状态等待数据传输，从而造成计算资源的浪费。

## 正文

本文将深入探讨为AI工作负载提供多GPU通信支持的硬件基础架构。在深入研究高级并行技术之前，我们需要先了解实现GPU间通信的关键技术。

为什么GPU需要通信？在多个GPU上训练AI模型时，每个GPU处理不同的数据批次，但它们都需要通过在反向传播期间共享梯度或交换模型权重来保持同步。具体的通信内容和时机取决于所采用的并行策略。现代AI训练属于通信密集型任务，因此高效的GPU间数据传输对整体性能至关重要。

### 通信技术栈

#### PCIe
PCIe（外围组件互连快速总线）使用独立的点对点串行通道将GPU等扩展卡连接到主板。对于使用16个通道的GPU，不同代际的PCIe提供的双向带宽如下：
*   **Gen4 x16**：约 32 GB/s
*   **Gen5 x16**：约 64 GB/s
*   **Gen6 x16**：约 128 GB/s（即 16通道 × 8 GB/s/通道 = 128 GB/s）

高端服务器CPU通常提供128个PCIe通道，而现代GPU需要16个通道以获得最佳带宽。这就是为什么一台服务器通常配备8个GPU（128 = 16 × 8）。此外，服务器机箱的功耗和物理空间限制也使得单节点超过8个GPU变得不切实际。

#### NVLink
NVLink实现了同一服务器（节点）内GPU之间的直接通信，完全绕过CPU。这种NVIDIA专有的互连技术在GPU之间创建了直接的内存到内存路径，具有极高的带宽：
*   **NVLink 3 (A100)**：每块GPU约 600 GB/s
*   **NVLink 4 (H100)**：每块GPU约 900 GB/s
*   **NVLink 5 (Blackwell)**：每块GPU高达 1.8 TB/s

**关于CPU-GPU通信的补充：**
某些CPU架构（如IBM POWER8、POWER9和NVIDIA Grace）支持将NVLink作为PCIe的替代方案。这克服了数据传输（如将训练批次从CPU移动到GPU）中的PCIe瓶颈，大幅加速了CPU与GPU之间的通信。这种能力使得“CPU卸载”（CPU-offloading，一种通过将数据存储在系统内存中来节省显存的技术）在实际AI应用中变得可行。由于扩展系统内存通常比扩展显存更具成本效益，这种方法具有显著的经济优势。

然而，NVLink也存在局限性。在配备8块H100的服务器中，每块GPU需要与其他7块GPU通信，这会将900 GB/s的带宽分割成7个点对点连接，每个连接仅约128 GB/s。为了解决这个问题，NVSwitch应运而生。

#### NVSwitch
NVSwitch充当GPU通信的中央枢纽，根据需要动态路由（或交换）GPU之间的数据。借助NVSwitch，每块Hopper架构的GPU都可以同时以900 GB/s的速度与所有其他Hopper GPU通信，即峰值带宽不再受通信GPU数量的限制。这就是NVSwitch的“无阻塞（non-blocking）”特性。每块GPU通过多个NVLink连接到多个NVSwitch芯片，从而确保最大带宽。

虽然NVSwitch最初是作为节点内解决方案，但它已被扩展用于互连多个节点，创建支持多达256个GPU的集群，并能以接近本地NVLink的速度进行全互联（all-to-all）通信。

NVSwitch的代际演进如下：
*   **第一代**：每台服务器最多支持16个GPU（兼容Tesla V100）
*   **第二代**：同样支持最多16个GPU，但带宽更高、延迟更低
*   **第三代**：专为H100 GPU设计，最多支持256个GPU

#### InfiniBand
InfiniBand负责节点间通信。虽然它比NVSwitch慢得多（也更便宜），但常用于数据中心以扩展至数千个GPU。现代InfiniBand支持NVIDIA GPUDirect® RDMA（远程直接内存访问），允许网络适配器直接访问GPU内存，而无需CPU参与（避免了向主机内存进行昂贵的数据复制）。

当前的InfiniBand速度包括：
*   **HDR**：每个端口约 25 GB/s
*   **NDR**：每个端口约 50 GB/s
*   **NDR200**：每个端口约 100 GB/s

由于网络协议开销以及需要两次穿过PCIe总线（发送端和接收端各一次），这些速度明显慢于节点内的NVLink。

### 关键设计原则

#### 理解线性扩展
线性扩展是分布式计算的终极目标。简而言之，它意味着GPU数量翻倍，吞吐量也应翻倍，训练时间减半。当通信开销与计算时间相比微乎其微，允许每个GPU满负荷运行时，就会出现这种情况。然而，在AI工作负载中，完美的线性扩展非常罕见，因为通信需求会随着设备数量的增加而增长，而且通常不可能实现完美的计算与通信重叠。

#### 计算与通信重叠的重要性
当GPU处于空闲状态，等待数据传输完成后才能进行处理时，就是在浪费资源。通信操作应尽可能与计算重叠。当无法重叠时，我们将这种通信称为“暴露操作（exposed operation）”。

#### 节点内与节点间：性能悬崖
现代服务器级主板最多支持8个GPU。在这个范围内，得益于高带宽、低延迟的节点内通信，通常可以实现接近线性的扩展。

一旦扩展到8个GPU以上，并开始使用通过InfiniBand连接的多个节点，性能就会出现大幅下降。节点间通信比节点内NVLink慢得多，会引入网络协议开销、更高的延迟和带宽限制。随着GPU数量的增加，每个GPU必须与更多的对等节点进行协调，从而花费更多空闲时间等待数据传输完成。

### 总结

本文介绍了CPU-GPU和GPU-GPU通信的基础知识（包括PCIe、NVLink、NVSwitch和InfiniBand），以及分布式GPU计算的关键设计原则。掌握这些知识后，在设计AI工作负载时就能做出更明智的决策。在后续的内容中，我们将进一步探讨分布式数据并行（DDP）等具体的并行技术。

## 相关文档

- [[01-博客/Towards Data Science/多GPU环境下的AI：GPU之间如何通信|多GPU环境下的AI：GPU之间如何通信]]；关联理由：版本演进；说明：该文与当前条目同题，属于同一内容的早期版本，可用于对照修订差异。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/protocol]]
