# evals

## 文档信息

- 类型：主题词条
- 更新日期：2026-02-22

## 定义

用于聚合评估与 Evals 相关文档。Evals 是对人工智能模型（尤其是大语言模型）的能力、安全性、准确性及特定任务表现进行系统性测试与量化评估的框架与方法论。

## 核心内涵

- **基准测试**：通过标准化的数据集和任务集，对模型在常识推理、代码生成、数学计算等维度的基础能力进行横向对比。
- **自动化评估**：利用脚本或辅助模型（LLM-as-a-Judge）对目标模型的输出进行批量化、自动化的打分与验证，提升评估效率。
- **多维度指标**：涵盖准确率、召回率、相关性、连贯性以及安全性（如防范有害内容生成）等复合评价体系。
- **持续监测**：在模型微调或系统迭代过程中，作为质量保障的核心环节，防止模型能力出现灾难性遗忘或退化。

## 实践要点

- **数据集构建**：确保评估数据的多样性、无偏性与时效性，防范模型在训练阶段“刷榜”导致评估失效。
- **评估标准对齐**：在引入自动化评估时，需定期引入人工抽检（Human-in-the-loop），确保机器打分与人类偏好保持一致。
- **流水线集成**：将 Evals 深度整合到持续集成与部署流程中，设定明确的通过阈值，拦截不达标的模型版本。
- **边界测试**：针对对抗性输入、极端场景及长尾问题设计专门的测试用例，探测模型的鲁棒性与安全底线。

## 相关词条

- [[00-元语/benchmark]]
- [[00-元语/llm]]
- [[00-元语/llmops]]
- [[00-元语/CI]]

## 关联主题
- [[00-元语/benchmark]]
- [[00-元语/llm]]
- [[00-元语/llmops]]
- [[00-元语/CI]]
- [[00-元语/observability]]
- [[00-元语/security]]
